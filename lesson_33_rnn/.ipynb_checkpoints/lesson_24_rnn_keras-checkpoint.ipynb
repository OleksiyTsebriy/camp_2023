{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Recurrent neural networks \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "## Samples or sequences to sequence problems\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "    <td>Speech recogniiton</td>\n",
    "    <td><img src = \"images/lesson_21_waves.jpg\" align = 'left' width = 100, heaight = 100> </td>\n",
    "    <td>Machine learning provides computer systems the ability <br> \n",
    "         to learn without being explicitly programmed\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Machine translation</td>\n",
    "    <td> 機器學習為計算機系統提供了能力\n",
    "          在沒有明確編程的情況下學習\n",
    "    </td>\n",
    "    <td>Machine learning provides computer systems the ability <br> \n",
    "         to learn without being explicitly programmed\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Video activity recognition</td>\n",
    "    <td><img src = \"images/lesson_21_var.png\" align = 'left' width = 100, heaight = 100> \n",
    "    </td>\n",
    "    <td>Start running\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Named entity recognition</td>\n",
    "    <td>Michael arrived to New York from Boston  in 21 Sep 1978 \n",
    "    </td>\n",
    "    <td>Person: Michael<br>Location: New York, Boston<br> date: 1978/09/21 \n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td>Sentiment analysis</td>\n",
    "    <td> It was not as terrifying enough for thriller as expected\n",
    "    </td>\n",
    "    <td><img src = \"images/lesson_21_stars.jpg\" align = 'left' width = 100, heaight = 100>\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Notation\n",
    "\n",
    "</font>\n",
    "\n",
    "For example `named entity recognition`.\n",
    "\n",
    "Some sample $(x,y) = (X^{(i)},Y^{(i)})$\n",
    "\n",
    "<img src = \"images/lesson_21_notation2.png\" align = 'left' width = 600, heaight = 600> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "Len of input $T_{x}=11; \\quad$ Len of output $T_{y}=11$ \n",
    "\n",
    "Note: $T_{x}$ and $T_{y}$ may differ\n",
    "\n",
    "Each $x^{<t>}$ is represented as one-hot vector built being based on some vocabulary (e.g. 10,000 most frequent words + `\"UNK\"` + `\"EOS\"`). \n",
    "<br> Shape of every sample $x = (vocab\\_size, T_{x}) $\n",
    "\n",
    "Each $y^{<t>}$ is represented as one-hot vector built being based on number of entities types \n",
    "<br> Shape of  of every sample $y= (entity\\_num, T_{y}) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## RNN structure\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "Every cell contains the same parameters \n",
    "\n",
    "<img src = \"images/lesson_21_rnn_simple1.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "$$A^{<0>}=0$$<br>\n",
    "$$A^{<t>}= g_{a}(W_{aa}@ A^{<t-1>} + W_{ax}@ x^{<t>}+ b_{a}), \\quad (g_{a} \\text{is usually RELU/tanh)}$$ <br>\n",
    "$$y^{<t>}= g_{y}(W_{ay}@ A^{<t>} + b_{y}), \\quad (g_{y} \\text{ is usually sigmoid/softmax)}$$\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## RNN cell\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"images/lesson_21_rnn_cell2.png\" align = 'left' width = 500, heaight = 500> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## RNN types samples\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"images/lesson_21_oto.png\" align = 'left' width = 100, heaight = 100> \n",
    "<img src = \"images/lesson_21_mto.png\" align = 'left' width = 400, heaight = 400> \n",
    "<img src = \"images/lesson_21_otm.png\" align = 'left' width = 400, heaight = 400> \n",
    "<div style=\"clear:left;\"></div>\n",
    "<img src = \"images/lesson_21_mtm.png\" align = 'left'  width = 400, heaight = 400> \n",
    "<img src = \"images/lesson_21_mtm1.png\" align = 'left'  width = 400, heaight = 400> \n",
    "\n",
    "\n",
    "<img src = \"images/lesson_21_ed.png\" align = 'left'  width = 400, heaight = 400> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Language model\n",
    "\n",
    "</font>\n",
    "\n",
    "Language model is a probability distribution over sequences of words.\n",
    "<br>It provides ability to estimate the relative likelihood of similar words and phrases e.g.<br>\n",
    "$\\quad P(\\text{\"recognize speech\"}) \\quad \\text{and} \\quad P(\\text{\"wreck a nice beach\"}) $\n",
    "\n",
    "Usually the task is to compare probability of different words following by some context of $n$ previous words ($n$-gram)  e.g. $\\quad P(\\text{\"beings\"} | \\text{\"we are human\"}) \\quad \\text{and} \\quad P(\\text{\"beans\"} | \\text{\"we are human\"}) $\n",
    "\n",
    "RNN is one of implementations of Language model.\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "### Implementation steps\n",
    "\n",
    "</font>\n",
    "\n",
    "1. Get corpus (large text)\n",
    "2. Tokenize and build vocabulary of tokens (make sure to have + `\"UNK\"` for words out of vocabulary  and `\"EOS\"` to denote end of sentence)\n",
    "3. Prepare training set by tokenizing to sentence. \n",
    "If the sentence is `Pluto is the largest dwarf planet in the Solar System`, then \n",
    "\n",
    "<img src = \"images/lesson_21_lm1.png\" align = 'left' width = 800, heaight = 800> \n",
    "<div style=\"clear:left;\"></div>\n",
    "4. Compute cost \n",
    "$\\mathcal {L} = \\sum \\limits_{i=1}^{m} \\sum \\limits_{t=1}^{T_{X_{i}}} y_{i}^{<t>} log \\,\\hat y_{i}^{<t>} $\n",
    "<br>Note: if the task is to predict the only following word after $n$-gram context then cost msy include the only last output $\\mathcal {L} = \\sum \\limits_{i=1}^{m}  y_{i}^{<T_{y_{i}}>} log \\,\\hat y_{i}^{<T_{y_{i}}>} $\n",
    "<br>5. Run optimization. <br>\n",
    "6. Having trained model, all $\\hat y^{<1>}, \\hat y^{<2>},... \\hat y^{<T_{y}>}$ are probabilty distributions over vocabulary tokens (outputs of corresponding softmax layer) \n",
    "<br>So you may compute $P$(\"Pluto\") from $\\hat y^{<1>}$, $P$(\"is\"|\"Pluto\") from $\\hat y^{<2>}$ and so get the probability of phrase to compare with another phrase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "### Sampling\n",
    "\n",
    "</font>\n",
    "\n",
    "Sampling is performed being based on probability distribution (outputs of corresponding softmax layer).\n",
    "<br>You may configure how much strictly to sample the word - from selecting the most probable word to consider all words with the same probability and as the result pick random word from all vocabulary.\n",
    "<img src = \"images/lesson_21_sampling.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "Note: \n",
    "\n",
    "- You need to resample in case of \"UNK\" is generated.\n",
    "- You may stop sampling by setting the limit in case of \"EOS\" is not generated for long.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Sample 1: Language model using numpy  \n",
    "\n",
    "</font>\n",
    "\n",
    "Character level language model is implemented as RNN for dinosaurus names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# cwd= os.getcwd()\n",
    "# path = os.path.join(cwd,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters number = 19,909 \n",
      " Unique characters numver = 27.\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "Training set samples number = 1,536\n",
      "Iteration: 0, Loss: 39.548882\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 9.605753\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 22.380867\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 20.168279\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 39.618127\n",
      "\n",
      "Meustratipontaros\n",
      "Infaadps\n",
      "Jvstolopon\n",
      "Macalosaurus\n",
      "Ytrrasaurus\n",
      "Daaiselaa\n",
      "Trohia\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 23.773308\n",
      "\n",
      "Nlyussaurus\n",
      "Lmacairus\n",
      "Lytrrangosaurus\n",
      "Ngaachudcarththenyragwanosaurus\n",
      "Ystrlonlthus\n",
      "Elaeropedosaurus\n",
      "Trrangosaurus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 16.116337\n",
      "\n",
      "Nixrosaurus\n",
      "Indaagosaurus\n",
      "Kystolia\n",
      "Necahisaurus\n",
      "Yusjanasausus\n",
      "Ehaesicekatosaurus\n",
      "Usjanasaurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.960518\n",
      "\n",
      "Mitrtoma\n",
      "Inecaisaurus\n",
      "Jystlepcorawenosaurus\n",
      "Macaeskadbitholenus\n",
      "Ytrrasaurus\n",
      "Eiadosaurus\n",
      "Trocephiaumtitan\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 25.654200\n",
      "\n",
      "Matruraptor\n",
      "Incaaeratidbshuchesaurus\n",
      "Ivusanon\n",
      "Madalisaurus\n",
      "Yusidon\n",
      "Dabarnaiantasaurus\n",
      "Usilonis\n",
      "\n",
      "\n",
      "Iteration: 36000, Loss: 18.991645\n",
      "\n",
      "Meutosaurus\n",
      "Incaa\n",
      "Ivkosaurus\n",
      "Macaisauropteryx\n",
      "Yutodon\n",
      "Eiacosaurus\n",
      "Trocephalusaurasaurus\n",
      "\n",
      "\n",
      "Iteration: 40000, Loss: 18.951287\n",
      "\n",
      "Mesvosaurus\n",
      "Jigabhosaurus\n",
      "Kurus\n",
      "Mafahosaurus\n",
      "Yusthomoraverevarcherochus\n",
      "Ehahosaurus\n",
      "Trogomingvenktargilos\n",
      "\n",
      "\n",
      "Iteration: 44000, Loss: 16.567862\n",
      "\n",
      "Mixpsidon\n",
      "Indabasaurus\n",
      "Ixus\n",
      "Mecalosaurus\n",
      "Yushanhosaurus\n",
      "Ehaeosaurus\n",
      "Trohonosaurus\n",
      "\n",
      "\n",
      "Iteration: 48000, Loss: 37.365335\n",
      "\n",
      "Mitrtonia\n",
      "Krecaeropeciusameotopavenos\n",
      "Lytyrangosaurus\n",
      "Mdabesodherus\n",
      "Ytromilophylus\n",
      "Ehadruradosaurus\n",
      "Trraptor\n",
      "\n",
      "\n",
      "Iteration: 52000, Loss: 20.212239\n",
      "\n",
      "Meutrocomisaurus\n",
      "Iracaesteia\n",
      "Ixuspandor\n",
      "Macaiselacosaurus\n",
      "Xtrocheosaurus\n",
      "Edantosaurus\n",
      "Streosaurus\n",
      "\n",
      "\n",
      "Iteration: 56000, Loss: 24.979204\n",
      "\n",
      "Miutosaurus\n",
      "Jiegansngeantitan\n",
      "Kystnirasegugnosaurus\n",
      "Mecamnorachussanites\n",
      "Yusokicorax\n",
      "Eiakosaurus\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "Iteration: 60000, Loss: 27.051994\n",
      "\n",
      "Jiwusraptor\n",
      "Ingaalosaurus\n",
      "Ivtyraptor\n",
      "Kecaerope\n",
      "Yrosaurus\n",
      "Egainophamus\n",
      "Trochenonygosaurus\n",
      "\n",
      "\n",
      "Iteration: 64000, Loss: 15.510556\n",
      "\n",
      "Mivusaurus\n",
      "Inca\n",
      "Ivusaurus\n",
      "Mecaishadansaurus\n",
      "Yuskimesaurus\n",
      "Eiahmon\n",
      "Strinesaurus\n",
      "\n",
      "\n",
      "Iteration: 68000, Loss: 23.407519\n",
      "\n",
      "Matytlalennixernuchongosaurus\n",
      "Inga\n",
      "Iustrion\n",
      "Madalosaurus\n",
      "Xystbmironykus\n",
      "Eiaiosaurus\n",
      "Strephoreus\n",
      "\n",
      "\n",
      "Iteration: 72000, Loss: 19.921516\n",
      "\n",
      "Louskollaosaurus\n",
      "Hkegagosaurus\n",
      "Hyrosaurus\n",
      "Lecakosaurus\n",
      "Xtosaurus\n",
      "Caaeropaceratops\n",
      "Trodoloraurosaurus\n",
      "\n",
      "\n",
      "Iteration: 76000, Loss: 36.867525\n",
      "\n",
      "Miuuvringosaurus\n",
      "Isanalosaurus\n",
      "Itotodon\n",
      "Mbaaisondelusaurus\n",
      "Xustanisaurus\n",
      "Eeagpihaceratops\n",
      "Strareplevhospanosaurestevelalelta\n",
      "\n",
      "\n",
      "Iteration: 80000, Loss: 21.056122\n",
      "\n",
      "Lctytoloptor\n",
      "Inceaisaurasteus\n",
      "Ivuysaurus\n",
      "Lacaersaurus\n",
      "Wotrephorgylopteryx\n",
      "Ecadropedtops\n",
      "Strepion\n",
      "\n",
      "\n",
      "Iteration: 84000, Loss: 23.174231\n",
      "\n",
      "Miutrodon\n",
      "Ingcamosaurus\n",
      "Ivurops\n",
      "Mecamosaurus\n",
      "Xtroceratosaurus\n",
      "Eg\n",
      "Stoengosaurus\n",
      "\n",
      "\n",
      "Iteration: 88000, Loss: 15.685819\n",
      "\n",
      "Miutus\n",
      "Kolaaiosaurus\n",
      "Lytroinasaurus\n",
      "Ngbakus\n",
      "Wtruinisaurus\n",
      "Gcagosaurus\n",
      "Strion\n",
      "\n",
      "\n",
      "Iteration: 92000, Loss: 19.156647\n",
      "\n",
      "Motrosaurus\n",
      "Isalanosaurus\n",
      "Iwusaurus\n",
      "Mecantus\n",
      "Vrosaurus\n",
      "Ehalosaurus\n",
      "Sthenlosaurus\n",
      "\n",
      "\n",
      "Iteration: 96000, Loss: 22.589076\n",
      "\n",
      "Meutroceratops\n",
      "Isalaesria\n",
      "Itrosaurus\n",
      "Macaiseia\n",
      "Wrotaodon\n",
      "Eg\n",
      "Sqoceratops\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
    "    print('%s' % (txt,), end='')\n",
    "\n",
    "\n",
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x) * 0.01  \n",
    "    Waa = np.random.randn(n_a, n_a) * 0.01  \n",
    "    Wya = np.random.randn(n_y, n_a) * 0.01  \n",
    "    b = np.zeros((n_a, 1))  \n",
    "    by = np.zeros((n_y, 1))  \n",
    "\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)  # hidden state\n",
    "    p_t = softmax(\n",
    "        np.dot(Wya, a_next) + by)  # unnormalized log probabilities for next chars # probabilities for next chars\n",
    "\n",
    "    return a_next, p_t\n",
    "\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next']  # backprop into h\n",
    "    daraw = (1 - a * a) * da  # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b'] += -lr * gradients['db']\n",
    "    parameters['by'] += -lr * gradients['dby']\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size=27):\n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "\n",
    "    a[-1] = np.copy(a0)\n",
    "\n",
    "    # initialize your loss to 0\n",
    "    loss = 0\n",
    "\n",
    "    for t in range(len(X)):\n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.        \n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "\n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t - 1], x[t])\n",
    "\n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "\n",
    "    cache = (y_hat, a, x)\n",
    "\n",
    "    return loss, cache\n",
    "\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "\n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "\n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "\n",
    "    # Backpropagate\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t - 1])\n",
    "\n",
    "\n",
    "    return gradients, a\n",
    "\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "\n",
    "    Arguments:\n",
    "    gradients - a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue - everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "\n",
    "    Returns:\n",
    "    gradients - a dictionary with the clipped gradients.\n",
    "    '''\n",
    "\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients[\n",
    "        'dby']\n",
    "    # clip to mitigate exploding gradients\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b.\n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "\n",
    "    # Create the vector x for the first character (initializing the sequence generation)\n",
    "    x = np.zeros((vocab_size, 1)) \n",
    "    # Initialize a_prev as zeros\n",
    "    a_prev = np.zeros(shape=(n_a, 1))\n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate\n",
    "    indices = []\n",
    "\n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1  # this is just to pass the first while condition\n",
    "\n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append\n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "\n",
    "    while (idx != newline_character and counter != 50):\n",
    "        # Forward propagate \n",
    "        a = np.tanh(Wax @ x + Waa @ a_prev + b)\n",
    "        z = Wya @ a + by\n",
    "        y = softmax(z)\n",
    "\n",
    "        np.random.seed(counter + seed)\n",
    "\n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(vocab_size, p=y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "\n",
    "        # Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros(shape=(vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "\n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "\n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "\n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Forward propagate through time\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "\n",
    "    # Backpropagate through time \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "\n",
    "    # Clip your gradients between -5 (min) and 5 (max) \n",
    "    gradients = clip(gradients, 5)\n",
    "\n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "    return loss, gradients, a[len(X) - 1]\n",
    "\n",
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations=100000, n_a=50, dino_names=7, vocab_size=27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names.\n",
    "\n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration.\n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "\n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(fn) as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    print ('Training set samples number = {:,}'.format (len(examples)))\n",
    "\n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "\n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "\n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters        \n",
    "        loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=0.01)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 4000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "\n",
    "                seed += 1  \n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# execution ===\n",
    "\n",
    "fn = 'dinos.txt'\n",
    "\n",
    "data = open(fn, 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('Total characters number = {:,} \\n Unique characters numver = {}.'.format (data_size, vocab_size))\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix )\n",
    "\n",
    "# Training the model¶\n",
    "parameters = model(data, ix_to_char, char_to_ix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Gated Recurrent Unit (GRU)\n",
    "\n",
    "</font>\n",
    "\n",
    "It introduses gates $\\mathcal{\\Gamma}_{r}$ (as responsible for relevancy) and $\\mathcal{\\Gamma}_{u}$ (as responsible for update) and additional computation of $\\tilde {C\\,}^{<t>}$ as candidate to update the input activation $A^{<t-1>}$\n",
    "\n",
    "$$C^{<t-1>} = A^{<t-1>}\\\\\n",
    "\\mathcal{\\Gamma}_{r} = \\sigma (W_{rc} @ C^{<t-1>} + W_{rx} @ {x\\,}^{<t>} + b_{r}) \\\\\n",
    "\\mathcal{\\Gamma}_{u} = \\sigma (W_{uc} @ C^{<t-1>} + W_{ux} @ {x\\,}^{<t>} + b_{u}) \\\\\n",
    "\\tilde {C\\,}^{<t>} = tanh (W_{cc} @  {C\\,}^{<t-1>} \\cdot \\mathcal{\\Gamma}_{r}  + W_{cx} @ {x\\,}^{<t>} + b_{c})\\\\\n",
    "{A\\,}^{<t>}  = {C\\,}^{<t>} = \\mathcal{\\Gamma}_{u}  \\cdot \\tilde {C\\,}^{<t>} + (1-\\mathcal{\\Gamma}_{u})\\cdot  {C\\,}^{<t-1>}\\\\ \\text{ }\\\\\n",
    "y^{<t>}= softmax (W_{ay} @ A^{<t>} + b_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "</font>\n",
    "It introduces separate line of activation units $C^{<t>}$ and uses 3 gates:\n",
    "\n",
    "- $\\mathcal{\\Gamma}_{u}$ (update) \n",
    "- $\\mathcal{\\Gamma}_{f}$ (forget) \n",
    "- $\\mathcal{\\Gamma}_{o}$ (output) \n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/LSTM.png\" align = 'left' width = 800, heaight = 800> \n",
    "<img src = \"images/LSTM_2.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "$\\quad y^{<t>}= softmax (W_{ay} @ A^{<t>} + b_{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Bidirectional\n",
    "\n",
    "</font>\n",
    "Computation is performed in 2 directions and then both activations values $A_{right}^{<t>}$ and $A_{left}^{<t>}$ of the same time $t$ are considered for $y^{<t>}$:\n",
    "\n",
    "<img src = \"images/lesson_21_bidirectional2.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "$\\quad y^{<t>}= g (W_{ay \\,right} @ A_{right}^{<t>} +  W_{ay \\,left} @ A_{left}^{<t>} + b_{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Deep RNN\n",
    "\n",
    "</font>\n",
    "<img src = \"images/lesson_21_deep_rnn1.png\" align = 'left' width = 400, heaight = 400> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Sample 2: language model using tensorflow\n",
    "\n",
    "</font>\n",
    "\n",
    "Word level language model is implemented as 2layer bidirectional LSTM \n",
    "\n",
    "#### Note: See [colab](https://drive.google.com/file/d/1xeYVoCOiO6GUANGb4GF0d0V0fKxa_9WP/view?usp=sharing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'data') \n",
    "save_model_path =  os.path.join(path , 'model_rnn.ckpt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/_vz5kwcn2bxbjntlwb3gq_ph0000gp/T/ipykernel_3010/2397038525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "v_9\n",
    "\n",
    "- bidirectional layers implemented (using native rnn.stack_bidirectional_dynamic_rnn)\n",
    "- Multi bidirectional layers implemented\n",
    "\n",
    "\n",
    "v_8\n",
    "\n",
    "- the bidirectional manually as two lstm\n",
    "- uses two separated inputs contexts: x_fw, x_bw\n",
    "- uses two separated parameters:  and just add the results at the last (output layer)\n",
    "\n",
    "v_7\n",
    "\n",
    "- bidirectional\n",
    " 1) input is input+ target+ input e.g. len of 9 (4+1+4 ) vector\n",
    " 2) it uses the only layer of\n",
    " 3) use the rnn bidirectional\n",
    "\n",
    "v_6:\n",
    "- minibatch  [General Accuracy: 88.0%]\n",
    "\n",
    "v_5\n",
    "- sampling (no just argmax )\n",
    "-  2 layers (lstm)\n",
    "-   from graph\n",
    "\n",
    "v_4\n",
    "- saving restoring models from scratch\n",
    "\n",
    "v_3\n",
    "- refacrtored \n",
    " \n",
    "\n",
    "v_2\n",
    "    Code is based on sample:\n",
    "    https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537\n",
    "\n",
    "    it considers 3 words (n_input)\n",
    "    the input data is 3 integers NOT the one-hot vector\n",
    "    but output data is probabilities distribution vector\n",
    "\n",
    "    The dimensions are modified comparing to ones in the sample due to no need of redundant dimension\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from string import ascii_lowercase\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    file_name =  os.path.join(path , 'belling_the_cat.txt') \n",
    "    ''':returns ndarray of words  '''\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()] # currently there is only line but in case of multi line...\n",
    "    content.append('_target_')\n",
    "    content = np.array(content)\n",
    "    print('Words amount: {} '.format(len(content)-1))\n",
    "    word_ind, ind_word = build_dataset(content)\n",
    "    vocab_size = len(word_ind)  # n_x\n",
    "    print('Vocab_size : {:,}'.format(vocab_size))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    cursor = 0\n",
    "    while cursor + 2 * n_input < len(content)-1: # -1 = due to added the target word to vocabulary\n",
    "        x_context_left = [word_ind[str(content[i])] for i in range(cursor, cursor + n_input)]\n",
    "        x_context_right = [word_ind[str(content[i])] for i in range(cursor+n_input+1, cursor + 2 * n_input+1)]\n",
    "\n",
    "        x_context = x_context_left + [ word_ind['_target_']] + x_context_right\n",
    "        X.append(x_context)\n",
    "\n",
    "        y_onehot = np.zeros([vocab_size], dtype=float)\n",
    "\n",
    "        y_onehot[word_ind[str(content[cursor + n_input])]] = 1.0  # next word\n",
    "\n",
    "        Y.append(y_onehot)\n",
    "        cursor += 1\n",
    "    X= np.array(X)\n",
    "    X= np.reshape(X, [X.shape[0], X.shape[1], 1]) # required by stack_bidirectional_dynamic_rnn - the axis for features\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    print('Training set len: {:,}'.format(X.shape[0]))\n",
    "\n",
    "    return X, Y, word_ind, ind_word\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    '''words -  the list of words '''\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(n_hidden, vocab_size):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "\n",
    "    \"\"\"\n",
    "    W = tf.get_variable(\"W\", [n_hidden *2 , vocab_size], initializer=tf.contrib.layers.xavier_initializer(seed=1)) # to use tf.nn.static_bidirectional_rnn which retutn outputs  - a length T list of outputs (one for each input), which are depth-concatenated forward and backward outputs\n",
    "    b = tf.get_variable(\"b\", [vocab_size], initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def create_placeholders(n_input, vocab_size):\n",
    "    # X[batch_size, n_context]  - Note: it will be splitted by last dimension to pass to RNN - the value represented is the int number the number in vocab\n",
    "\n",
    "    # Let's add 1 as the last dimension which means there is only 1 feature - you may use it to pass one hot vector or whatever features\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_input * 2 +1, 1], name='X')  # batch_size, context, single id in vocabulary (last layer is added due to required by tf layer)\n",
    "\n",
    "    # Y [batch_size, len_of_vocabulary]  - Note: we need to use it as one-hot vector to calculate the cross entropy cost\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, vocab_size] , name='Y')  #  one-hot vector\n",
    "    return (X,Y)\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters, n_hidden, layers=2):\n",
    "\n",
    "    # looks as batch size = 1\n",
    "\n",
    "    print('\\nShape of input =  {}'.format(X))\n",
    "\n",
    "\n",
    "    # Note: the following is required for static_bidirectional_rnn but not for stack_bidirectional_dynamic_rnn !!!\n",
    "    # X = tf.split(X, (n_input* 2+ 1) , axis=-1)  #  default axis=0\n",
    "\n",
    "    cells_fw = [tf.nn.rnn_cell.LSTMCell(num_units=n_hidden, state_is_tuple=True) for i in range(layers)]\n",
    "    cells_bw = [tf.nn.rnn_cell.LSTMCell(num_units=n_hidden, state_is_tuple=True) for i in range(layers)]\n",
    "\n",
    "    outputs, output_state_fw, output_state_bw =  rnn.stack_bidirectional_dynamic_rnn(cells_fw= cells_fw, cells_bw= cells_bw, inputs= X, dtype=tf.float32) # returns a tuple (outputs, output_state_fw, output_state_bw) where: * outputs: Output Tensor shaped: batch_size, max_time, layers_output]. Where layers_output are depth-concatenated forward and backward outputs.\n",
    "\n",
    "    # tf.add(tf.matmul(outputs[:, n_input+1], parameters['W']), parameters['b'])\n",
    "\n",
    "    Z = tf.nn.softmax(tf.add(tf.matmul(outputs[:, n_input+1], parameters['W']), parameters['b']),         name='Z')\n",
    "    print('\\nOutput of RNN (\"Z\") =  {}'.format(Z))\n",
    "    return Z\n",
    "\n",
    "\n",
    "def compute_cost(Z, Y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "        e.g. value 2.2901573\n",
    "    \"\"\"\n",
    "    # Make sure sum over rows = 1\n",
    "    # labels = np.array(Y, dtype=np.float32)\n",
    "    # labels_sum = tf.reduce_sum(labels, axis=-1)\n",
    "    # print(labels_sum)\n",
    "\n",
    "\n",
    "    cost_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z, labels=Y))\n",
    "    # Defaulted  axis is -1 which is the last dimension.\n",
    "    # each row of labels[i] must be a valid probability distribution\n",
    "\n",
    "    return cost_cross_entropy\n",
    "\n",
    "\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):  # todo\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, n_context)\n",
    "    Y -- true \"label\" , of shape (number of examples, vocab_size )\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :] #.reshape((Y.shape[0], m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(\n",
    "        m / mini_batch_size)  # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size: k * mini_batch_size + mini_batch_size:, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size: m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size: m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def model(data, save_model_path, learning_rate=0.001, n_input = 3, n_hidden=512, num_epochs = 500, minibatch_size = 32, layers= 2):\n",
    "    '''\n",
    "    :param data:\n",
    "    :param learning_rate:\n",
    "    :param n_input: int - len of context\n",
    "    :param n_hidden: int - number of units in RNN cell\n",
    "    :param training_iters: int\n",
    "    :param display_step: int\n",
    "    :param save_model_path: str\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    tf.reset_default_graph()  # to be able to rerun the model without overwriting tf variables - actually I dont understand what it is for\n",
    "\n",
    "    X_train, Y_train, word_ind, ind_word = data\n",
    "    m, vocab_size = Y_train.shape # len(word_ind)\n",
    "\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_hidden, vocab_size)\n",
    "\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_input, vocab_size)\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z = forward_propagation(X, parameters,n_hidden, layers)\n",
    "\n",
    "    cost = compute_cost(Z, Y)\n",
    "\n",
    "    # costs = []  # To keep track of the cost\n",
    "\n",
    "    # Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "    # Model evaluation\n",
    "    correct_pred = tf.equal(tf.argmax(Z, -1), tf.argmax(Y, -1))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name = 'Acc')\n",
    "\n",
    "    # use to save model\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    seed = 3  # to keep consistent results\n",
    "    costs = []  # To keep track of the cost\n",
    "\n",
    "    print('\\nTrainable variables: ')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        vars = tf.trainable_variables()\n",
    "        for var in vars:\n",
    "            print(var)\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    print('\\nTraining model...')\n",
    "    with tf.Session() as sess:\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(1,num_epochs+1): # to avoid print at first iter\n",
    "            epoch_cost = 0.  # Defines a cost related to an epoch\n",
    "            acc_epoch = 0\n",
    "            # acc_troubleshooting = []\n",
    "            num_minibatches = math.ceil(m / minibatch_size)\n",
    "\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _, acc, minibatch_cost = sess.run([optimizer, accuracy, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                # acc_troubleshooting.append(acc)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "                acc_epoch += acc / num_minibatches\n",
    "\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if  epoch % 10 == 0:\n",
    "                print(\"\\n===\\nCost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                print(\"Average Accuracy (during performance of last epoch)= {:.1%}\".format(acc_epoch))\n",
    "                # print (acc_troubleshooting)\n",
    "\n",
    "                general_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "                print(\"General Accuracy: {:.1%}\".format (general_accuracy))\n",
    "\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Save the variables to disk\n",
    "        save_path = saver.save(sess, save_model_path)\n",
    "        print(\"Model saved in path: {}\".format(save_path))\n",
    "\n",
    "        print ('\\nCheck at saving')\n",
    "        print(sess.run('W:0'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_input = 5\n",
    "n_hidden = 64\n",
    "minibatch_size=2\n",
    "layers= 1\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "\n",
    "model(data, save_model_path=save_model_path, learning_rate=0.001, n_input=n_input, n_hidden=n_hidden,\n",
    "      num_epochs=num_epochs, minibatch_size=minibatch_size, layers=layers)\n",
    "\n",
    "\n",
    "\n",
    "# n_input = 5\n",
    "# n_hidden = 64\n",
    "# minibatch_size=4\n",
    "# layers= 1\n",
    "# num_epochs = 1000\n",
    "# is_to_train = 1\n",
    "\n",
    "# results: \n",
    "\n",
    "# Cost after epoch 700: 3.757871\n",
    "# Average Accuracy (during performance of last epoch)= 98.5%\n",
    "# General Accuracy: 98.5%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "v_6\n",
    "\n",
    "Implemented :\n",
    "    minibatch\n",
    "\n",
    "'''\n",
    "\n",
    "from string import ascii_lowercase\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    file_name =  os.path.join(path , 'belling_the_cat.txt') \n",
    "    ''':returns ndarray of words ,  '''\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()] # currently there is only line but in case of multi line...\n",
    "    content = np.array(content)\n",
    "    print('Words amount: {} '.format(len(content)))\n",
    "    word_ind, ind_word = build_dataset(content)\n",
    "    vocab_size = len(word_ind)  # n_x\n",
    "    print('Vocab_size : {:,}'.format(vocab_size))\n",
    "\n",
    "    X = []\n",
    "    Y= []\n",
    "    cursor = 0\n",
    "    while cursor + n_input < len(content): # exclude the last since we get content[cursor + n_input] for y\n",
    "        x_context = [word_ind[str(content[i])] for i in range(cursor, cursor + n_input)]\n",
    "        # x_context = np.reshape(x_context, [1, -1])\n",
    "        X.append(x_context)\n",
    "\n",
    "        y_onehot = np.zeros([vocab_size], dtype=float)\n",
    "\n",
    "        y_onehot[word_ind[str(content[cursor + n_input])]] = 1.0  # next word\n",
    "\n",
    "        Y.append(y_onehot)\n",
    "        cursor += 1\n",
    "    X= np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    print('Training set len: {:,}'.format(X.shape[0]))\n",
    "\n",
    "    return X,Y, word_ind, ind_word\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    '''words -  the list of words '''\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(n_hidden, vocab_size):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: configure the output layer whereas the parameters of rnn are declared automatically\n",
    "    W = tf.get_variable(\"W\", [n_hidden, vocab_size], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b = tf.get_variable(\"b\", [vocab_size], initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def create_placeholders(n_input, vocab_size):\n",
    "    # X[batch_size, n_context]  - Note: it will be splitted by last dimension to pass to RNN - the value represented is the int number the number in vocab\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_input], name='X')  # note this is not the one-hot vector - first is the number of words to consider, another dim is the len of vector - is 1 because  we pass the integer value for every word\n",
    "    # Y [batch_size, len_of_vocabulary]  - Note: we need to use it as one-hot vector to calculate the cross entropy cost\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, vocab_size] , name='Y')  #  one-hot vector\n",
    "    return (X,Y)\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters, n_hidden, layers=2):\n",
    "\n",
    "    # looks as batch size = 1\n",
    "\n",
    "    print('\\nShape of input =  {}'.format(X))\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "\n",
    "    X = tf.split(X, n_input, axis=-1)  #  default axis=0\n",
    "    # print(X) # [<tf.Tensor 'split:0' shape=(1, 1) dtype=float32>, <tf.Tensor 'split:1' shape=(1, 1) dtype=float32>, <tf.Tensor 'split:2' shape=(1, 1) dtype=float32>]\n",
    "\n",
    "    if layers == 2:\n",
    "        # 2-layer LSTM, each layer has n_hidden units.\n",
    "        # Average Accuracy= 95.20% at 50k iter\n",
    "\n",
    "        rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "        # general sample\n",
    "            # num_units = [128, 64]\n",
    "            # cells = [BasicLSTMCell(num_units=n) for n in num_units]\n",
    "            # stacked_rnn_cell = MultiRNNCell(cells)\n",
    "\n",
    "    elif layers == 1 :\n",
    "        # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "        # Average Accuracy= 90.60% 50k iter\n",
    "        # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above\n",
    "        rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    else: # could be used for case 2\n",
    "        # general sample\n",
    "        # num_units = [n_hidden for i in range(layers)]\n",
    "        cells = [rnn.BasicLSTMCell(num_units=n_hidden) for i in range(layers)]\n",
    "        rnn_cell = rnn.MultiRNNCell(cells)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, X, dtype=tf.float32)  # inputs: A length T list of inputs, each a Tensor of shape [batch_size, input_size], or a nested tuple of such elements.\n",
    "    # print(outputs, states)\n",
    "\n",
    "    # there are n_input outputs but  we only want the last output\n",
    "    # Z = tf.matmul(outputs[-1], parameters['W']) + parameters['b']\n",
    "    Z = tf.nn.softmax(tf.add (tf.matmul(outputs[-1], parameters['W']), parameters['b']), name='Z')\n",
    "    print('\\nOutput of RNN (\"Z\") =  {}'.format(Z))\n",
    "    return Z\n",
    "\n",
    "\n",
    "def compute_cost(Z, Y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "        e.g. value 2.2901573\n",
    "    \"\"\"\n",
    "    # Make sure sum over rows = 1\n",
    "    # labels = np.array(Y, dtype=np.float32)\n",
    "    # labels_sum = tf.reduce_sum(labels, axis=-1)\n",
    "    # print(labels_sum)\n",
    "\n",
    "    # logits = tf.transpose(Z)  # use if necessary\n",
    "    # labels = tf.transpose(Y)\n",
    "\n",
    "    cost_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z, labels=Y))\n",
    "    # Defaulted  axis is -1 which is the last dimension.\n",
    "    # each row of labels[i] must be a valid probability distribution\n",
    "\n",
    "    return cost_cross_entropy\n",
    "\n",
    "\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):  # todo\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, n_context)\n",
    "    Y -- true \"label\" , of shape (number of examples, vocab_size )\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :] #.reshape((Y.shape[0], m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(\n",
    "        m / mini_batch_size)  # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size: k * mini_batch_size + mini_batch_size:, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size: m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size: m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def model(data, save_model_path, learning_rate=0.001, n_input = 3, n_hidden=512, num_epochs = 500, minibatch_size = 32, layers= 2):\n",
    "    '''\n",
    "    :param data:\n",
    "    :param learning_rate:\n",
    "    :param n_input: int - len of context\n",
    "    :param n_hidden: int - number of units in RNN cell\n",
    "    :param training_iters: int\n",
    "    :param display_step: int\n",
    "    :param save_model_path: str\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    tf.reset_default_graph()  # to be able to rerun the model without overwriting tf variables - actually I dont understand what it is for\n",
    "\n",
    "    X_train, Y_train, word_ind, ind_word = data\n",
    "    m, vocab_size = Y_train.shape # len(word_ind)\n",
    "\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_hidden, vocab_size)\n",
    "\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_input, vocab_size)\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z = forward_propagation(X, parameters,n_hidden, layers)\n",
    "\n",
    "    cost = compute_cost(Z, Y)\n",
    "\n",
    "    # costs = []  # To keep track of the cost\n",
    "\n",
    "    # Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "    # Model evaluation\n",
    "    correct_pred = tf.equal(tf.argmax(Z, -1), tf.argmax(Y, -1))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name = 'Acc')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # use to save model\n",
    "    saver = tf.train.Saver()\n",
    "    # tf.add_to_collection('Z', Z)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    seed = 3  # to keep consistent results\n",
    "    costs = []  # To keep track of the cost\n",
    "\n",
    "    print('\\nTrainable variables: ')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        vars = tf.trainable_variables()\n",
    "        for var in vars:\n",
    "            print(var)\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    print('\\nTraining model...')\n",
    "    with tf.Session() as sess:\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(1,num_epochs+1): # to avoid print at first iter\n",
    "            epoch_cost = 0.  # Defines a cost related to an epoch\n",
    "            acc_epoch = 0\n",
    "            acc_troubleshooting = []\n",
    "            num_minibatches = math.ceil(m / minibatch_size)\n",
    "            # print('num_minibatches :', num_minibatches )\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _, acc, minibatch_cost = sess.run([optimizer, accuracy, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                acc_troubleshooting.append(acc)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "                acc_epoch += acc / num_minibatches\n",
    "\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if  epoch % 10 == 0:\n",
    "                print(\"\\n===\\nCost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                print(\"Average Accuracy (during performance of last epoch)= {:.1%}\".format(acc_epoch))\n",
    "                print (acc_troubleshooting)\n",
    "\n",
    "                general_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "                print(\"General Accuracy: {:.1%}\".format (general_accuracy))\n",
    "\n",
    "                print_predictions(sess, X, Z, X_train, Y_train, word_ind, ind_word, n_input, number=50)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(sess, save_model_path)\n",
    "        print(\"Model saved in path: {}\".format(save_path))\n",
    "\n",
    "        print ('\\nCheck at saving')\n",
    "        print(sess.run('W:0'))\n",
    "\n",
    "\n",
    "\n",
    "def print_predictions(sess, X, Z, X_train, Y_train, word_ind, ind_word, n_input, start=0, number = None):\n",
    "\n",
    "    accuracy=[]\n",
    "    cursor = start\n",
    "    if not number:\n",
    "        end = len(X_train)\n",
    "    else:\n",
    "        end = min(len(X_train), number + start)  # to cover al\n",
    "    print('\\nDemonstrate the predictions for contexts in range ({}, {}):'.format(start, end))\n",
    "    while cursor < end:\n",
    "        # x_context = [word_ind[str(content[i])] for i in range(cursor, cursor + n_input)]\n",
    "        # x_context = np.reshape(x_context, [1, -1])\n",
    "\n",
    "        x_batch = X_train[cursor].reshape(1,-1)\n",
    "        pred_onehot = sess.run(Z, feed_dict={X: x_batch})\n",
    "        pred_index = int(tf.argmax(pred_onehot, 1).eval())\n",
    "        pred_word = ind_word[pred_index]\n",
    "\n",
    "        true_index = np.argmax(Y_train[cursor])\n",
    "        true_word = ind_word[true_index]\n",
    "        print('{} -> {} [{}]'.format(' '.join(ind_word[i] for i in X_train[cursor]), pred_word, true_word))\n",
    "        cursor += 1\n",
    "        accuracy.append(pred_index == true_index)\n",
    "    print('\\nFactual Accuracy for range of {} contexts = {:.1%}'.format(number, np.average(accuracy)))\n",
    "\n",
    "\n",
    "def generate_sample_index(pred_tf):\n",
    "    '''\n",
    "    :param pred is tensor\n",
    "    :return: tf - index of sampled word\n",
    "    Note : here is all operations ared performed with tensors not with np objects\n",
    "    need to eval to get int value\n",
    "    '''\n",
    "\n",
    "    # sampling due to probability distribution\n",
    "    pred_tf = tf.squeeze(pred_tf) # to exclude dimensionals of 1 - could be resolved by reshape\n",
    "    dist = tf.distributions.Multinomial(total_count=1., probs = pred_tf)  # response with larger dimension - first dumension is dedicated for number of samples\n",
    "    # here could be provided the ndarray or list e.g.   p = [.2, .3, .5]\n",
    "\n",
    "    sample_tf = dist.sample(1)\n",
    "    return tf.argmax(sample_tf, axis=1)[0] # since the sample_tf is tf of size 2 tf.argmax returns the array of size 1 - to get value we take first element\n",
    "\n",
    "\n",
    "\n",
    "def use_trained_model(save_model_path, data):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X_train, Y_train, word_ind, ind_word = data\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # sess.run(init)\n",
    "        save_graph_path =  save_model_path+ '.meta'\n",
    "        saver = tf.train.import_meta_graph(save_graph_path)\n",
    "\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, save_model_path)\n",
    "        print(\"\\nModel restored.\")\n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        X = graph.get_tensor_by_name(\"X:0\")\n",
    "\n",
    "        # print('\\nCheck at restoring')\n",
    "        # print(sess.run(W))\n",
    "\n",
    "        Z = graph.get_tensor_by_name(\"Z:0\")\n",
    "\n",
    "        Y = graph.get_tensor_by_name(\"Y:0\")\n",
    "        accuracy=  graph.get_tensor_by_name(\"Acc:0\")\n",
    "\n",
    "        general_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        print(\"General Accuracy: {:.1%}\".format(general_accuracy))\n",
    "\n",
    "\n",
    "        #  lets predict for every context:\n",
    "        # print_predictions(sess, X, Z, X_train, Y_train, word_ind, ind_word, n_input ) # , number=10)\n",
    "\n",
    "        is_continue  =  True\n",
    "\n",
    "        while is_continue:\n",
    "            prompt = \"\\nProvide {} words (leave blank to generate from next words of text): \".format(n_input)\n",
    "            sentence = input(prompt)\n",
    "            sentence = sentence.strip()\n",
    "            words = sentence.split(' ')\n",
    "            if len(words) > n_input:\n",
    "                words = words[:n_input]\n",
    "                print('It will use first {} words: {}\\n'.format(n_input, words))\n",
    "\n",
    "            x_batch = [word_ind[str(words[i])] for i in range(len(words))]\n",
    "            sentence+=' -> '\n",
    "\n",
    "            for i in range(200):\n",
    "                x_batch = np.reshape(x_batch, [1, -1]) # make sure it is row\n",
    "\n",
    "                sample_ind_tf = generate_sample_index(Z)\n",
    "                sample_ind = sess.run(sample_ind_tf, feed_dict={X:x_batch})\n",
    "                # print ('Sampling: ', ind_word[sample_ind])\n",
    "\n",
    "                pred_index = int(tf.argmax(Z, axis = 1).eval(feed_dict={X:x_batch}))\n",
    "                if pred_index!= sample_ind:\n",
    "                    print ('Sampling {} [argmax: {}]'.format( ind_word[sample_ind], ind_word[pred_index]))\n",
    "                else:\n",
    "                    print('Sampling {}'.format(ind_word[sample_ind]))\n",
    "                sentence = \"{} {}\".format (sentence,ind_word[sample_ind])\n",
    "                x_batch = x_batch[:,1:] # exclude fisrt word\n",
    "                x_batch = np.concatenate((x_batch, [[sample_ind]]), axis=-1) # add last word\n",
    "\n",
    "            print('\\n=====\\n',sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_input = 4 # len of context\n",
    "n_hidden = 64\n",
    "minibatch_size=4\n",
    "layers= 2\n",
    "is_to_train = 1\n",
    "data = load_data()\n",
    "\n",
    "\n",
    "if is_to_train:\n",
    "     model (data, save_model_path = save_model_path, learning_rate=0.001, n_input = n_input, n_hidden=n_hidden, num_epochs = 500, minibatch_size=minibatch_size, layers= layers)\n",
    "use_trained_model(save_model_path, data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Sample 3: language model using keras \n",
    "\n",
    "</font>\n",
    "\n",
    "Word level language model is implemented as LSTM for poem of T.Shevchenko \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/_vz5kwcn2bxbjntlwb3gq_ph0000gp/T/ipykernel_3010/2106872166.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLambdaCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "# import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'Shevchenko_T._Kobzar.txt'\n",
    "with open(fn) as f:\n",
    "    text = f.read().lower()\n",
    "# print('corpus length = {\",\"}', len(text)) \n",
    "text = re.sub(r'\\[.*\\]', \"\", text)\n",
    "text = re.sub(r'\\d+', \"\", text)\n",
    "text = re.sub(r'«|»', \"\", text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print ('Len of original text= {:,}'.format (len(text)))\n",
    "keep = 0.2\n",
    "text =  text[: int (len(text)* keep)]\n",
    "print ('Len of snippet= {:,}'.format (len(text)))\n",
    "\n",
    "print (text[:5000])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'(?u)(?:\\b\\w+\\b|\\.|\\,|\\!|\\?|\\-|\\n)').fit([text])\n",
    "# max_features= 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('len of features = {:,}\\n'.format(len(vectorizer.get_feature_names())))\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = vectorizer.vocabulary_\n",
    "# word2index['UNK'] = len(word2index)\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k,v in word2index.items()}\n",
    "index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = vectorizer.build_tokenizer()\n",
    "raw_tokens = word_tokenizer(text) \n",
    "\n",
    "tokens = sorted(set(raw_tokens))\n",
    "print('len of all tokens = {:,}'.format(len(raw_tokens)))\n",
    "print('len of unique tokens = {:,}'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_context  = 7\n",
    "step = 1 # shift to build new sample \n",
    "contexts = []\n",
    "targets = []\n",
    "for i in range(0, len(raw_tokens) - n_context, step):\n",
    "    contexts.append(raw_tokens[i: i + n_context])\n",
    "    targets.append(raw_tokens[i + n_context])\n",
    "print('len(samples) = {:,}'.format(len(contexts)))\n",
    "\n",
    "for i in range (20):\n",
    "    print ('{} -> {}'.format (contexts[i], targets[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting to one-hot vectors...')\n",
    "x = np.zeros((len(contexts), n_context, len(tokens)), dtype=np.bool)\n",
    "y = np.zeros((len(contexts), len(tokens)), dtype=np.bool)\n",
    "for i, context in enumerate(contexts):\n",
    "    for t, token in enumerate(context):\n",
    "        x[i, t, word2index[token]] = 1\n",
    "    y[i, word2index[targets[i]]] = 1\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model (single LSTM)...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(n_context, len(tokens))))\n",
    "model.add(Dense(len(tokens), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "def sample(preds, diversity=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / diversity\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.    \n",
    "    print('\\n----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(raw_tokens) - n_context - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('--- diversity:', diversity)        \n",
    "        context = raw_tokens[start_index: start_index + n_context]\n",
    "        print('--- Generated with the following context: {}'.format(context))\n",
    "        # sys.stdout.write(generated)\n",
    "        orig_context = list (context)\n",
    "        generated = []\n",
    "        for i in range(40): # number of tokens to gerenerate \n",
    "            x_pred = np.zeros((1, n_context, len(tokens)))\n",
    "            for t, token in enumerate(context):\n",
    "                x_pred[0, t, word2index[token]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = index2word[next_index]\n",
    "\n",
    "            generated += [next_word]\n",
    "            \n",
    "            # update context for next pass             \n",
    "            context = context[1:] + [next_word]\n",
    "\n",
    "        print ('{} -> {}'.format(' '.join(orig_context) , ' '.join(generated)))\n",
    "            #             sys.stdout.write(next_char)\n",
    "            #             sys.stdout.flush()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output after 60 epochs:** \n",
    "```\n",
    "Epoch 60/60\n",
    "26087/26087 [==============================] - 46s 2ms/step - loss: 2.0366\n",
    "\n",
    "----- Generating text after Epoch: 59\n",
    "--- diversity: 0.2\n",
    "--- Generated with the following context: ['то', 'и', 'стара', 'мати', ',', '\\n', 'що']\n",
    "то и стара мати , \n",
    " що -> жить на ! \n",
    " з украі ну на міи , \n",
    " \n",
    " тяжко лихо немає . \n",
    " а хто долі серед без , як пішов ? . . \n",
    " то ж ж , ж ти ж ж , і ,\n",
    "--- diversity: 0.5\n",
    "--- Generated with the following context: ['то', 'и', 'стара', 'мати', ',', '\\n', 'що']\n",
    "то и стара мати , \n",
    " що -> розумні на , а тим часом \n",
    " ярема не ножі , з боже є з козаками ? думи моі , крові . іди , де мені з нащо ? де будеш , ось ? \n",
    " не слова , \n",
    " що\n",
    "--- diversity: 1.0\n",
    "--- Generated with the following context: ['то', 'и', 'стара', 'мати', ',', '\\n', 'що']\n",
    "то и стара мати , \n",
    " що -> привела на чужині зозуленька . ! \n",
    " не пожари воля знали пии темнии , сеи згадаєш сизии один вздовж бабусю мовчіть вии запорожці , прилини втирають хмари жупани полюбила поглядає єсть золоті кличе лежить треті була де ще степ цілує\n",
    "--- diversity: 1.2\n",
    "--- Generated with the following context: ['то', 'и', 'стара', 'мати', ',', '\\n', 'що']\n",
    "то и стара мати , \n",
    " що -> навіки щиро хали щиро греблю поганці , московщину кохаи знала , весна панове ревуть . літа тихесенько \n",
    " плаче козацького таке стоя \n",
    " москаля , сховається гонта \n",
    " крові чорноброві ходімо побачиш цілує уже журись \n",
    " слухать чує неба з\n",
    "<keras.callbacks.callbacks.History at 0x1a8b797190>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Sample 2A: language model using keras \n",
    "\n",
    "</font>\n",
    "\n",
    "Character level language model is implemented as LSTM for poem of T.Shevchenko \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text into sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3 # shift to build new sample \n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('len(samples) = {:,}'.format(len(sentences)))\n",
    "\n",
    "\n",
    "print('Converting to one-hot vectors...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "print('Build model (single LSTM)...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.    \n",
    "    print('\\n----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generated: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400): # number of chars to gerenerate \n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text after Epoch: 59 (diversity: 0.5)\n",
    "<br>\n",
    "`як він, свої думи тії\n",
    "і серце убо\"\n",
    "оре.\n",
    "як він, свої думи тії\n",
    "і серце убогії горили.\n",
    "а до то прості на собі, поки не знать... а не чуть, а там діти?\n",
    "і могили погину, синіїм піде з киїти, а як і на всеї дожить.\n",
    "а ще сумуючи, поки все бордная! привела в світі та зарегожа поможем засталося. не просла сусіди, а що криниці, молоденька веселі,\n",
    "ми за то розказила\n",
    "і при другаять. а може, співає,\n",
    "і все знає,\n",
    "і дівила світі, як то плети,\n",
    "а я ж думать за що, нехай була одно`\n",
    "\n",
    "Generating text after Epoch: 59 (diversity: 1.0)\n",
    "<br>\n",
    "`а галайда, знай, гукає:\n",
    "«кари ляхам, спьшає німили: сходиться знали\n",
    "і все роззуттва сором ми по всі жали, і ти...» — коли їла.\n",
    "мов ні його співали? кричіє! роз кету, скажи мате,\n",
    "що й золобе бога, чую!\n",
    "за тій світу заборкає.\n",
    "з землих я княшіво-ховав,\n",
    "у хату підерячки, щоб великпертика й не веселі, плачуть передову. .....\n",
    "нуме понадвала стане, тому так! ея\n",
    "і деревамі,\n",
    "а точки.\n",
    "я сказать, у бав. ! тобі їй пузандій молоденько і`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hometask\n",
    "\n",
    "1) Find text to train (any book)<br>\n",
    "2) Build train and validation set <br>\n",
    "3) Train bidirectional language model that predicts the POS of word being based on its `n_context= 3` neighbours from the left and `n_context= 3` neighbours from the right <br>\n",
    "4) Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
