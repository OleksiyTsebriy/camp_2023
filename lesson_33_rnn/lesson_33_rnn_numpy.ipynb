{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Recurrent neural networks \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "## Samples or sequences to sequence problems\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "    <td>Speech recogniiton</td>\n",
    "    <td><img src = \"images/lesson_21_waves.jpg\" align = 'left' width = 100, heaight = 100> </td>\n",
    "    <td>Machine learning provides computer systems the ability <br> \n",
    "         to learn without being explicitly programmed\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Machine translation</td>\n",
    "    <td> 機器學習為計算機系統提供了能力\n",
    "          在沒有明確編程的情況下學習\n",
    "    </td>\n",
    "    <td>Machine learning provides computer systems the ability <br> \n",
    "         to learn without being explicitly programmed\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Video activity recognition</td>\n",
    "    <td><img src = \"images/lesson_21_var.png\" align = 'left' width = 100, heaight = 100> \n",
    "    </td>\n",
    "    <td>Start running\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Named entity recognition</td>\n",
    "    <td>Michael arrived to New York from Boston  in 21 Sep 1978 \n",
    "    </td>\n",
    "    <td>Person: Michael<br>Location: New York, Boston<br> date: 1978/09/21 \n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td>Sentiment analysis</td>\n",
    "    <td> It was not as terrifying enough for thriller as expected\n",
    "    </td>\n",
    "    <td><img src = \"images/lesson_21_stars.jpg\" align = 'left' width = 100, heaight = 100>\n",
    "    </td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Notation\n",
    "\n",
    "</font>\n",
    "\n",
    "For example `named entity recognition`.\n",
    "\n",
    "Some sample $(x,y) = (X^{(i)},Y^{(i)})$\n",
    "\n",
    "<img src = \"images/lesson_21_notation2.png\" align = 'left' width = 600, heaight = 600> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "Len of input $T_{x}=11; \\quad$ Len of output $T_{y}=11$ \n",
    "\n",
    "Note: $T_{x}$ and $T_{y}$ may differ\n",
    "\n",
    "Each $x^{<t>}$ is represented as one-hot vector built being based on some vocabulary (e.g. 10,000 most frequent words + `\"UNK\"` + `\"EOS\"`). \n",
    "<br> Shape of every sample $x = (vocab\\_size, T_{x}) $\n",
    "\n",
    "Each $y^{<t>}$ is represented as one-hot vector built being based on number of entities types \n",
    "<br> Shape of  of every sample $y= (entity\\_num, T_{y}) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## RNN structure\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "Every cell contains the same parameters \n",
    "\n",
    "<img src = \"images/lesson_21_rnn_simple1.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "$$A^{<0>}=0$$<br>\n",
    "$$A^{<t>}= g_{a}(W_{aa}@ A^{<t-1>} + W_{ax}@ x^{<t>}+ b_{a}), \\quad (g_{a} \\text{is usually RELU/tanh)}$$ <br>\n",
    "$$y^{<t>}= g_{y}(W_{ay}@ A^{<t>} + b_{y}), \\quad (g_{y} \\text{ is usually sigmoid/softmax)}$$\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## RNN cell\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"images/lesson_21_rnn_cell2.png\" align = 'left' width = 500, heaight = 500> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## RNN types samples\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"images/lesson_21_oto.png\" align = 'left' width = 100, heaight = 100> \n",
    "<img src = \"images/lesson_21_mto.png\" align = 'left' width = 400, heaight = 400> \n",
    "<img src = \"images/lesson_21_otm.png\" align = 'left' width = 400, heaight = 400> \n",
    "<div style=\"clear:left;\"></div>\n",
    "<img src = \"images/lesson_21_mtm.png\" align = 'left'  width = 400, heaight = 400> \n",
    "<img src = \"images/lesson_21_mtm1.png\" align = 'left'  width = 400, heaight = 400> \n",
    "\n",
    "\n",
    "<img src = \"images/lesson_21_ed.png\" align = 'left'  width = 400, heaight = 400> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Language model\n",
    "\n",
    "</font>\n",
    "\n",
    "Language model is a probability distribution over sequences of words.\n",
    "<br>It provides ability to estimate the relative likelihood of similar words and phrases e.g.<br>\n",
    "$\\quad P(\\text{\"recognize speech\"}) \\quad \\text{and} \\quad P(\\text{\"wreck a nice beach\"}) $\n",
    "\n",
    "Usually the task is to compare probability of different words following by some context of $n$ previous words ($n$-gram)  e.g. $\\quad P(\\text{\"beings\"} | \\text{\"we are human\"}) \\quad \\text{and} \\quad P(\\text{\"beans\"} | \\text{\"we are human\"}) $\n",
    "\n",
    "RNN is one of implementations of Language model.\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "### Implementation steps\n",
    "\n",
    "</font>\n",
    "\n",
    "1. Get corpus (large text)\n",
    "2. Tokenize and build vocabulary of tokens (make sure to have + `\"UNK\"` for words out of vocabulary  and `\"EOS\"` to denote end of sentence)\n",
    "3. Prepare training set by tokenizing to sentence. \n",
    "If the sentence is `Pluto is the largest dwarf planet in the Solar System`, then \n",
    "\n",
    "<img src = \"images/lesson_21_lm1.png\" align = 'left' width = 800, heaight = 800> \n",
    "<div style=\"clear:left;\"></div>\n",
    "4. Compute cost \n",
    "$\\mathcal {L} = \\sum \\limits_{i=1}^{m} \\sum \\limits_{t=1}^{T_{X_{i}}} y_{i}^{<t>} log \\,\\hat y_{i}^{<t>} $\n",
    "<br>Note: if the task is to predict the only following word after $n$-gram context then cost msy include the only last output $\\mathcal {L} = \\sum \\limits_{i=1}^{m}  y_{i}^{<T_{y_{i}}>} log \\,\\hat y_{i}^{<T_{y_{i}}>} $\n",
    "<br>5. Run optimization. <br>\n",
    "6. Having trained model, all $\\hat y^{<1>}, \\hat y^{<2>},... \\hat y^{<T_{y}>}$ are probabilty distributions over vocabulary tokens (outputs of corresponding softmax layer) \n",
    "<br>So you may compute $P$(\"Pluto\") from $\\hat y^{<1>}$, $P$(\"is\"|\"Pluto\") from $\\hat y^{<2>}$ and so get the probability of phrase to compare with another phrase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "### Sampling\n",
    "\n",
    "</font>\n",
    "\n",
    "Sampling is performed being based on probability distribution (outputs of corresponding softmax layer).\n",
    "<br>You may configure how much strictly to sample the word - from selecting the most probable word to consider all words with the same probability and as the result pick random word from all vocabulary.\n",
    "<img src = \"images/lesson_21_sampling.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "Note: \n",
    "\n",
    "- You need to resample in case of \"UNK\" is generated.\n",
    "- You may stop sampling by setting the limit in case of \"EOS\" is not generated for long.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Sample 1: Language model using numpy  \n",
    "\n",
    "</font>\n",
    "\n",
    "Character level language model is implemented as RNN for dinosaurus names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# cwd= os.getcwd()\n",
    "# path = os.path.join(cwd,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters number = 19,909 \n",
      " Unique characters numver = 27.\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "Training set samples number = 1,536\n",
      "Iteration: 0, Loss: 39.548882\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 9.605753\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 22.380867\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 20.168279\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 39.618127\n",
      "\n",
      "Meustratipontaros\n",
      "Infaadps\n",
      "Jvstolopon\n",
      "Macalosaurus\n",
      "Ytrrasaurus\n",
      "Daaiselaa\n",
      "Trohia\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 23.773308\n",
      "\n",
      "Nlyussaurus\n",
      "Lmacairus\n",
      "Lytrrangosaurus\n",
      "Ngaachudcarththenyragwanosaurus\n",
      "Ystrlonlthus\n",
      "Elaeropedosaurus\n",
      "Trrangosaurus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 16.116337\n",
      "\n",
      "Nixrosaurus\n",
      "Indaagosaurus\n",
      "Kystolia\n",
      "Necahisaurus\n",
      "Yusjanasausus\n",
      "Ehaesicekatosaurus\n",
      "Usjanasaurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.960518\n",
      "\n",
      "Mitrtoma\n",
      "Inecaisaurus\n",
      "Jystlepcorawenosaurus\n",
      "Macaeskadbitholenus\n",
      "Ytrrasaurus\n",
      "Eiadosaurus\n",
      "Trocephiaumtitan\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 25.654200\n",
      "\n",
      "Matruraptor\n",
      "Incaaeratidbshuchesaurus\n",
      "Ivusanon\n",
      "Madalisaurus\n",
      "Yusidon\n",
      "Dabarnaiantasaurus\n",
      "Usilonis\n",
      "\n",
      "\n",
      "Iteration: 36000, Loss: 18.991645\n",
      "\n",
      "Meutosaurus\n",
      "Incaa\n",
      "Ivkosaurus\n",
      "Macaisauropteryx\n",
      "Yutodon\n",
      "Eiacosaurus\n",
      "Trocephalusaurasaurus\n",
      "\n",
      "\n",
      "Iteration: 40000, Loss: 18.951287\n",
      "\n",
      "Mesvosaurus\n",
      "Jigabhosaurus\n",
      "Kurus\n",
      "Mafahosaurus\n",
      "Yusthomoraverevarcherochus\n",
      "Ehahosaurus\n",
      "Trogomingvenktargilos\n",
      "\n",
      "\n",
      "Iteration: 44000, Loss: 16.567862\n",
      "\n",
      "Mixpsidon\n",
      "Indabasaurus\n",
      "Ixus\n",
      "Mecalosaurus\n",
      "Yushanhosaurus\n",
      "Ehaeosaurus\n",
      "Trohonosaurus\n",
      "\n",
      "\n",
      "Iteration: 48000, Loss: 37.365335\n",
      "\n",
      "Mitrtonia\n",
      "Krecaeropeciusameotopavenos\n",
      "Lytyrangosaurus\n",
      "Mdabesodherus\n",
      "Ytromilophylus\n",
      "Ehadruradosaurus\n",
      "Trraptor\n",
      "\n",
      "\n",
      "Iteration: 52000, Loss: 20.212239\n",
      "\n",
      "Meutrocomisaurus\n",
      "Iracaesteia\n",
      "Ixuspandor\n",
      "Macaiselacosaurus\n",
      "Xtrocheosaurus\n",
      "Edantosaurus\n",
      "Streosaurus\n",
      "\n",
      "\n",
      "Iteration: 56000, Loss: 24.979204\n",
      "\n",
      "Miutosaurus\n",
      "Jiegansngeantitan\n",
      "Kystnirasegugnosaurus\n",
      "Mecamnorachussanites\n",
      "Yusokicorax\n",
      "Eiakosaurus\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "Iteration: 60000, Loss: 27.051994\n",
      "\n",
      "Jiwusraptor\n",
      "Ingaalosaurus\n",
      "Ivtyraptor\n",
      "Kecaerope\n",
      "Yrosaurus\n",
      "Egainophamus\n",
      "Trochenonygosaurus\n",
      "\n",
      "\n",
      "Iteration: 64000, Loss: 15.510556\n",
      "\n",
      "Mivusaurus\n",
      "Inca\n",
      "Ivusaurus\n",
      "Mecaishadansaurus\n",
      "Yuskimesaurus\n",
      "Eiahmon\n",
      "Strinesaurus\n",
      "\n",
      "\n",
      "Iteration: 68000, Loss: 23.407519\n",
      "\n",
      "Matytlalennixernuchongosaurus\n",
      "Inga\n",
      "Iustrion\n",
      "Madalosaurus\n",
      "Xystbmironykus\n",
      "Eiaiosaurus\n",
      "Strephoreus\n",
      "\n",
      "\n",
      "Iteration: 72000, Loss: 19.921516\n",
      "\n",
      "Louskollaosaurus\n",
      "Hkegagosaurus\n",
      "Hyrosaurus\n",
      "Lecakosaurus\n",
      "Xtosaurus\n",
      "Caaeropaceratops\n",
      "Trodoloraurosaurus\n",
      "\n",
      "\n",
      "Iteration: 76000, Loss: 36.867525\n",
      "\n",
      "Miuuvringosaurus\n",
      "Isanalosaurus\n",
      "Itotodon\n",
      "Mbaaisondelusaurus\n",
      "Xustanisaurus\n",
      "Eeagpihaceratops\n",
      "Strareplevhospanosaurestevelalelta\n",
      "\n",
      "\n",
      "Iteration: 80000, Loss: 21.056122\n",
      "\n",
      "Lctytoloptor\n",
      "Inceaisaurasteus\n",
      "Ivuysaurus\n",
      "Lacaersaurus\n",
      "Wotrephorgylopteryx\n",
      "Ecadropedtops\n",
      "Strepion\n",
      "\n",
      "\n",
      "Iteration: 84000, Loss: 23.174231\n",
      "\n",
      "Miutrodon\n",
      "Ingcamosaurus\n",
      "Ivurops\n",
      "Mecamosaurus\n",
      "Xtroceratosaurus\n",
      "Eg\n",
      "Stoengosaurus\n",
      "\n",
      "\n",
      "Iteration: 88000, Loss: 15.685819\n",
      "\n",
      "Miutus\n",
      "Kolaaiosaurus\n",
      "Lytroinasaurus\n",
      "Ngbakus\n",
      "Wtruinisaurus\n",
      "Gcagosaurus\n",
      "Strion\n",
      "\n",
      "\n",
      "Iteration: 92000, Loss: 19.156647\n",
      "\n",
      "Motrosaurus\n",
      "Isalanosaurus\n",
      "Iwusaurus\n",
      "Mecantus\n",
      "Vrosaurus\n",
      "Ehalosaurus\n",
      "Sthenlosaurus\n",
      "\n",
      "\n",
      "Iteration: 96000, Loss: 22.589076\n",
      "\n",
      "Meutroceratops\n",
      "Isalaesria\n",
      "Itrosaurus\n",
      "Macaiseia\n",
      "Wrotaodon\n",
      "Eg\n",
      "Sqoceratops\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
    "    print('%s' % (txt,), end='')\n",
    "\n",
    "\n",
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x) * 0.01  \n",
    "    Waa = np.random.randn(n_a, n_a) * 0.01  \n",
    "    Wya = np.random.randn(n_y, n_a) * 0.01  \n",
    "    b = np.zeros((n_a, 1))  \n",
    "    by = np.zeros((n_y, 1))  \n",
    "\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)  # hidden state\n",
    "    p_t = softmax(\n",
    "        np.dot(Wya, a_next) + by)  # unnormalized log probabilities for next chars # probabilities for next chars\n",
    "\n",
    "    return a_next, p_t\n",
    "\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next']  # backprop into h\n",
    "    daraw = (1 - a * a) * da  # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b'] += -lr * gradients['db']\n",
    "    parameters['by'] += -lr * gradients['dby']\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size=27):\n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "\n",
    "    a[-1] = np.copy(a0)\n",
    "\n",
    "    # initialize your loss to 0\n",
    "    loss = 0\n",
    "\n",
    "    for t in range(len(X)):\n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.        \n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "\n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t - 1], x[t])\n",
    "\n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "\n",
    "    cache = (y_hat, a, x)\n",
    "\n",
    "    return loss, cache\n",
    "\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "\n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "\n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "\n",
    "    # Backpropagate\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t - 1])\n",
    "\n",
    "\n",
    "    return gradients, a\n",
    "\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "\n",
    "    Arguments:\n",
    "    gradients - a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue - everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "\n",
    "    Returns:\n",
    "    gradients - a dictionary with the clipped gradients.\n",
    "    '''\n",
    "\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients[\n",
    "        'dby']\n",
    "    # clip to mitigate exploding gradients\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b.\n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "\n",
    "    # Create the vector x for the first character (initializing the sequence generation)\n",
    "    x = np.zeros((vocab_size, 1)) \n",
    "    # Initialize a_prev as zeros\n",
    "    a_prev = np.zeros(shape=(n_a, 1))\n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate\n",
    "    indices = []\n",
    "\n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1  # this is just to pass the first while condition\n",
    "\n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append\n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "\n",
    "    while (idx != newline_character and counter != 50):\n",
    "        # Forward propagate \n",
    "        a = np.tanh(Wax @ x + Waa @ a_prev + b)\n",
    "        z = Wya @ a + by\n",
    "        y = softmax(z)\n",
    "\n",
    "        np.random.seed(counter + seed)\n",
    "\n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(vocab_size, p=y.ravel())\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "\n",
    "        # Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros(shape=(vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "\n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "\n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "\n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Forward propagate through time\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "\n",
    "    # Backpropagate through time \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "\n",
    "    # Clip your gradients between -5 (min) and 5 (max) \n",
    "    gradients = clip(gradients, 5)\n",
    "\n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "    return loss, gradients, a[len(X) - 1]\n",
    "\n",
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations=100000, n_a=50, dino_names=7, vocab_size=27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names.\n",
    "\n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration.\n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "\n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(fn) as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    print ('Training set samples number = {:,}'.format (len(examples)))\n",
    "\n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "\n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "\n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters        \n",
    "        loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=0.01)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 4000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "\n",
    "                seed += 1  \n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# execution ===\n",
    "\n",
    "fn = 'dinos.txt'\n",
    "\n",
    "data = open(fn, 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('Total characters number = {:,} \\n Unique characters numver = {}.'.format (data_size, vocab_size))\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix )\n",
    "\n",
    "# Training the model¶\n",
    "parameters = model(data, ix_to_char, char_to_ix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Gated Recurrent Unit (GRU)\n",
    "\n",
    "</font>\n",
    "\n",
    "It introduses gates $\\mathcal{\\Gamma}_{r}$ (as responsible for relevancy) and $\\mathcal{\\Gamma}_{u}$ (as responsible for update) and additional computation of $\\tilde {C\\,}^{<t>}$ as candidate to update the input activation $A^{<t-1>}$\n",
    "\n",
    "$$C^{<t-1>} = A^{<t-1>}\\\\\n",
    "\\mathcal{\\Gamma}_{r} = \\sigma (W_{rc} @ C^{<t-1>} + W_{rx} @ {x\\,}^{<t>} + b_{r}) \\\\\n",
    "\\mathcal{\\Gamma}_{u} = \\sigma (W_{uc} @ C^{<t-1>} + W_{ux} @ {x\\,}^{<t>} + b_{u}) \\\\\n",
    "\\tilde {C\\,}^{<t>} = tanh (W_{cc} @  {C\\,}^{<t-1>} \\cdot \\mathcal{\\Gamma}_{r}  + W_{cx} @ {x\\,}^{<t>} + b_{c})\\\\\n",
    "{A\\,}^{<t>}  = {C\\,}^{<t>} = \\mathcal{\\Gamma}_{u}  \\cdot \\tilde {C\\,}^{<t>} + (1-\\mathcal{\\Gamma}_{u})\\cdot  {C\\,}^{<t-1>}\\\\ \\text{ }\\\\\n",
    "y^{<t>}= softmax (W_{ay} @ A^{<t>} + b_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "</font>\n",
    "It introduces separate line of activation units $C^{<t>}$ and uses 3 gates:\n",
    "\n",
    "- $\\mathcal{\\Gamma}_{u}$ (update) \n",
    "- $\\mathcal{\\Gamma}_{f}$ (forget) \n",
    "- $\\mathcal{\\Gamma}_{o}$ (output) \n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/LSTM.png\" align = 'left' width = 800, heaight = 800> \n",
    "<img src = \"images/LSTM_2.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "$\\quad y^{<t>}= softmax (W_{ay} @ A^{<t>} + b_{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Bidirectional\n",
    "\n",
    "</font>\n",
    "Computation is performed in 2 directions and then both activations values $A_{right}^{<t>}$ and $A_{left}^{<t>}$ of the same time $t$ are considered for $y^{<t>}$:\n",
    "\n",
    "<img src = \"images/lesson_21_bidirectional2.png\" align = 'left' width = 800, heaight = 800> \n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "$\\quad y^{<t>}= g (W_{ay \\,right} @ A_{right}^{<t>} +  W_{ay \\,left} @ A_{left}^{<t>} + b_{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Deep RNN\n",
    "\n",
    "</font>\n",
    "<img src = \"images/lesson_21_deep_rnn1.png\" align = 'left' width = 400, heaight = 400> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hometask\n",
    "\n",
    "1) Find text to train (any book)<br>\n",
    "2) Build train and validation set <br>\n",
    "3) Train bidirectional language model that predicts the POS of word being based on its `n_context= 3` neighbours from the left and `n_context= 3` neighbours from the right <br>\n",
    "4) Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
