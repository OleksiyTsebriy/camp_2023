{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Natural Language Toolkit (NLTK)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## 1. NLTK Corpora\n",
    "\n",
    "</font>\n",
    "\n",
    "[Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download necessary corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review content of loaded corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.1 Gutenberg corpus\n",
    "\n",
    "</font>\n",
    "\n",
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose target file id and review raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moby_dick_id= melville-moby_dick.txt\n",
      "[Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
      "\r\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\r\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\r\n",
      "known nations of the world.  He loved to dust his old grammars; it\r\n",
      "so\n"
     ]
    }
   ],
   "source": [
    "moby_dick_id= gutenberg.fileids()[-6]\n",
    "print ('moby_dick_id=', moby_dick_id)\n",
    "\n",
    "type(gutenberg.raw(moby_dick_id))\n",
    "print(gutenberg.raw(moby_dick_id)[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words number = 260,819\n",
      "Unique words number = 19,317\n",
      "\n",
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',']\n"
     ]
    }
   ],
   "source": [
    "md_text= gutenberg.words(moby_dick_id)\n",
    "print ('Words number = {:,}'.format(len(md_text))) \n",
    "print ('Unique words number = {:,}\\n'.format(len(set(md_text)))) \n",
    "print (md_text[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'],\n",
       " ['ETYMOLOGY', '.'],\n",
       " ['(',\n",
       "  'Supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'Late',\n",
       "  'Consumptive',\n",
       "  'Usher',\n",
       "  'to',\n",
       "  'a',\n",
       "  'Grammar',\n",
       "  'School',\n",
       "  ')']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_sent= gutenberg.sents(moby_dick_id)\n",
    "len(md_sent)\n",
    "md_sent[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barrels', 'vagabond', 'conclusion', '\"\\'', 'platters', 'diminished', 'cliffs', 'upraising', 'rapid', 'tornado', '1729', 'Herman', 'extracted', 'caput', 'papered', 'MEMORIAL', 'blows', 'torment', 'listened', 'AFRICA']\n"
     ]
    }
   ],
   "source": [
    "print (list (set(md_text))[:20]) # Note : set requires converting  to list to apply slizing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Most frequent words\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317 <class 'nltk.probability.FreqDist'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 18713, 'the': 13721, '.': 6862, 'of': 6536, 'and': 6024, 'a': 4569, 'to': 4542, ';': 4072, 'in': 3916, 'that': 2982, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency distribution \n",
    "from nltk import FreqDist\n",
    "dist = FreqDist(md_text) # the same as text1.vocab() \n",
    "print (len(dist) , type (dist))\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dist.keys()\n",
    "print (len(vocab))\n",
    "list (vocab)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review frequent but not short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ['called', 'through', 'almost', 'whales', 'thought', 'before', 'against', 'towards', 'things', 'nothing', 'without', 'should', 'little', 'seemed', 'though', 'captain', 'himself', 'moment', 'CHAPTER', 'something', 'Captain', 'between', 'whaling', 'another', 'Queequeg', 'Pequod', 'Starbuck']\n"
     ]
    }
   ],
   "source": [
    "freq_words = [w for w in vocab if len(w) > 5 and dist[w] > 100] \n",
    "print (len(freq_words), freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.2. Access the NLTK corpora\n",
    "\n",
    "   \n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/stopwords', '/words', '/gutenberg', '/crubadan']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "#  determine the path from logs when you load the corpus items e.g. the nltk.download('gutenberg')\n",
    "target_directory= '/Users/oleksiy.tsebriy/nltk_data/corpora'\n",
    "print ( [x[0].replace(target_directory, '') for x in os.walk(target_directory)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.3. Words corpus \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words amount: 236,736\n",
      "\n",
      "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac', 'abaca', 'abacate', 'abacay', 'abacinate', 'abacination', 'abaciscus', 'abacist', 'aback', 'abactinal', 'abactinally', 'abaction', 'abactor', 'abaculus', 'abacus', 'Abadite', 'abaff', 'abaft', 'abaisance', 'abaiser', 'abaissed', 'abalienate', 'abalienation', 'abalone', 'Abama', 'abampere', 'abandon', 'abandonable', 'abandoned', 'abandonedly', 'abandonee', 'abandoner', 'abandonment', 'Abanic', 'Abantes', 'abaptiston', 'Abarambo', 'Abaris', 'abarthrosis', 'abarticular', 'abarticulation', 'abas', 'abase', 'abased', 'abasedly', 'abasedness', 'abasement', 'abaser', 'Abasgi', 'abash', 'abashed', 'abashedly', 'abashedness', 'abashless', 'abashlessly', 'abashment', 'abasia', 'abasic', 'abask', 'Abassin', 'abastardize', 'abatable', 'abate', 'abatement', 'abater', 'abatis', 'abatised', 'abaton', 'abator', 'abattoir', 'Abatua', 'abature', 'abave', 'abaxial', 'abaxile', 'abaze', 'abb', 'Abba', 'abbacomes', 'abbacy', 'Abbadide']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "correct_spellings = words.words()\n",
    "print ('Words amount: {:,}\\n'.format(len(correct_spellings)))\n",
    "print (correct_spellings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## 2. Simple NLP Tasks \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.1. Tokenize\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### Custom Tokenizer\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'Ternopil', \"don't\", 'care', 'Ivano', \"Frankivs'k\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# This was used for parcing keywords for tokens \n",
    "def tokenize_sentence(target_str):\n",
    "    target_str = re.sub(r\"[_-]\", ' ', target_str) # to get single tokens and apply the n-grams later \n",
    "    re_pattern = r\"[\\w\\']+\" \n",
    "    tokens = re.findall(re_pattern, target_str)\n",
    "    return [token for token in tokens if len(token) > 2] \n",
    "tokenize_sentence(\"I like Ternopil. I don't care of Ivano-Frankivs'k\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### NLTK sentence tokenizer\n",
    "\n",
    "</font>\n",
    "\n",
    "sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue. The pen costs $2.45. What about pensil? Is ... the same price? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith!',\n",
       " 'How are you doing today?',\n",
       " 'The whether is great and the Python is awesome.',\n",
       " 'The sky is blue.',\n",
       " 'The pen costs $2.45.',\n",
       " 'What about pensil?',\n",
       " 'Is ... the same price?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "target_text= '''Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue. The pen costs $2.45. What about pensil? Is ... the same price? '''\n",
    "print (target_text)\n",
    "sent_tokenize(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### NLTK word tokenizer\n",
    "\n",
    "</font>\n",
    "\n",
    "word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.\n",
      "['Hello', 'Mr.', 'Smith', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'The', 'whether', 'is', 'great', 'and', 'the', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'blue', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "target_text= '''Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.'''\n",
    "print (target_text)\n",
    "print (word_tokenize(target_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.2. Stop words\n",
    "\n",
    "</font>\n",
    "\n",
    "stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total len : 179\n",
      "['no', 'through', \"mustn't\", \"wouldn't\", 'under', 'yours', 'were', 't', 'itself', 'all', 'wouldn', 'because', 'why', \"you're\", \"aren't\", 'had', \"won't\", 'its', \"didn't\", 'mustn', 'here', 'which', \"she's\", 'am', 'ain', 'weren', 'mightn', \"that'll\", \"couldn't\", 'nor', 'just', 'herself', 'be', 'them', 'myself', 'on', 'same', 'can', 'at', \"you've\", 'haven', \"haven't\", 'down', 'd', 'ourselves', 'some', \"wasn't\", 'aren', 'then', 'off']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words= set(stopwords.words('english'))\n",
    "# stop_words= set(stopwords.words('russian'))\n",
    "print ('Total len : {}'.format(len(list(stop_words))))\n",
    "print(list(stop_words)[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'words', 'filtering', '.', 'Hello', 'Mr.', 'Smith', '!', 'How', 'today', '?', 'The', 'whether', 'great', 'Python', 'awesome', '.', 'The', 'sky', 'blue', '.']\n",
      "\n",
      "Excluded tokens (stopwords): \n",
      "['is', 'an', 'the', 'are', 'you', 'doing', 'is', 'and', 'the', 'is', 'is']\n"
     ]
    }
   ],
   "source": [
    "target_text = 'This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.'\n",
    "tokens = word_tokenize(target_text)\n",
    "filtered_tokens= [t for t in tokens if t not in stop_words]\n",
    "\n",
    "print (filtered_tokens)\n",
    "print ('\\nExcluded tokens (stopwords): \\n{}'.format([t for t in tokens if t in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.3. Stemming \n",
    "\n",
    "</font>\n",
    "\n",
    "PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['thi', 'is', 'an', 'exampl', 'show', 'the', 'stop', 'word',\n",
       "       'filter', '.', 'hello', 'mr.', 'smith', '!', 'how', 'are', 'you',\n",
       "       'do', 'today', '?', 'the', 'whether', 'is', 'great', 'and', 'the',\n",
       "       'python', 'is', 'awesom', '.', 'the', 'sky', 'is', 'blue'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "target_text = 'This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue '\n",
    "\n",
    "words= np.array(word_tokenize(target_text))\n",
    "ps= PorterStemmer()\n",
    "v_stem= np.vectorize(ps.stem)\n",
    "stemmed_words= v_stem(words)\n",
    "\n",
    "print (target_text)\n",
    "stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['go', ',', 'went', ',', 'gone', ',', 'go', ',', 'gon', 'na', ',',\n",
       "       'goe', ',', 'go'], dtype='<U4')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one more sample\n",
    "target_text = 'go, went, gone, going, gonna, goes, goings'\n",
    "words= np.array(word_tokenize(target_text))\n",
    "v_stem(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['appli', ',', 'appli', ',', 'appli', ',', 'appli', ',', 'appli'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another sample ) \n",
    "target_text = 'apply, applied, applies, applying , applyings'\n",
    "words= np.array(word_tokenize(target_text))\n",
    "v_stem(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.3. Lemmatizing\n",
    "\n",
    "</font>\n",
    "\n",
    "WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Without specified POS  \n",
    "\n",
    "</font>\n",
    "noun by default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wn_lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apply', ',', 'applied', ',', 'applies', ',', 'applying', ',', 'applyings']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wn_lemmatizer.lemmatize(w) for w in word_tokenize(target_text)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', ',', 'am', ',', 'are', ',', 'wa', ',', 'were', ',', 'been', ',', 'being']\n"
     ]
    }
   ],
   "source": [
    "# sample w/o pos tag \n",
    "target_text = 'is, am, are, was, were, been, being'\n",
    "print([wn_lemmatizer.lemmatize(w) for w in word_tokenize(target_text)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### With specified POS  \n",
    "\n",
    "</font>\n",
    "requesting lemmatizing to verb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is, am, are, was, were, been, being\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['be', ',', 'be', ',', 'be', ',', 'be', ',', 'be', ',', 'be', ',', 'be']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (target_text)\n",
    "[wn_lemmatizer.lemmatize(w,'v') for w in word_tokenize(target_text)] # v - means verb # the same as the following:\n",
    "# [wn_lemmatizer.lemmatize(w,wn.VERB) for w in word_tokenize(target_text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### With specified POS  \n",
    "\n",
    "</font>\n",
    "requesting lemmatizing to noun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', ',', 'car']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text = 'car, cars'\n",
    "[wn_lemmatizer.lemmatize(w,wn.NOUN) for w in word_tokenize(target_text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.4. Part of Speech (POS)\n",
    "\n",
    "</font>\n",
    "\n",
    "pos_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/state_union.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('state_union') #just one more sample corpus\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "train_text= state_union.raw('2005-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territo\n"
     ]
    }
   ],
   "source": [
    "print (train_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the next several months, on issue after issue, let us do what Americans have always done, and build a better world for our children and our grandchildren.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(train_text)\n",
    "s = sentences [10]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:\n",
      "['Over', 'the', 'next', 'several', 'months', ',', 'on', 'issue', 'after', 'issue', ',', 'let', 'us', 'do', 'what', 'Americans', 'have', 'always', 'done', ',', 'and', 'build', 'a', 'better', 'world', 'for', 'our', 'children', 'and', 'our', 'grandchildren', '.']\n",
      "\n",
      "tags:\n",
      "[('Over', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('on', 'IN'), ('issue', 'NN'), ('after', 'IN'), ('issue', 'NN'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('do', 'VB'), ('what', 'WP'), ('Americans', 'NNPS'), ('have', 'VBP'), ('always', 'RB'), ('done', 'VBN'), (',', ','), ('and', 'CC'), ('build', 'VB'), ('a', 'DT'), ('better', 'JJR'), ('world', 'NN'), ('for', 'IN'), ('our', 'PRP$'), ('children', 'NNS'), ('and', 'CC'), ('our', 'PRP$'), ('grandchildren', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "words= word_tokenize(s)\n",
    "print ('words:\\n{}'.format(words))\n",
    "tagged= nltk.pos_tag(words)\n",
    "print ('\\ntags:\\n{}'.format (tagged))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review NLTK POS\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Ambiguity of NLTK POS tagging \n",
    "\n",
    "</font>\n",
    "NLTK tags the only one the most probable of possible cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('visiting', 'VBG'),\n",
       " ('friends', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('surprising', 'JJ')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'visiting friends can be surprising'\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(s)) # alternative possible case [('visiting', 'JJ'),...] # friends is the subject\n",
    "tagged # visiting is the subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Converting POS formats \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'visiting': 'v', 'friends': 'n', 'can': '', 'be': 'v', 'surprising': 'a'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def penn_to_wn(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# def penn_to_wn(tag):\n",
    "#     return get_wordnet_pos(tag)\n",
    "\n",
    "\n",
    "{t[0]: penn_to_wn(t[1]) for t in tagged }\n",
    "\n",
    "# Part-of-speech constants\n",
    "#  ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention about \n",
    "# 1) pos used in recognizing location \n",
    "# 2) pos used in language model to predict placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Chunks of POS\n",
    "\n",
    "</font>\n",
    "\n",
    "RegexpParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "train_text= state_union.raw('2005-GWBush.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== \n",
      "sentence= America's economy is the fastest growing of any major industrialized nation.\n",
      "\n",
      "tagged:\n",
      "[('America', 'NNP'), (\"'s\", 'POS'), ('economy', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('fastest', 'JJS'), ('growing', 'NN'), ('of', 'IN'), ('any', 'DT'), ('major', 'JJ'), ('industrialized', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "\n",
      "chunked:\n",
      "<bound method Tree.flatten of Tree('S', [('America', 'NNP'), (\"'s\", 'POS'), ('economy', 'NN'), ('is', 'VBZ'), ('the', 'DT'), Tree('chunk', [('fastest', 'JJS'), ('growing', 'NN')]), ('of', 'IN'), ('any', 'DT'), ('major', 'JJ'), ('industrialized', 'VBN'), ('nation', 'NN'), ('.', '.')])>\n",
      "\n",
      "Detected chunks:\n",
      "['fastest growing']\n",
      "======================================== \n",
      "sentence= In the past four years, we provided tax relief to every person who pays income taxes, overcome a recession, opened up new markets abroad, prosecuted corporate criminals, raised homeownership to its highest level in history, and in the last year alone, the United States has added 2.3 million new jobs.\n",
      "\n",
      "tagged:\n",
      "[('In', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('four', 'CD'), ('years', 'NNS'), (',', ','), ('we', 'PRP'), ('provided', 'VBD'), ('tax', 'NN'), ('relief', 'NN'), ('to', 'TO'), ('every', 'DT'), ('person', 'NN'), ('who', 'WP'), ('pays', 'VBZ'), ('income', 'NN'), ('taxes', 'NNS'), (',', ','), ('overcome', 'VBP'), ('a', 'DT'), ('recession', 'NN'), (',', ','), ('opened', 'VBD'), ('up', 'RP'), ('new', 'JJ'), ('markets', 'NNS'), ('abroad', 'RB'), (',', ','), ('prosecuted', 'JJ'), ('corporate', 'JJ'), ('criminals', 'NNS'), (',', ','), ('raised', 'VBD'), ('homeownership', 'NN'), ('to', 'TO'), ('its', 'PRP$'), ('highest', 'JJS'), ('level', 'NN'), ('in', 'IN'), ('history', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('year', 'NN'), ('alone', 'RB'), (',', ','), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('has', 'VBZ'), ('added', 'VBN'), ('2.3', 'CD'), ('million', 'CD'), ('new', 'JJ'), ('jobs', 'NNS'), ('.', '.')]\n",
      "\n",
      "chunked:\n",
      "<bound method Tree.flatten of Tree('S', [('In', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('four', 'CD'), ('years', 'NNS'), (',', ','), ('we', 'PRP'), ('provided', 'VBD'), ('tax', 'NN'), ('relief', 'NN'), ('to', 'TO'), ('every', 'DT'), ('person', 'NN'), ('who', 'WP'), ('pays', 'VBZ'), ('income', 'NN'), ('taxes', 'NNS'), (',', ','), ('overcome', 'VBP'), ('a', 'DT'), ('recession', 'NN'), (',', ','), ('opened', 'VBD'), ('up', 'RP'), Tree('chunk', [('new', 'JJ'), ('markets', 'NNS')]), ('abroad', 'RB'), (',', ','), Tree('chunk', [('prosecuted', 'JJ'), ('corporate', 'JJ'), ('criminals', 'NNS')]), (',', ','), ('raised', 'VBD'), ('homeownership', 'NN'), ('to', 'TO'), ('its', 'PRP$'), Tree('chunk', [('highest', 'JJS'), ('level', 'NN')]), ('in', 'IN'), ('history', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('the', 'DT'), Tree('chunk', [('last', 'JJ'), ('year', 'NN')]), ('alone', 'RB'), (',', ','), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('has', 'VBZ'), ('added', 'VBN'), ('2.3', 'CD'), ('million', 'CD'), Tree('chunk', [('new', 'JJ'), ('jobs', 'NNS')]), ('.', '.')])>\n",
      "\n",
      "Detected chunks:\n",
      "['new markets', 'prosecuted corporate criminals', 'highest level', 'last year', 'new jobs']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = sent_tokenize(train_text)\n",
    "\n",
    "for s in sentences[13:15]:\n",
    "    print('='*40, '\\nsentence= {}'.format(s))\n",
    "    words= word_tokenize(s)\n",
    "    # print ('words= {}'.format(words))\n",
    "    tagged= nltk.pos_tag(words) # part of speech (POS)  # works for list of words\n",
    "    print('\\ntagged:\\n{}'.format(tagged))\n",
    "\n",
    "    # detect chunks \n",
    "    chunk_gram = 'chunk: {<JJ.?>+<NN.?>+} ' # selects adj and noun followed by\n",
    "    \n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tagged)    \n",
    "    print('\\nchunked:\\n{}'.format(chunked.flatten))\n",
    "\n",
    "    # extract chunks \n",
    "    chunks = []\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'chunk'):\n",
    "        chunk = \"\"\n",
    "        for leaf in subtree.leaves():\n",
    "            chunk += leaf[0] + ' '\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    print('\\nDetected chunks:\\n{}'.format(chunks))\n",
    "    \n",
    "#     draw tree \n",
    "#     chunked.draw()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of visualization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged:\n",
      " [('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN')]\n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "print ('Tagged:\\n', sentence)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "result.draw() # it appears in separate window "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"chunks.png\" height=500 width= 500 align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "s = 'Donald came to New York for summit'\n",
    "words= word_tokenize(s)\n",
    "tagged_words= nltk.pos_tag(words) # part of speech (POS)  # works for list of words\n",
    "chunked= nltk.ne_chunk(tagged_words)\n",
    "# print ('named_entities:\\n{}'.format(chunked))\n",
    "\n",
    "def extract_ne_chunks(chunked): \n",
    "    chunks = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:    \n",
    "            print (i.flatten)\n",
    "            chunks.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "extract_ne_chunks(chunked)\n",
    "\n",
    "# chunked.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tree.flatten of Tree('PERSON', [('Donald', 'NNP')])>\n",
      "<bound method Tree.flatten of Tree('GPE', [('New', 'NNP'), ('York', 'NNP')])>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Donald', 'New York']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tree import Tree\n",
    "s = 'Donald came to New York for summit'\n",
    "words= word_tokenize(s)\n",
    "tagged_words= nltk.pos_tag(words) # part of speech (POS)  # works for list of words\n",
    "chunked= nltk.ne_chunk(tagged_words)\n",
    "# print ('named_entities:\\n{}'.format(chunked))\n",
    "\n",
    "def extract_ne_chunks(chunked): \n",
    "    chunks = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:    \n",
    "            print (i.flatten)\n",
    "            chunks.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "extract_ne_chunks(chunked)\n",
    "\n",
    "# chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.5. Wordnet\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Synsets\n",
    "\n",
    "</font>\n",
    "\n",
    "wn.synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/oleksiy.tsebriy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_word = look \n",
      "Synset('expression.n.01')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['expression.n.01',\n",
       " 'look.n.02',\n",
       " 'look.n.03',\n",
       " 'spirit.n.02',\n",
       " 'look.v.01',\n",
       " 'look.v.02',\n",
       " 'look.v.03',\n",
       " 'search.v.02',\n",
       " 'front.v.01',\n",
       " 'attend.v.02',\n",
       " 'look.v.07',\n",
       " 'expect.v.03',\n",
       " 'look.v.09',\n",
       " 'count.v.08']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word = 'look'\n",
    "print('target_word = {} '.format(target_word))\n",
    "\n",
    "synsets = wn.synsets(target_word)\n",
    "print (synsets[0])\n",
    "[synset.name() for  synset  in synsets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Lemmas\n",
    "\n",
    "</font>\n",
    "\n",
    "synset.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression.n.01:['expression', 'look', 'aspect', 'facial_expression', 'face']\n",
      "look.n.02:['look', 'looking', 'looking_at']\n",
      "look.n.03:['look']\n",
      "spirit.n.02:['spirit', 'tone', 'feel', 'feeling', 'flavor', 'flavour', 'look', 'smell']\n",
      "look.v.01:['look']\n",
      "look.v.02:['look', 'appear', 'seem']\n",
      "look.v.03:['look']\n",
      "search.v.02:['search', 'look']\n",
      "front.v.01:['front', 'look', 'face']\n",
      "attend.v.02:['attend', 'take_care', 'look', 'see']\n",
      "look.v.07:['look']\n",
      "expect.v.03:['expect', 'look', 'await', 'wait']\n",
      "look.v.09:['look']\n",
      "count.v.08:['count', 'bet', 'depend', 'look', 'calculate', 'reckon']\n",
      "\n",
      " Lemma('expression.n.01.aspect')\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print ('{}:{}'.format(synset.name(), [lemma.name() for  lemma in synset.lemmas()]))\n",
    "\n",
    "print ('\\n',synsets[0].lemmas()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "</font>\n",
    "\n",
    "synset.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression.n.01:\tthe feelings expressed on a person's face\n",
      "look.n.02:\tthe act of directing the eyes toward something and perceiving it visually\n",
      "look.n.03:\tphysical appearance\n",
      "spirit.n.02:\tthe general atmosphere of a place or situation and the effect that it has on people\n",
      "look.v.01:\tperceive with attention; direct one's gaze towards\n",
      "look.v.02:\tgive a certain impression or have a certain outward aspect\n",
      "look.v.03:\thave a certain outward or facial expression\n",
      "search.v.02:\tsearch or seek\n",
      "front.v.01:\tbe oriented in a certain direction, often with respect to another reference point; be opposite to\n",
      "attend.v.02:\ttake charge of or deal with\n",
      "look.v.07:\tconvey by one's expression\n",
      "expect.v.03:\tlook forward to the probable occurrence of\n",
      "look.v.09:\taccord in appearance with\n",
      "count.v.08:\thave faith or confidence in\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print ('{}:\\t{}'.format(synset.name(), synset.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Examples\n",
    "\n",
    "</font>\n",
    "synset.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression.n.01:\t['a sad expression', 'a look of triumph', 'an angry face']\n",
      "\n",
      "look.n.02:\t['he went out to have a look', 'his look was fixed on her eyes', 'he gave it a good looking at', 'his camera does his looking for him']\n",
      "\n",
      "look.n.03:\t[\"I don't like the looks of this place\"]\n",
      "\n",
      "spirit.n.02:\t['the feel of the city excited him', 'a clergyman improved the tone of the meeting', 'it had the smell of treason']\n",
      "\n",
      "look.v.01:\t['She looked over the expanse of land', 'Look at your child!', 'Look--a deer in the backyard!']\n",
      "\n",
      "look.v.02:\t['She seems to be sleeping', 'This appears to be a very difficult problem', 'This project looks fishy', 'They appeared like people who had not eaten or slept for a long time']\n",
      "\n",
      "look.v.03:\t['How does she look?', 'The child looks unhappy', 'She looked pale after the surgery']\n",
      "\n",
      "search.v.02:\t['We looked all day and finally found the child in the forest', 'Look elsewhere for the perfect gift!']\n",
      "\n",
      "front.v.01:\t['The house looks north', 'My backyard look onto the pond', 'The building faces the park']\n",
      "\n",
      "attend.v.02:\t['Could you see about lunch?', 'I must attend to this matter', 'She took care of this business']\n",
      "\n",
      "look.v.07:\t['She looked her devotion to me']\n",
      "\n",
      "expect.v.03:\t['We were expecting a visit from our relatives', 'She is looking to a promotion', 'he is waiting to be drafted']\n",
      "\n",
      "look.v.09:\t[\"You don't look your age!\"]\n",
      "\n",
      "count.v.08:\t['you can count on me to help you any time', 'Look to your friends for support', 'You can bet on that!', 'Depend on your family in times of crisis']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print ('{}:\\t{}\\n'.format(synset.name(), synset.examples()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Synonyms and Antonyms\n",
    "\n",
    "</font>\n",
    "synset.lemmas()\n",
    "<br>lemma.antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_word = good \n",
      "\n",
      "synonyms: {'near', 'goodness', 'undecomposed', 'salutary', 'respectable', 'secure', 'upright', 'safe', 'effective', 'skilful', 'trade_good', 'beneficial', 'serious', 'good', 'proficient', 'adept', 'estimable', 'sound', 'practiced', 'dependable', 'well', 'ripe', 'unspoilt', 'in_force', 'thoroughly', 'unspoiled', 'full', 'just', 'right', 'soundly', 'skillful', 'in_effect', 'honest', 'commodity', 'dear', 'expert', 'honorable'}\n",
      "antonyms:\n",
      " [{'evil': 'good'}, {'evilness': 'goodness'}, {'bad': 'good'}, {'badness': 'goodness'}, {'bad': 'good'}, {'evil': 'good'}, {'ill': 'well'}]\n"
     ]
    }
   ],
   "source": [
    "def get_syn_ant(target_word):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    print('target_word = {} '.format(target_word))\n",
    "\n",
    "    for syn in wn.synsets(target_word):\n",
    "        for l in syn.lemmas(): # use lemmas to obtain synonyms\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():  # some lemmas have antonyms\n",
    "                antonyms.append({la.name(): l.name() for la in l.antonyms()})\n",
    "#                 antonyms.append(l.antonyms()[0].name())  # Assuming selecting just first antonym for lemma\n",
    "    return set(synonyms), antonyms\n",
    "\n",
    "target_word = 'good'\n",
    "synonyms, antonyms =  get_syn_ant (target_word)\n",
    "print ('\\nsynonyms:', synonyms),\n",
    "print ('antonyms:\\n', antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.6. Edit Distance\n",
    "\n",
    "</font>\n",
    "\n",
    "nltk.edit_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, hell: 1/1\n",
      "hell, hall: 1/1\n",
      "men, manual: 4/4\n",
      ", : 4/4\n",
      "casual, causal: 1/2\n",
      "top, pot: 2/2\n",
      "top, open: 3/3\n"
     ]
    }
   ],
   "source": [
    "target_pairs= [\n",
    "    ('hello', 'hell'),\n",
    "    ('hell', 'hall'),\n",
    "    ('men', 'manual'),\n",
    "    ('', ''),\n",
    "    ('casual', 'causal'),\n",
    "    ('top', 'pot'),\n",
    "    ('top', 'open')\n",
    "    \n",
    "]\n",
    "for word_1, word_2 in target_pairs:\n",
    "    print ('{}, {}: {}/{}'.format(word_1, word_2, nltk.edit_distance(word_1, word_2,transpositions=True), \n",
    "                                 nltk.edit_distance(word_1, word_2,transpositions=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.7. Wordnet similarity\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Wordnet hierarchy\n",
    "\n",
    "</font>\n",
    "WordNet organizes information in a hierarchy \n",
    "Verbs, nouns, adjectives etc. all have separate hierarchy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src = 'wn_tree1.png' height=500 width= 500 align=\"left\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src = 'wn_tree1.png' height=500 width= 500 align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Path similarity\n",
    "\n",
    "</font>\n",
    "\n",
    "$1/(n+1)$ , $n$ - steps between concepts\n",
    "<br>\n",
    "synset_1.path_similarity(synset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deer and elk: 0.5\n",
      "deer and horse: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "deer_synset = wn.synset('deer.n.01')\n",
    "elk_synset=  wn.synset('elk.n.01')\n",
    "horse_synset=  wn.synset('horse.n.01')\n",
    "print ('{} and {}: {}'.format('deer','elk', deer_synset.path_similarity(elk_synset))) # d= 2 \n",
    "print ('{} and {}: {}'.format('deer','horse', deer_synset.path_similarity(horse_synset))) # d= 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Wu-Palmer similarity\n",
    "\n",
    "</font>\n",
    "synset_1.wup_similarity(synset_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deer and elk: 0.967741935483871\n",
      "deer and horse: 0.8\n"
     ]
    }
   ],
   "source": [
    "# comparing to wup_similarity\n",
    "print ('{} and {}: {}'.format('deer','elk', deer_synset.wup_similarity(elk_synset)))  \n",
    "print ('{} and {}: {}'.format('deer','horse', deer_synset.wup_similarity(horse_synset))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink.n.01  vs water.n.01: 0.26666666666666666\n",
      "drink.n.01  vs milk.n.01: 0.25\n",
      "milk.n.01  vs water.n.01: 0.42105263157894735\n",
      "drink.n.01  vs coca_cola.n.01: 0.2222222222222222\n",
      "pepsi.n.01  vs coca_cola.n.01: 0.9090909090909091\n",
      "new_year.n.01  vs santa_claus.n.01: 0.25\n",
      "new_year.n.01  vs christmas.n.01: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "words_pairs= [('drink.n.01', 'water.n.01'),\n",
    "              ('drink.n.01', 'milk.n.01'),\n",
    "              ('milk.n.01', 'water.n.01'),\n",
    "              ('drink.n.01', 'coca_cola.n.01'),\n",
    "              ('pepsi.n.01', 'coca_cola.n.01'),\n",
    "              ('New_year.n.01', 'Santa_Claus.n.01'),\n",
    "              ('New_year.n.01', 'christmas.n.01')\n",
    "              ]\n",
    "for words_pair in words_pairs:\n",
    "    w1 = wn.synset(words_pair[0])\n",
    "    w2 = wn.synset(words_pair[1])\n",
    "    print('{}  vs {}: {}'.format(w1.name(), w2.name(), w1.wup_similarity(w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink.n.01 pepsi.n.01 0.2222222222222222\n",
      "drink.n.02 pepsi.n.01 0.18181818181818182\n",
      "beverage.n.01 pepsi.n.01 0.8421052631578947\n",
      "drink.n.04 pepsi.n.01 0.2857142857142857\n",
      "swallow.n.02 pepsi.n.01 0.25\n",
      "drink.v.01 pepsi.n.01 0\n",
      "drink.v.02 pepsi.n.01 0\n",
      "toast.v.02 pepsi.n.01 0\n",
      "drink_in.v.01 pepsi.n.01 0\n",
      "drink.v.05 pepsi.n.01 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8421052631578947"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import editdistance # standalone package\n",
    "\n",
    "def calc_words_similarity(word_a,word_b, verbose= 0):\n",
    "    ''' Calculates wup similarity for all synsets if synsets available, and score based on edit distance otherwise\n",
    "        in case of synsets available returns max of all values\n",
    "    '''\n",
    "    word_a_synsets= wn.synsets(word_a)\n",
    "    word_b_synsets= wn.synsets(word_b)\n",
    "    if len (word_a_synsets)==0 or len (word_b_synsets)==0:\n",
    "        if verbose: \n",
    "            print ('One of words has no synsets')\n",
    "        return calc_edit_score(word_a,word_b)\n",
    "    else:\n",
    "        similarities_all_synsets= []\n",
    "        for word_a_synset in word_a_synsets:\n",
    "            for word_b_synset in word_b_synsets:\n",
    "                if word_a_synset.pos() == word_b_synset.pos():  # this speeds up the calculation                   \n",
    "                    synsets_similarity =  word_a_synset.wup_similarity(word_b_synset)\n",
    "                    if synsets_similarity: # some time return None\n",
    "                        similarities_all_synsets.append(synsets_similarity)\n",
    "                    else: # synsets_similarity is None\n",
    "                        synsets_similarity= 0 \n",
    "                else: # different pos \n",
    "                    synsets_similarity= 0 \n",
    "                similarities_all_synsets.append(synsets_similarity)\n",
    "                if verbose:\n",
    "                    print (word_a_synset.name(), word_b_synset.name(), synsets_similarity)\n",
    "        return max(similarities_all_synsets)\n",
    "\n",
    "def calc_edit_score(w1, w2):\n",
    "    '''calc the score being based on edit (Levinshtein) distance\n",
    "        1 difference - 0.8, 2- 0.64, 3-.51 ...\n",
    "    '''\n",
    "    edit_dist= nltk.edit_distance(w1, w2)    \n",
    "    if edit_dist> len (w1) or edit_dist> len (w2): # no common letter\n",
    "        print ('No common letter. Consider similarity as 0.')\n",
    "        return 0\n",
    "    return 0.8 ** edit_dist\n",
    "\n",
    "calc_words_similarity('drink','pepsi', verbose= 1)\n",
    "# calc_words_similarity('deer','horse', verbose= 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Custom phrase similarity\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science is fascinating\n",
      "Work with data is interesting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8634920634920635"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_phrase_similarity(phrase_a, phrase_b):\n",
    "    '''\n",
    "    phrase_a, phrase_b - str\n",
    "    '''\n",
    "    if len (phrase_a) * len (phrase_b)== 0:\n",
    "        return 0\n",
    "\n",
    "    tokens_a = nltk.word_tokenize(phrase_a)\n",
    "    tokens_a = [w for w in tokens_a if w not in stop_words]\n",
    "\n",
    "    tokens_b = nltk.word_tokenize(phrase_b)\n",
    "    tokens_b = [w for w in tokens_b if w not in stop_words]\n",
    "\n",
    "    \n",
    "    a_similarity =  target_phrase_avg(tokens_a, tokens_b)\n",
    "    b_similarity =  target_phrase_avg(tokens_b, tokens_a)\n",
    "\n",
    "    return np.mean([a_similarity, b_similarity])\n",
    "\n",
    "def target_phrase_avg(tokens_a, tokens_b):\n",
    "    all_similarities= [target_word_similarity_score(tokens_a, b) for b in tokens_b]\n",
    "    if len(all_similarities)>0:\n",
    "        return np.mean(all_similarities)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def target_word_similarity_score(current_words, target_word):\n",
    "    '''\n",
    "    Calculates similarity between all current words (all synsets) and ONE target word (all synsets)\n",
    "    :param current_words: list of words of current phrase\n",
    "    :param target_word: str\n",
    "    :return: float\n",
    "    '''\n",
    "    if len(current_words)==0:\n",
    "        return 0\n",
    "    all_current_words_similarities= []\n",
    "    for current_word in current_words:\n",
    "        all_current_words_similarities.append(calc_words_similarity(current_word, target_word))\n",
    "    if len(all_current_words_similarities)==0:\n",
    "        return 0\n",
    "    return max(all_current_words_similarities)\n",
    "\n",
    "phrase_a = 'Data Science is fascinating'\n",
    "phrase_b = 'Work with data is interesting'\n",
    "\n",
    "# phrase_a = 'Data Science is fascinating'\n",
    "# phrase_b = 'the milk is white'\n",
    "\n",
    "print ('{}\\n{}'.format(phrase_a, phrase_b))\n",
    "calc_phrase_similarity(phrase_a, phrase_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_raw = gutenberg.raw('melville-moby_dick.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 1\n",
    "\n",
    "</font>\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in `moby_raw`?\n",
    "<br>*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_one():\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return len(word_tokenize(moby_raw)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255,028\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_one()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 2\n",
    "\n",
    "</font>\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does `moby_raw` have?\n",
    "<br>*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_two():    \n",
    "    return len(set(nltk.word_tokenize(moby_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,742\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_two()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 3\n",
    "\n",
    "</font>\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does `moby_raw` have?\n",
    "<br>*This function should return an integer.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in nltk.word_tokenize(moby_raw)]\n",
    "    return len(set(lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,887\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_three()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 1\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "<br>*This function should return a float.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.08139566804842562`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 2\n",
    "\n",
    "</font>\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "<br>*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def answer_two():    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.4125668166077752`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 3\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "<br>*This function should return a list of 10 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(',', 19204),\n",
    " ('the', 13715),\n",
    " ('.', 7308),\n",
    " ('of', 6513),\n",
    " ('and', 6010),\n",
    " ('a', 4545),\n",
    " ('to', 4515),\n",
    " (';', 4173),\n",
    " ('in', 3908),\n",
    " ('that', 2978)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 4\n",
    "\n",
    "</font>\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "<br>*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print (answer_four())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 5\n",
    "\n",
    "</font>\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "<br>\n",
    "*This function should return a tuple `(longest_word, length)`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`(\"twelve-o'clock-at-night\", 23)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 6\n",
    "\n",
    "</font>\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(answer_six())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2978, 'that'), (2459, 'his'), (2196, 'it'), (2097, 'I')]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 7\n",
    "\n",
    "</font>\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "<br>*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(answer_seven())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`25.881952902963864`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 8\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(answer_eight())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[('NN', 32730), ('IN', 28657), ('DT', 25867), (',', 19204), ('JJ', 17620)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 9\n",
    "\n",
    "</font>\n",
    "\n",
    "Create spelling recommender, that take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest `edit distance` (you may need  to use `nltk.edit_distance(word_1, word_2, transpositions=True)`), and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "Recommender should provide recommendations for the three words: `['cormulent', 'incendenece', 'validrate']`.\n",
    "<br>*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_nine(default_words= ['cormulent', 'incendenece', 'validrate']):\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['corpulent', 'intendence', 'validate']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "NLTK 3.4 documentation\n",
    "<br>\n",
    "https://www.nltk.org/\n",
    "\n",
    "Accessing Text Corpora and Lexical Resources\n",
    "<br>\n",
    "https://www.nltk.org/book/ch02.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.chunk package\n",
    "<br>\n",
    "https://www.nltk.org/api/nltk.chunk.html\n",
    "\n",
    "Edit distance\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Edit_distance\n",
    "\n",
    "Applied Text Mining in Python\n",
    "<br>\n",
    "https://www.coursera.org/learn/python-text-mining/home/welcome\n",
    "\n",
    "Natural Language Processing tutorial\n",
    "<br>\n",
    "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
