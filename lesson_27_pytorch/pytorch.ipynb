{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syllabys\n",
    "\n",
    "1. [Overview](#overview)\n",
    "2. [Tensors and accelerated software](#tensors-and-accelerated-software)\n",
    "3. [PyTorch data interfaces](#pytorch-data-interfaces)\n",
    "4. [Model structure](#model-structure)\n",
    "5. [Training process](#model-training)\n",
    "6. [References and materials](#materials-and-usefull-links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Neural network workflow</h3>\n",
    "\n",
    "![Neural network workflow](https://miro.medium.com/v2/resize:fit:1400/1*ZXAOUqmlyECgfVa81Sr6Ew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network principles:\n",
    "\n",
    "1. Forward propagation:\n",
    "\n",
    "    1. Input\n",
    "    2. Hidden state\n",
    "    3. Activation function\n",
    "    4. Output\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. Backward propagation:\n",
    "\n",
    "    1. Loss function\n",
    "    2. Optimization and weight update\n",
    "\n",
    "\n",
    "Iterative repetition of forward and backward propagation to minimize loss function and get optimal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NumPy-based neural network (example on logistic regression)</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required functions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "logistic_function = lambda x: 1/ (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_cost(theta, x, y):\n",
    "    m = len(y)\n",
    "    y_pred = logistic_function(np.dot(x , theta))\n",
    "    error = (y * np.log(y_pred)) + ((1 - y) * np.log(1 - y_pred))\n",
    "    cost = -1 / m * sum(error)\n",
    "    gradient = 1 / m * np.dot(x.transpose(), (y_pred - y))\n",
    "    return cost[0] , gradient\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha, iterations):\n",
    "    costs = []\n",
    "    for i in range(iterations):\n",
    "        cost, gradient = compute_cost(theta, x, y)\n",
    "        theta -= (alpha * gradient)\n",
    "        costs.append(cost)\n",
    "    return theta, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initialization 0.693147180559946\n",
      "Gradient at initialization: [[-0.4518845   0.4518845   0.4518845   0.4518845   0.4518845  -0.4518845\n",
      "  -0.4518845  -0.4518845  -0.4518845   0.4518845  -0.4518845   0.4518845\n",
      "  -0.4518845   0.4518845  -0.4518845  -0.4518845  -0.4518845   0.4518845\n",
      "  -0.4518845   0.4518845   0.4518845   0.4518845   0.4518845   0.4518845\n",
      "   0.4518845   0.4518845  -0.4518845  -0.4518845  -0.4518845   0.4518845\n",
      "  -0.4518845   0.4518845   0.4518845   0.4518845  -0.4518845  -0.4518845\n",
      "   0.4518845  -0.4518845  -0.4518845  -0.4518845  -0.4518845   0.4518845\n",
      "   0.4518845  -0.4518845   0.4518845   0.4518845   0.4518845   0.4518845\n",
      "  -0.4518845  -0.4518845  -0.4518845   0.4518845   0.4518845  -0.4518845\n",
      "  -0.4518845  -0.4518845  -0.4518845  -0.4518845  -0.4518845   0.4518845\n",
      "   0.4518845  -0.4518845   0.4518845  -0.4518845   0.4518845   0.4518845\n",
      "   0.4518845  -0.4518845  -0.4518845  -0.4518845   0.4518845   0.4518845\n",
      "   0.4518845  -0.4518845   0.4518845  -0.4518845   0.4518845   0.4518845\n",
      "  -0.4518845   0.4518845  -0.4518845  -0.4518845  -0.4518845   0.4518845\n",
      "  -0.4518845  -0.4518845  -0.4518845   0.4518845   0.4518845   0.4518845\n",
      "  -0.4518845  -0.4518845  -0.4518845   0.4518845   0.4518845  -0.4518845\n",
      "   0.4518845   0.4518845   0.4518845  -0.4518845 ]\n",
      " [-0.42632008  0.42632008  0.42632008  0.42632008  0.42632008 -0.42632008\n",
      "  -0.42632008 -0.42632008 -0.42632008  0.42632008 -0.42632008  0.42632008\n",
      "  -0.42632008  0.42632008 -0.42632008 -0.42632008 -0.42632008  0.42632008\n",
      "  -0.42632008  0.42632008  0.42632008  0.42632008  0.42632008  0.42632008\n",
      "   0.42632008  0.42632008 -0.42632008 -0.42632008 -0.42632008  0.42632008\n",
      "  -0.42632008  0.42632008  0.42632008  0.42632008 -0.42632008 -0.42632008\n",
      "   0.42632008 -0.42632008 -0.42632008 -0.42632008 -0.42632008  0.42632008\n",
      "   0.42632008 -0.42632008  0.42632008  0.42632008  0.42632008  0.42632008\n",
      "  -0.42632008 -0.42632008 -0.42632008  0.42632008  0.42632008 -0.42632008\n",
      "  -0.42632008 -0.42632008 -0.42632008 -0.42632008 -0.42632008  0.42632008\n",
      "   0.42632008 -0.42632008  0.42632008 -0.42632008  0.42632008  0.42632008\n",
      "   0.42632008 -0.42632008 -0.42632008 -0.42632008  0.42632008  0.42632008\n",
      "   0.42632008 -0.42632008  0.42632008 -0.42632008  0.42632008  0.42632008\n",
      "  -0.42632008  0.42632008 -0.42632008 -0.42632008 -0.42632008  0.42632008\n",
      "  -0.42632008 -0.42632008 -0.42632008  0.42632008  0.42632008  0.42632008\n",
      "  -0.42632008 -0.42632008 -0.42632008  0.42632008  0.42632008 -0.42632008\n",
      "   0.42632008  0.42632008  0.42632008 -0.42632008]\n",
      " [-0.98980913  0.98980913  0.98980913  0.98980913  0.98980913 -0.98980913\n",
      "  -0.98980913 -0.98980913 -0.98980913  0.98980913 -0.98980913  0.98980913\n",
      "  -0.98980913  0.98980913 -0.98980913 -0.98980913 -0.98980913  0.98980913\n",
      "  -0.98980913  0.98980913  0.98980913  0.98980913  0.98980913  0.98980913\n",
      "   0.98980913  0.98980913 -0.98980913 -0.98980913 -0.98980913  0.98980913\n",
      "  -0.98980913  0.98980913  0.98980913  0.98980913 -0.98980913 -0.98980913\n",
      "   0.98980913 -0.98980913 -0.98980913 -0.98980913 -0.98980913  0.98980913\n",
      "   0.98980913 -0.98980913  0.98980913  0.98980913  0.98980913  0.98980913\n",
      "  -0.98980913 -0.98980913 -0.98980913  0.98980913  0.98980913 -0.98980913\n",
      "  -0.98980913 -0.98980913 -0.98980913 -0.98980913 -0.98980913  0.98980913\n",
      "   0.98980913 -0.98980913  0.98980913 -0.98980913  0.98980913  0.98980913\n",
      "   0.98980913 -0.98980913 -0.98980913 -0.98980913  0.98980913  0.98980913\n",
      "   0.98980913 -0.98980913  0.98980913 -0.98980913  0.98980913  0.98980913\n",
      "  -0.98980913  0.98980913 -0.98980913 -0.98980913 -0.98980913  0.98980913\n",
      "  -0.98980913 -0.98980913 -0.98980913  0.98980913  0.98980913  0.98980913\n",
      "  -0.98980913 -0.98980913 -0.98980913  0.98980913  0.98980913 -0.98980913\n",
      "   0.98980913  0.98980913  0.98980913 -0.98980913]\n",
      " [-0.50771546  0.50771546  0.50771546  0.50771546  0.50771546 -0.50771546\n",
      "  -0.50771546 -0.50771546 -0.50771546  0.50771546 -0.50771546  0.50771546\n",
      "  -0.50771546  0.50771546 -0.50771546 -0.50771546 -0.50771546  0.50771546\n",
      "  -0.50771546  0.50771546  0.50771546  0.50771546  0.50771546  0.50771546\n",
      "   0.50771546  0.50771546 -0.50771546 -0.50771546 -0.50771546  0.50771546\n",
      "  -0.50771546  0.50771546  0.50771546  0.50771546 -0.50771546 -0.50771546\n",
      "   0.50771546 -0.50771546 -0.50771546 -0.50771546 -0.50771546  0.50771546\n",
      "   0.50771546 -0.50771546  0.50771546  0.50771546  0.50771546  0.50771546\n",
      "  -0.50771546 -0.50771546 -0.50771546  0.50771546  0.50771546 -0.50771546\n",
      "  -0.50771546 -0.50771546 -0.50771546 -0.50771546 -0.50771546  0.50771546\n",
      "   0.50771546 -0.50771546  0.50771546 -0.50771546  0.50771546  0.50771546\n",
      "   0.50771546 -0.50771546 -0.50771546 -0.50771546  0.50771546  0.50771546\n",
      "   0.50771546 -0.50771546  0.50771546 -0.50771546  0.50771546  0.50771546\n",
      "  -0.50771546  0.50771546 -0.50771546 -0.50771546 -0.50771546  0.50771546\n",
      "  -0.50771546 -0.50771546 -0.50771546  0.50771546  0.50771546  0.50771546\n",
      "  -0.50771546 -0.50771546 -0.50771546  0.50771546  0.50771546 -0.50771546\n",
      "   0.50771546  0.50771546  0.50771546 -0.50771546]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize mock data\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=4, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "\n",
    "theta_init = np.zeros((X.shape[1], X.shape[0]))\n",
    "cost, gradient = compute_cost(theta_init, X, y)\n",
    "\n",
    "print(\"Cost at initialization\", cost)\n",
    "print(\"Gradient at initialization:\", gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta after running gradient descent: [[ 0.84258378 -0.84258378 -0.84258378 -0.84258378 -0.84258378  0.84258378\n",
      "   0.84258378  0.84258378  0.84258378 -0.84258378  0.84258378 -0.84258378\n",
      "   0.84258378 -0.84258378  0.84258378  0.84258378  0.84258378 -0.84258378\n",
      "   0.84258378 -0.84258378 -0.84258378 -0.84258378 -0.84258378 -0.84258378\n",
      "  -0.84258378 -0.84258378  0.84258378  0.84258378  0.84258378 -0.84258378\n",
      "   0.84258378 -0.84258378 -0.84258378 -0.84258378  0.84258378  0.84258378\n",
      "  -0.84258378  0.84258378  0.84258378  0.84258378  0.84258378 -0.84258378\n",
      "  -0.84258378  0.84258378 -0.84258378 -0.84258378 -0.84258378 -0.84258378\n",
      "   0.84258378  0.84258378  0.84258378 -0.84258378 -0.84258378  0.84258378\n",
      "   0.84258378  0.84258378  0.84258378  0.84258378  0.84258378 -0.84258378\n",
      "  -0.84258378  0.84258378 -0.84258378  0.84258378 -0.84258378 -0.84258378\n",
      "  -0.84258378  0.84258378  0.84258378  0.84258378 -0.84258378 -0.84258378\n",
      "  -0.84258378  0.84258378 -0.84258378  0.84258378 -0.84258378 -0.84258378\n",
      "   0.84258378 -0.84258378  0.84258378  0.84258378  0.84258378 -0.84258378\n",
      "   0.84258378  0.84258378  0.84258378 -0.84258378 -0.84258378 -0.84258378\n",
      "   0.84258378  0.84258378  0.84258378 -0.84258378 -0.84258378  0.84258378\n",
      "  -0.84258378 -0.84258378 -0.84258378  0.84258378]\n",
      " [ 0.51131246 -0.51131246 -0.51131246 -0.51131246 -0.51131246  0.51131246\n",
      "   0.51131246  0.51131246  0.51131246 -0.51131246  0.51131246 -0.51131246\n",
      "   0.51131246 -0.51131246  0.51131246  0.51131246  0.51131246 -0.51131246\n",
      "   0.51131246 -0.51131246 -0.51131246 -0.51131246 -0.51131246 -0.51131246\n",
      "  -0.51131246 -0.51131246  0.51131246  0.51131246  0.51131246 -0.51131246\n",
      "   0.51131246 -0.51131246 -0.51131246 -0.51131246  0.51131246  0.51131246\n",
      "  -0.51131246  0.51131246  0.51131246  0.51131246  0.51131246 -0.51131246\n",
      "  -0.51131246  0.51131246 -0.51131246 -0.51131246 -0.51131246 -0.51131246\n",
      "   0.51131246  0.51131246  0.51131246 -0.51131246 -0.51131246  0.51131246\n",
      "   0.51131246  0.51131246  0.51131246  0.51131246  0.51131246 -0.51131246\n",
      "  -0.51131246  0.51131246 -0.51131246  0.51131246 -0.51131246 -0.51131246\n",
      "  -0.51131246  0.51131246  0.51131246  0.51131246 -0.51131246 -0.51131246\n",
      "  -0.51131246  0.51131246 -0.51131246  0.51131246 -0.51131246 -0.51131246\n",
      "   0.51131246 -0.51131246  0.51131246  0.51131246  0.51131246 -0.51131246\n",
      "   0.51131246  0.51131246  0.51131246 -0.51131246 -0.51131246 -0.51131246\n",
      "   0.51131246  0.51131246  0.51131246 -0.51131246 -0.51131246  0.51131246\n",
      "  -0.51131246 -0.51131246 -0.51131246  0.51131246]\n",
      " [ 3.34654108 -3.34654108 -3.34654108 -3.34654108 -3.34654108  3.34654108\n",
      "   3.34654108  3.34654108  3.34654108 -3.34654108  3.34654108 -3.34654108\n",
      "   3.34654108 -3.34654108  3.34654108  3.34654108  3.34654108 -3.34654108\n",
      "   3.34654108 -3.34654108 -3.34654108 -3.34654108 -3.34654108 -3.34654108\n",
      "  -3.34654108 -3.34654108  3.34654108  3.34654108  3.34654108 -3.34654108\n",
      "   3.34654108 -3.34654108 -3.34654108 -3.34654108  3.34654108  3.34654108\n",
      "  -3.34654108  3.34654108  3.34654108  3.34654108  3.34654108 -3.34654108\n",
      "  -3.34654108  3.34654108 -3.34654108 -3.34654108 -3.34654108 -3.34654108\n",
      "   3.34654108  3.34654108  3.34654108 -3.34654108 -3.34654108  3.34654108\n",
      "   3.34654108  3.34654108  3.34654108  3.34654108  3.34654108 -3.34654108\n",
      "  -3.34654108  3.34654108 -3.34654108  3.34654108 -3.34654108 -3.34654108\n",
      "  -3.34654108  3.34654108  3.34654108  3.34654108 -3.34654108 -3.34654108\n",
      "  -3.34654108  3.34654108 -3.34654108  3.34654108 -3.34654108 -3.34654108\n",
      "   3.34654108 -3.34654108  3.34654108  3.34654108  3.34654108 -3.34654108\n",
      "   3.34654108  3.34654108  3.34654108 -3.34654108 -3.34654108 -3.34654108\n",
      "   3.34654108  3.34654108  3.34654108 -3.34654108 -3.34654108  3.34654108\n",
      "  -3.34654108 -3.34654108 -3.34654108  3.34654108]\n",
      " [ 0.70179165 -0.70179165 -0.70179165 -0.70179165 -0.70179165  0.70179165\n",
      "   0.70179165  0.70179165  0.70179165 -0.70179165  0.70179165 -0.70179165\n",
      "   0.70179165 -0.70179165  0.70179165  0.70179165  0.70179165 -0.70179165\n",
      "   0.70179165 -0.70179165 -0.70179165 -0.70179165 -0.70179165 -0.70179165\n",
      "  -0.70179165 -0.70179165  0.70179165  0.70179165  0.70179165 -0.70179165\n",
      "   0.70179165 -0.70179165 -0.70179165 -0.70179165  0.70179165  0.70179165\n",
      "  -0.70179165  0.70179165  0.70179165  0.70179165  0.70179165 -0.70179165\n",
      "  -0.70179165  0.70179165 -0.70179165 -0.70179165 -0.70179165 -0.70179165\n",
      "   0.70179165  0.70179165  0.70179165 -0.70179165 -0.70179165  0.70179165\n",
      "   0.70179165  0.70179165  0.70179165  0.70179165  0.70179165 -0.70179165\n",
      "  -0.70179165  0.70179165 -0.70179165  0.70179165 -0.70179165 -0.70179165\n",
      "  -0.70179165  0.70179165  0.70179165  0.70179165 -0.70179165 -0.70179165\n",
      "  -0.70179165  0.70179165 -0.70179165  0.70179165 -0.70179165 -0.70179165\n",
      "   0.70179165 -0.70179165  0.70179165  0.70179165  0.70179165 -0.70179165\n",
      "   0.70179165  0.70179165  0.70179165 -0.70179165 -0.70179165 -0.70179165\n",
      "   0.70179165  0.70179165  0.70179165 -0.70179165 -0.70179165  0.70179165\n",
      "  -0.70179165 -0.70179165 -0.70179165  0.70179165]]\n",
      "Resulting cost: 0.0028598267679711166\n"
     ]
    }
   ],
   "source": [
    "theta, costs = gradient_descent(X, y, theta_init, 1, 200)\n",
    "\n",
    "print(\"Theta after running gradient descent:\", theta)\n",
    "print(\"Resulting cost:\", costs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the simplest one-layer model:\n",
    "\n",
    "![Logistic regression schema](https://media5.datahacker.rs/2021/01/83.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in real life actual DNN architectures will use more complex architectures:\n",
    "\n",
    "![More complex architectures](https://ascelibrary.org/cms/10.1061/(ASCE)ST.1943-541X.0003392/asset/176e59f6-aad9-4df8-9074-2b852e1155f0/assets/images/large/figure6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet architecture](https://www.researchgate.net/publication/322621180/figure/fig2/AS:584852684410885@1516451154473/The-representation-of-model-architecture-image-for-ResNet-152-VGG-19-and-two-layered.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DL frameworks are the tools that make DL processes less difficult</h2>\n",
    "\n",
    "Frameworks offer:\n",
    "\n",
    "- abstraction from the low-level operation with matrices and complex math operation \n",
    "- high-level programming interface and tools for building blocks to design, train, and validate deep neural networks\n",
    "- parallelizing of the operation to accelerate processes\n",
    "- automation of optimization and backpropagation\n",
    "\n",
    "\n",
    "According to [Papers with code](https://paperswithcode.com/trends) among most implemented papers in Python at the moment there are next top-3 frameworks:\n",
    "\n",
    "\n",
    "![Framework trends](supplementary_data/frameworks_usage_trends.png)\n",
    "\n",
    "- PyTorch (60% of repositores with paper implementations)\n",
    "- TensorFlow (3% of repositores with paper implementations)\n",
    "- JAX (2% of repositores with paper implementations)\n",
    "\n",
    "\n",
    "On Sep 2020, the top was:\n",
    "\n",
    "- PyTorch (50%)\n",
    "- TensorFlow (20%)\n",
    "- JAX (0%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors and accelerated software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensors` are a specialized `data structure` that are very similar to arrays and matrices. In PyTorch, tensors are used to `encode the inputs and outputs of a model`, as well as the `model’s parameters`.\n",
    "\n",
    "A tensor is the abstract version of nested lists of numbers. A 'vector' is a one-dimensional tensor. A 'matrix' is a two-dimensional tensor, and higher dimensions are referred to as 'n-dimensional tensors'.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can `run on GPUs` or other hardware accelerators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/work/Work/custom/.venv/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: Pillow in /home/work/Work/custom/.venv/lib/python3.10/site-packages (9.5.0)\n",
      "Requirement already satisfied: networkx in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: typing-extensions in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: filelock in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: sympy in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: setuptools in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape of tensor: torch.Size([2, 2])\n",
      "Datatype of tensor: torch.int64\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Tensor\n",
    "\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)\n",
    "\n",
    "print(f\"Shape of tensor: {x_data.shape}\")\n",
    "print(f\"Datatype of tensor: {x_data.dtype}\")\n",
    "print(f\"Device tensor is stored on: {x_data.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html) or more generally [here](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CUDA</h3>\n",
    "\n",
    "CUDA® is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs). With CUDA, developers are able to dramatically speed up computing applications by harnessing the power of GPUs.\n",
    "\n",
    "\n",
    "In GPU-accelerated applications, the sequential part of the workload runs on the CPU – which is optimized for single-threaded performance – while the compute intensive portion of the application runs on thousands of GPU cores in parallel. \n",
    "\n",
    "CUDA installation is available on [NVIDIA's official website](https://developer.nvidia.com/cuda-toolkit).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cuda` is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device. The selected device can be changed with a `torch.cuda.device` context manager.\n",
    "\n",
    "However, once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed on the same device as the tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    x_data = x_data.to(\"cuda\")\n",
    "    print(f\"Device tensor is stored on: {x_data.device}\")\n",
    "\n",
    "# or use:\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x_data = x_data.to(device)\n",
    "\n",
    "# if more than one GPU is available, you can select the specific one (index starts from 0)\n",
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of the CUDA significantly speeds up models processes, such as training or inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch data interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually data is the first step to all ML tasks. It defines the model's input and output.\n",
    "\n",
    "PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that are designed to handle and manage data for training machine learning models and serve as high-level interface.\n",
    "\n",
    "`Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples.\n",
    "\n",
    "PyTorch domain libraries provide a number of pre-loaded datasets and they could be used for prototyping DL models -  [Image Datasets](https://pytorch.org/vision/stable/datasets.html), [Text Datasets](https://pytorch.org/text/stable/datasets.html), and [Audio Datasets](https://pytorch.org/audio/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/work/Work/custom/.venv/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: numpy in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: torch==2.0.0 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: requests in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: sympy in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (11.7.91)\n",
      "Requirement already satisfied: typing-extensions in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
      "Requirement already satisfied: filelock in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: jinja2 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (0.40.0)\n",
      "Requirement already satisfied: lit in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.2)\n",
      "Requirement already satisfied: cmake in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.26.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/work/Work/custom/.venv/lib/python3.10/site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# torchvision library to work with vision tasks (images, videos)\n",
    "!pip install torchvision \n",
    "\n",
    "# torchaudio library to work with audio tasks - !pip install torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular data can be fully loaded into the memory and usually doesn't require some complex pre-processing.\n",
    "So it simply can be loaded as Tensor objects without creation of interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7500, 4]), torch.Size([2500, 4]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=4, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, n_samples=10000\n",
    ")\n",
    "\n",
    "X, y = torch.from_numpy(X), torch.from_numpy(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it's datasets that cannot be loaded fully into memory and require special interface for reading/transforming operation, e.g., image data, audio data, etc.\n",
    "\n",
    "There are two types of datasets:\n",
    "\n",
    "- map-style datasets\n",
    "- iterable-style datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://pjreddie.com/media/files/cifar.tgz && tar xzf cifar.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "with open(\"cifar/labels.txt\") as label_file:\n",
    "    labels = label_file.read().split()\n",
    "label_mapping = dict(zip(labels, list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map-style datasets require implementation of the __getitem__() and __len__() methods\n",
    "\n",
    "class Cifar10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, labels, data_size = 0, transforms = None):\n",
    "        files = os.listdir(data_dir)\n",
    "        files = [os.path.join(data_dir,x) for x in files]\n",
    "        \n",
    "        if data_size < 0 or data_size > len(files):\n",
    "            assert(\"Data size should be between 0 to number of files in the dataset\")\n",
    "        \n",
    "        if data_size == 0:\n",
    "            data_size = len(files)\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        self.files = random.sample(files, self.data_size)\n",
    "        self.transforms = transforms\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_address = self.files[idx]\n",
    "        image = Image.open(image_address)\n",
    "        label_name = image_address[:-4].split(\"_\")[-1]\n",
    "        label = self.labels[label_name]\n",
    "                \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe1654b3430>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = Cifar10Dataset(data_dir=\"cifar/train/\", labels=label_mapping, transforms=None)\n",
    "first_img, first_label = train_set[0]\n",
    "\n",
    "print(first_img.shape)\n",
    "plt.imshow(first_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe145e5e2c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "train_set = Cifar10Dataset(data_dir=\"cifar/train/\", labels=label_mapping, transforms=transforms.Grayscale())\n",
    "first_img, first_label = train_set[0]\n",
    "\n",
    "print(first_img.shape)\n",
    "plt.imshow(first_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AugMix',\n",
       " 'AutoAugment',\n",
       " 'AutoAugmentPolicy',\n",
       " 'CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'ConvertImageDtype',\n",
       " 'ElasticTransform',\n",
       " 'FiveCrop',\n",
       " 'GaussianBlur',\n",
       " 'Grayscale',\n",
       " 'InterpolationMode',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'PILToTensor',\n",
       " 'Pad',\n",
       " 'RandAugment',\n",
       " 'RandomAdjustSharpness',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomAutocontrast',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomEqualize',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomInvert',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomPosterize',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSolarize',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " 'TrivialAugmentWide',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional_pil',\n",
       " '_functional_tensor',\n",
       " '_presets',\n",
       " 'autoaugment',\n",
       " 'functional',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader functions are:\n",
    "\n",
    "1. Batching of Data\n",
    "2. Shuffling of Data\n",
    "3. Loading multiple data at a single time using threads\n",
    "4. Prefetching, that is, while GPU processes the current batch, `DataLoader` can load the next batch into memory in meantime. This means GPU doesn't have to wait for the next batch and it speeds up training.\n",
    "\n",
    "`DataLoader` is instantiated with a `Dataset` object, then it can be iterated in the same way as `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([8, 32, 32])\n",
      "Labels batch shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpersistent_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
      "the given dataset.\n",
      "\n",
      "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      "iterable-style datasets with single- or multi-process loading, customizing\n",
      "loading order and optional automatic batching (collation) and memory pinning.\n",
      "\n",
      "See :py:mod:`torch.utils.data` documentation page for more details.\n",
      "\n",
      "Args:\n",
      "    dataset (Dataset): dataset from which to load the data.\n",
      "    batch_size (int, optional): how many samples per batch to load\n",
      "        (default: ``1``).\n",
      "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      "        at every epoch (default: ``False``).\n",
      "    sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      "        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      "        implemented. If specified, :attr:`shuffle` must not be specified.\n",
      "    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      "        returns a batch of indices at a time. Mutually exclusive with\n",
      "        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      "        and :attr:`drop_last`.\n",
      "    num_workers (int, optional): how many subprocesses to use for data\n",
      "        loading. ``0`` means that the data will be loaded in the main process.\n",
      "        (default: ``0``)\n",
      "    collate_fn (Callable, optional): merges a list of samples to form a\n",
      "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
      "        map-style dataset.\n",
      "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      "        into device/CUDA pinned memory before returning them.  If your data elements\n",
      "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      "        see the example below.\n",
      "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
      "        the size of dataset is not divisible by the batch size, then the last batch\n",
      "        will be smaller. (default: ``False``)\n",
      "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      "        from workers. Should always be non-negative. (default: ``0``)\n",
      "    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
      "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      "        input, after seeding and before data loading. (default: ``None``)\n",
      "    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
      "        by RandomSampler to generate random indexes and multiprocessing to generate\n",
      "        `base_seed` for workers. (default: ``None``)\n",
      "    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
      "        in advance by each worker. ``2`` means there will be a total of\n",
      "        2 * num_workers batches prefetched across all workers. (default value depends\n",
      "        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
      "        Otherwise if value of num_workers>0 default is ``2``).\n",
      "    persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
      "        the worker processes after a dataset has been consumed once. This allows to\n",
      "        maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      "    pin_memory_device (str, optional): the data loader will copy Tensors\n",
      "        into device pinned memory before returning them if pin_memory is set to true.\n",
      "\n",
      "\n",
      ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      "             cannot be an unpicklable object, e.g., a lambda function. See\n",
      "             :ref:`multiprocessing-best-practices` on more details related\n",
      "             to multiprocessing in PyTorch.\n",
      "\n",
      ".. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      "             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      "             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      "             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      "             configurations. This represents the best guess PyTorch can make because PyTorch\n",
      "             trusts user :attr:`dataset` code in correctly handling multi-process\n",
      "             loading to avoid duplicate data.\n",
      "\n",
      "             However, if sharding results in multiple workers having incomplete last batches,\n",
      "             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      "             be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      "             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      "             cases in general.\n",
      "\n",
      "             See `Dataset Types`_ for more details on these two types of datasets and how\n",
      "             :class:`~torch.utils.data.IterableDataset` interacts with\n",
      "             `Multi-process data loading`_.\n",
      "\n",
      ".. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
      "             :ref:`data-loading-randomness` notes for random seed related questions.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Work/custom/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community datasets could be loaded using special libraries such as `torchvision` for CV tasks, `torchaudio` for audio and signal processing tasks or `torchtext` for NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar/train/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:12<00:00, 14150200.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar/train/cifar-10-python.tar.gz to ./cifar/train/\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar/test/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:20<00:00, 8248464.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar/test/cifar-10-python.tar.gz to ./cifar/test/\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar/train/', transform=transform, download=True)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar/test/', transform=transform,  download=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single neuron:\n",
    "\n",
    "- takes some set of inputs\n",
    "- generates a corresponding scalar output\n",
    "- has a set of associated parameters that can be updated to optimize some objective function of interest\n",
    "\n",
    "\n",
    "In more complex network where multiple outputs are needed, the more complex subject is used - `layer`, a collection of multiple neurons:\n",
    "\n",
    "- take a set of inputs\n",
    "- generate corresponding outputs\n",
    "- are described by a set of tunable parameters\n",
    "\n",
    "\n",
    "Neural network:\n",
    "\n",
    "- complex of layers\n",
    "- takes in raw inputs (the features)\n",
    "- generates outputs (the predictions)\n",
    "- possesses parameters (the combined parameters from all constituent layers)\n",
    "\n",
    "\n",
    "\n",
    "To improve level of abstraction, the concept of `module` is introduced. \n",
    "A module could describe a single layer, a component consisting of multiple layers, or the entire model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch `nn` module is a core component facilitating the construction and training of neural networks. \n",
    "It provides a high-level abstraction for designing and implementing neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch `nn` module could be split into next groups:\n",
    "\n",
    "- containers - Module, Sequential\n",
    "- linear - Linear\n",
    "- activations - non-linear units (ReLU, Sigmoid, LeakyReLU, Tanh, Softmax, etc.)\n",
    "- normalizations - BatchNorm, LayerNorm, different variations\n",
    "- loss functions - L1Loss, MSELoss, CrossEntropyLoss, etc.\n",
    "- matrix layers (usually for CV tasks) - convolutional, pooling, padding, dropout\n",
    "- sequential layers (usually for NLP tasks) - recurrent (RNN, LSTM, GRU), transformer (model, encoder, decoder), sparse\n",
    "\n",
    "\n",
    "Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n",
    "\n",
    "Full list of possible modules is listed in [PyTorch nn documentation](https://pytorch.org/docs/stable/nn.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_layer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(28*28, 512)\n",
    "        self.linear_2 = nn.Linear(512, 512)\n",
    "        self.out_layer = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        \n",
    "        return self.out_layer(x)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Note that functional modules that are used in forward-propogation are not displayed here; \n",
    "# only modules from constructor are displayed\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases similar to above where the output of previous layers go straight to the input of next layers without any additional processing - `Sequential` module could be used to design better network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_2(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_2 = NeuralNetwork_2()\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of modules could have alternative version with `Lazy` prefix in name.\n",
    "\n",
    "Modules that lazily initialize parameters, or “lazy modules”, derive the shapes of their parameters from the first input(s) to their forward method. Until that first forward they contain `torch.nn.UninitializedParameter`'s that should not be accessed or used, and afterward they contain regular `torch.nn.Parameter`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_3(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): LazyLinear(in_features=0, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LazyLinear(in_features=0, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): LazyLinear(in_features=0, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/Work/custom/.venv/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_3 = NeuralNetwork_3()\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models above return `logits` - the vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. \n",
    "\n",
    "If the model is solving a multi-class classification problem, logits typically become an input to the `Softmax` function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class.\n",
    "\n",
    "If only top class is needed from the output model - the normalization function could be swapped to `argmax` function.\n",
    "\n",
    "\n",
    "Customary, `logits` are the final output of the model and it's normalization is performed outside model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer without parameters; used to implement functional modules like activation functions, etc\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "    \n",
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.7998e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0126,  0.6590, -0.4871],\n",
       "        [ 0.8025, -0.1942,  0.8908],\n",
       "        [ 0.0188, -0.1470, -1.4865],\n",
       "        [ 1.0358,  0.0047,  0.9329],\n",
       "        [-0.0488,  0.0264,  1.0749]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer with parameters that could be learned during back-propagation\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "    \n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6268, 0.0000, 0.0000],\n",
       "        [1.2272, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0906],\n",
       "        [3.7883]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Background</h3>\n",
    "\n",
    "Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors.\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "`Forward Propagation`: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
    "\n",
    "`Backward Propagation`: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple NN with 5 neurons:\n",
    "\n",
    "![Network structure](./supplementary_data/graph_example.png)\n",
    "\n",
    "\n",
    "Plotting computational graphs helps us visualize the dependencies of operators and variables within the calculation.\n",
    "\n",
    "![Network backpropoogation graph](./supplementary_data/graph_backpropogation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make work with model easier, the algorithm of `auto-differentiation` is introduced.\n",
    "\n",
    "To compute parameter gradients, PyTorch has a built-in differentiation engine called `torch.autograd`. It supports automatic computation of gradient for any `computational graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "\n",
    "w1 = torch.randn(5, 3, requires_grad=True)\n",
    "w2 = torch.randn(5, 3, requires_grad=True)\n",
    "w3 = torch.randn(3, requires_grad=True)\n",
    "w4 = torch.randn(3, requires_grad=True)\n",
    "\n",
    "b = torch.matmul(x, w1)\n",
    "c = torch.matmul(x, w2)\n",
    "\n",
    "d = w3 * b + w4 * c\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(d, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for d = <AddBackward0 object at 0x7fe145e98b50>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fe145e98460>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for d = {d.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters.\n",
    "\n",
    "To compute those derivatives, we call `loss.backward()`, and then retrieve the values from corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5632, -0.0099,  0.4143],\n",
      "        [-0.5632, -0.0099,  0.4143],\n",
      "        [-0.5632, -0.0099,  0.4143],\n",
      "        [-0.5632, -0.0099,  0.4143],\n",
      "        [-0.5632, -0.0099,  0.4143]])\n",
      "tensor([[-0.8021,  0.0095, -0.1299],\n",
      "        [-0.8021,  0.0095, -0.1299],\n",
      "        [-0.8021,  0.0095, -0.1299],\n",
      "        [-0.8021,  0.0095, -0.1299],\n",
      "        [-0.8021,  0.0095, -0.1299]])\n",
      "tensor([0.6488, 0.0334, 0.0359])\n",
      "tensor([-0.8622, -0.0726,  0.0171])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(w2.grad)\n",
    "print(w3.grad)\n",
    "print(w4.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. \n",
    "\n",
    "However, there are some cases when we do not need to do that. For example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. \n",
    "\n",
    "Or to mark some parameters in your neural network as `frozen parameters`, so they won't change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "b = torch.matmul(x, w1)\n",
    "print(b.requires_grad)\n",
    "\n",
    "\n",
    "# We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:\n",
    "\n",
    "with torch.no_grad():\n",
    "    b = torch.matmul(x, w1)\n",
    "print(b.requires_grad)\n",
    "\n",
    "\n",
    "# With `torch.inference_mode()` block:\n",
    "\n",
    "with torch.inference_mode():\n",
    "    b = torch.matmul(x, w1)\n",
    "print(b.requires_grad)\n",
    "\n",
    "\n",
    "# using `.detach()` method:\n",
    "\n",
    "b = torch.matmul(x, w1).detach()\n",
    "print(b.requires_grad)\n",
    "\n",
    "\n",
    "# disabling `requires_grad` flag \n",
    "b = torch.matmul(x, w1).detach()\n",
    "b.requires_grad = False\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about disabling could be read on [PyTorch auto-grad documentation](https://pytorch.org/docs/stable/notes/autograd.html#locally-disabling-gradient-computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NN model is trained in the iteration of the loop that optimizes models' weights. Such iterations are called an `epoch`.\n",
    "\n",
    "Each epoch consists of two main parts:\n",
    "\n",
    "- `The Train Loop` - iterate over the training dataset and try to converge to optimal parameters.\n",
    "- `The Validation/Test Loop` - iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "\n",
    "When presented with some training data, the untrained network is likely not to give the correct answer. \n",
    "\n",
    "`Loss function` measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
    "\n",
    "\n",
    "Common loss functions include `nn.MSELoss (Mean Square Error)` for `regression` tasks, and `nn.CrossEntropyLoss` for `classification` tasks.\n",
    "\n",
    "\n",
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. `Optimization algorithms` define how this process is performed (e.g., `Stochastic Gradient Descent`, `Adam`, `RMSProp`, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "\n",
    "- Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "- Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss for each parameter.\n",
    "- Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar/train/', transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar/test/', transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar/labels.txt\") as label_file:\n",
    "    labels = label_file.read().split()\n",
    "label_mapping = dict(zip(labels, list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = torch.numel(trainset[0][0])\n",
    "output_size = len(label_mapping)\n",
    "\n",
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10Model(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import  functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = CIFAR10Model(input_size, output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "# Initialize the loss function; also called `criterion`\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    \n",
    "    tqdm_loader = tqdm(dataloader, unit=\"batch\", desc=f\"Epoch {epoch}\", total=len(dataloader))\n",
    "    for batch, (X, y) in enumerate(tqdm_loader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            tqdm_loader.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12500/12500 [00:28<00:00, 437.19batch/s, loss=1.58] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.839392 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 12500/12500 [00:26<00:00, 464.76batch/s, loss=1.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.818013 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 12500/12500 [00:27<00:00, 460.47batch/s, loss=0.839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.753160 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 12500/12500 [00:26<00:00, 466.69batch/s, loss=1.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.733848 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12500/12500 [00:26<00:00, 471.05batch/s, loss=2.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.694875 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 12500/12500 [00:26<00:00, 472.32batch/s, loss=0.746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.666101 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 12500/12500 [00:26<00:00, 469.62batch/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.653447 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 12500/12500 [00:26<00:00, 473.30batch/s, loss=0.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.595254 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 12500/12500 [00:26<00:00, 468.82batch/s, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.560835 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 12500/12500 [00:26<00:00, 471.11batch/s, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.554592 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    train_loop(train_loader, model, loss_fn, optimizer, epoch=t)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "torch.save(model, 'model.pth')\n",
    "\n",
    "# OR\n",
    "\n",
    "torch.save(model.state_dict(), 'model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.load('model.pth')\n",
    "loaded_model.eval()\n",
    "\n",
    "# OR\n",
    "\n",
    "loaded_model = CIFAR10Model(input_size, output_size)\n",
    "loaded_model.load_state_dict(torch.load('model_state.pth'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` After loading saved model, `model.eval()` method is called to set specifics layers (such as, Dropout, Normalization, etc.) to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materials and usefull links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [PyTorch Basics tytorial](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
    "2. [PyTorch Data Interfaces](https://pytorch.org/docs/stable/data.html)\n",
    "3. [D2L Builders' Guide](https://d2l.ai/chapter_builders-guide/index.html)\n",
    "4. [PyTorch Auto-differentiation basics](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "5. [PyTorch Auto-differentiation intro](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#)\n",
    "6. [PyTorch Auto-differentiation mechanics](https://pytorch.org/docs/stable/notes/autograd.html#)\n",
    "7. [D2L Propogation and graphs](https://d2l.ai/chapter_multilayer-perceptrons/backprop.html)\n",
    "8. [Understanding PyTorch graphs and autograd](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)\n",
    "9. [PyTorch Optimization](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\n",
    "10. Grokking Deep Learning by Andrew W. Trask\n",
    "11. [PyTorch models visualization](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools:\n",
    "\n",
    "1. [PyTorch Lightning](https://lightning.ai/pytorch-lightning) - tool upon PyTorch to make training and model designing easier\n",
    "2. [Netron](https://netron.app/) - tool for visualizing the model architecture\n",
    "3. [PyTorch eco-system](https://pytorch.org/ecosystem/) - list of PyTorch tools\n",
    "4. [WandB](https://github.com/wandb/wandb) - tools for experementation tracking and its visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train NN model with custom linear architecture on MNIST or on any dataset from Kaggle (image data with resolution not higher than 80x80 pixels - \n",
    "[Sign language MNIST](https://www.kaggle.com/datasets/datamunge/sign-language-mnist), \n",
    "[A-Z Handwritten Alphabet](https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format), etc.)\n",
    "2. Visualize epoch progress (loss and accuracy curves)\n",
    "3. Visualize 10 random elements from test set and model results for those elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
