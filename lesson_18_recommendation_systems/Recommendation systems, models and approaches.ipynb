{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation systems, models and approaches\n",
    "\n",
    "### Lec 2:\n",
    "\n",
    "\n",
    "1. ALS & PySpark\n",
    "2. Attention models - STAMP\n",
    "   1. Real-life examples\n",
    "3. Transformers\n",
    "4. Other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product & ALS\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/data_sparcity.webp\" width=1200>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/Matrix factorization.webp\" width=1200>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/pysparkALS.png\" width=1200>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session-based - STAMP\n",
    "\n",
    "- can't track long-time user interests\n",
    "- great in online services like e-commerce and news portals, where most users either browse anonymously or may have very distinct interests for different sessions\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/stamp.png\" width=1000>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/attention.png\" width=1000>\n",
    "</p>\n",
    "\n",
    "```python\n",
    "    def build_model(self):\n",
    "        \"\"\" Build the neural network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Products in a session\n",
    "        self.inputs = tf.placeholder(\n",
    "            tf.int32,\n",
    "            [None, None],\n",
    "            name=\"inputs\")\n",
    "\n",
    "        # Last product in a session\n",
    "        self.last_inputs = tf.placeholder(\n",
    "            tf.int32,\n",
    "            [None],\n",
    "            name=\"last_inputs\")\n",
    "\n",
    "        batch_size = tf.shape(self.inputs)[0]\n",
    "\n",
    "        self.sequence_length = tf.placeholder(\n",
    "            tf.int64,\n",
    "            [None],\n",
    "            name='sequence_length')\n",
    "\n",
    "        # True last products in sessions (one product per session)\n",
    "        self.lab_input = tf.placeholder(\n",
    "            tf.int32,\n",
    "            [None],\n",
    "            name=\"lab_input\")\n",
    "\n",
    "        # Embeddings that we learn during training\n",
    "        self.embe_dict = tf.Variable(\n",
    "            self.pre_embedding,\n",
    "            dtype=tf.float32,\n",
    "            trainable=self.emb_up)\n",
    "\n",
    "        # This is only for filtering out padding\n",
    "        self.pe_mask = tf.Variable(\n",
    "            self.pre_embedding_mask,\n",
    "            dtype=tf.float32,\n",
    "            trainable=False)\n",
    "        \n",
    "        # Embeddings without padding\n",
    "        self.embe_dict *= self.pe_mask\n",
    "        \n",
    "        # Create a variable for embeddings mask based on business rules\n",
    "        self.business_rules_embeddings = tf.Variable(\n",
    "            self.business_rules_embeddings_mask,\n",
    "            dtype=tf.float32,\n",
    "            trainable=False)\n",
    "        \n",
    "        # In the variable we have either zeros or true embeddings, so\n",
    "        # we get recommendations scores that are either zero (for products\n",
    "        # we don't want in recommendations) or true score\n",
    "        self.business_rules_embeddings *= self.embe_dict\n",
    "        \n",
    "        sent_bitmap = tf.ones_like(tf.cast(self.inputs, tf.float32))\n",
    "\n",
    "        # Get embedding for products in a session\n",
    "        inputs = tf.nn.embedding_lookup(self.embe_dict, self.inputs,max_norm=1)\n",
    "        \n",
    "        # Get embeddings for last products\n",
    "        lastinputs= tf.nn.embedding_lookup(self.embe_dict, self.last_inputs,max_norm=1)\n",
    "\n",
    "        org_memory = inputs\n",
    "\n",
    "        # Calculate m_s (average of embeddings see Figure 2 in paper)\n",
    "        pool_out = pooler(\n",
    "            org_memory,\n",
    "            'mean',\n",
    "            axis=1,\n",
    "            sequence_length = tf.cast(tf.reshape(self.sequence_length,[batch_size, 1]), tf.float32))\n",
    "        \n",
    "        pool_out = tf.reshape(pool_out,[-1,self.hidden_size])\n",
    "\n",
    "        # Apply attention\n",
    "        attlayer = FwNnAttLayer(\n",
    "            self.edim,\n",
    "            active=self.active,\n",
    "            stddev=self.stddev,\n",
    "            norm_type='none')\n",
    "        \n",
    "        attout, alph= attlayer.forward(org_memory,lastinputs,pool_out,sent_bitmap)\n",
    "        attout = tf.reshape(attout,[-1,self.edim]) + pool_out\n",
    "        self.alph = tf.reshape(alph,[batch_size,1,-1])\n",
    "\n",
    "        # MLP Cell A\n",
    "        self.w1 = tf.Variable(\n",
    "            tf.random_normal([self.edim, self.edim], stddev=self.stddev),\n",
    "            trainable=True)\n",
    "\n",
    "        # MLP Cell B\n",
    "        self.w2 = tf.Variable(\n",
    "            tf.random_normal([self.edim, self.edim], stddev=self.stddev),\n",
    "            trainable=True)\n",
    "        \n",
    "        attout = tf.tanh(tf.matmul(attout,self.w1))\n",
    "        # attout = tf.nn.dropout(attout, self.output_keep_probs)\n",
    "        \n",
    "        lastinputs= tf.tanh(tf.matmul(lastinputs,self.w2))\n",
    "        # lastinputs= tf.nn.dropout(lastinputs, self.output_keep_probs)\n",
    "        \n",
    "        # Part of trilinear\n",
    "        prod = attout * lastinputs\n",
    "        \n",
    "        # Scores of all products in embeddings dictionary\n",
    "        sco_mat = tf.matmul(prod,self.embe_dict[1:], transpose_b= True)\n",
    "        self.softmax_input = sco_mat\n",
    "        \n",
    "        # For inference, it's not used for training, but it's saved during training and\n",
    "        # then loaded for inference\n",
    "        sco_mat_inference = tf.matmul(prod, self.business_rules_embeddings[1:], transpose_b=True)\n",
    "        self.softmax_input_inference = sco_mat_inference\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sco_mat, labels=self.lab_input)\n",
    "\n",
    "        # Optimization of the loss function\n",
    "        self.params = tf.trainable_variables()\n",
    "        self.optimize = super(Seq2SeqAttNN, self).optimize_normal(self.loss, self.params)\n",
    "```\n",
    "\n",
    "## STAMP GIT repo\n",
    "\n",
    "https://github.com/uestcnlp/STAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for recommendations\n",
    "\n",
    "```bash\n",
    "conda create -n transformers4rec-23.04 -c nvidia -c rapidsai -c pytorch -c conda-forge \\\n",
    "    transformers4rec=23.04 `# NVIDIA Merlin` \\\n",
    "    nvtabular=23.04 `# NVIDIA Merlin - Used in example notebooks` \\\n",
    "    python=3.10 `# Compatible Python environment` \\\n",
    "    cudf=23.02 `# RAPIDS cuDF - GPU accelerated DataFrame` \\\n",
    "    cudatoolkit=11.8 pytorch-cuda=11.8 `# NVIDIA CUDA version`\n",
    "```\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/gru_based.png\" width=1000>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/masking.png\" width=1000>\n",
    "</p>\n",
    "\n",
    "```python\n",
    "from transformers4rec import torch as tr\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt\n",
    "\n",
    "# Create a schema or read one from disk: tr.Schema().from_json(SCHEMA_PATH).\n",
    "schema: tr.Schema = tr.data.tabular_sequence_testing_data.schema\n",
    "\n",
    "max_sequence_length, d_model = 20, 64\n",
    "\n",
    "# Define the input module to process the tabular input features.\n",
    "input_module = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    continuous_projection=d_model,\n",
    "    aggregation=\"concat\",\n",
    "    masking=\"causal\",\n",
    ")\n",
    "\n",
    "# Define a transformer-config like the XLNet architecture.\n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=d_model, n_head=4, n_layer=2, total_seq_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = tr.SequentialBlock(\n",
    "    input_module,\n",
    "    tr.MLPBlock([d_model]),\n",
    "    tr.TransformerBlock(transformer_config, masking=input_module.masking)\n",
    ")\n",
    "\n",
    "# Define the evaluation top-N metrics and the cut-offs\n",
    "metrics = [NDCGAt(top_ks=[20, 40], labels_onehot=True),\n",
    "           RecallAt(top_ks=[20, 40], labels_onehot=True)]\n",
    "\n",
    "# Define a head with NextItemPredictionTask.\n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, metrics=metrics),\n",
    "    inputs=input_module,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model class.\n",
    "model = tr.Model(head)\n",
    "```\n",
    "\n",
    "## Examples\n",
    "\n",
    "https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/examples/tutorial"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
