{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Improving deep neural networks\n",
    "\n",
    "</font>\n",
    "\n",
    "- Optimization Methods\n",
    "    - Mini-Batch Gradient descent\n",
    "    - Exponentially weighted averages\n",
    "    - Gradient descent with momentum\n",
    "    - Bias correction in exponentially weighted averages\n",
    "    - RMSProp (root mean square prop)\n",
    "    - Adam\n",
    "    - Learning rate decay\n",
    "- Multi class classification    \n",
    "\n",
    "- Bias and Variance  \n",
    "- Regularization: Weight decay \n",
    "- Regularition: dropout\n",
    "- Getting more data\n",
    "- Early stopping\n",
    "- Vanishing / Exploding gradients\n",
    "    - Changing initialization \n",
    "    - Using non-saturation functions \n",
    "    - Batch normalization \n",
    "    - Gradient clipping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Optimization Methods\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Recall Gradient Descent\n",
    "\n",
    "</font>\n",
    "\n",
    "Gradient steps are taken with respect to all $m$ examples on each step, it is also called Batch Gradient Descent.\n",
    "\n",
    "For $l = 1, ..., L$: \n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\cdot \\frac{d\\mathcal{L}}{\\partial W^{[l]}} \\quad b^{[l]} = b^{[l]} - \\alpha \\cdot \\frac{d\\mathcal{L}}{\\partial b^{[l]}} $$\n",
    "\n",
    "``` python\n",
    "\n",
    "# Initialize_parameters\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "# Loop (gradient descent)\n",
    "for i in range(num_iterations):\n",
    "    # Forward propagation \n",
    "    A_last, caches = forward_propagation_whole_process(X, parameters, keep_prob)\n",
    "    # Compute cost\n",
    "    cost = compute_cost_with_regularization(A_last, Y, parameters, lambd)\n",
    "    # Backward propagation\n",
    "    grads = backward_propagation_whole_process(A_last, Y, caches, lambd, keep_prob)\n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "</font>\n",
    "\n",
    "Gradient steps are taken with respect to each single $m$ example on each step.\n",
    "``` python\n",
    "\n",
    "# Initialize_parameters\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "# Loop (gradient descent)\n",
    "for i in range(num_iterations):\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation \n",
    "        A_last, caches = forward_propagation_whole_process(X, parameters, keep_prob)\n",
    "        # Compute cost\n",
    "        cost = compute_cost_with_regularization(A_last, Y, parameters, lambd)\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation_whole_process(A_last, Y, caches, lambd, keep_prob)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"data/stochastic.png\" align = 'left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Mini-Batch Gradient descent\n",
    "\n",
    "</font>\n",
    "\n",
    "Gradient steps are taken with respect to $k$ of $m$ example on each step.\n",
    "<img src = \"data/mini_batch.png\" align = 'left' >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Mini-batch implementation \n",
    "\n",
    "</font>\n",
    "\n",
    "1. Shuffle samples \n",
    "2. Split to mini-batches \n",
    "\n",
    "<img src = \"data/batches.png\" align = 'left' height = '500' width = '500'>\n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "``` python\n",
    "\n",
    "# Initialize_parameters\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "# Loop (gradient descent for epochs)\n",
    "for i in range(num_epochs):    \n",
    "    # Define the random minibatches\n",
    "    minibatches = random_mini_batches(X, Y, minibatch_size, seed)\n",
    "    # Loop (gradient descent for minibatches)\n",
    "    for minibatch in minibatches:\n",
    "        # Select a minibatch\n",
    "        (minibatch_X, minibatch_Y) = minibatch\n",
    "        # Forward propagation \n",
    "        A_last, caches = forward_propagation_whole_process(minibatch_X, parameters, keep_prob)\n",
    "        # Compute cost\n",
    "        cost = compute_cost_with_regularization(A_last, minibatch_Y, parameters, lambd)\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation_whole_process(A_last, minibatch_Y, caches, lambd, keep_prob)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    X - input data, of shape (input size, number of examples)\n",
    "    Y - true \"label\" vector (1/0), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    list o mini_batch_X, mini_batch_Y\n",
    "    \"\"\"    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    #  Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Split to complete mini batches\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, mini_batch_size * k : mini_batch_size * (k+1)]\n",
    "        mini_batch_Y = shuffled_Y[:, mini_batch_size * k : mini_batch_size * (k+1)]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, m - (m//mini_batch_size) * mini_batch_size:]\n",
    "        mini_batch_Y =  shuffled_Y[:, m - (m//mini_batch_size) * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Exponentially weighted averages\n",
    "\n",
    "</font>\n",
    "\n",
    "$\n",
    "v_{0} = 0\\\\\n",
    "v_{l} = \\beta \\cdot v_{0} + (1 - \\beta)\\cdot t^{l} \\\\\n",
    "v_{2} = \\beta \\cdot v_{1} + (1 - \\beta)\\cdot t^{2} \\\\\n",
    "v_{3} = \\beta \\cdot v_{2} + (1 - \\beta)\\cdot t^{3} \\\\\n",
    "...\\\\\n",
    "v_{n} = \\beta \\cdot v_{n-1} + (1 - \\beta)\\cdot t^{n} \\\\\n",
    "$\n",
    "\n",
    "Sample for $\\beta =0.9$\n",
    "\n",
    "<img src = \"data/temperature_2.png\" align = 'left' height = '400' width = '400'>\n",
    "<img src = \"data/temperature_3.png\" align = 'left' height = '400' width = '400'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Gradient descent with momentum\n",
    "\n",
    "</font>\n",
    "\n",
    "For $l = 1, ..., L$: \n",
    "$$\n",
    "v_{W}^{[l]} = \\beta \\cdot  v_{W}^{[l]} + (1 - \\beta) \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W{[l]}} \n",
    "\\quad \\rightarrow \\quad W^{[l]} = W^{[l]} - \\alpha \\cdot v_{W}^{[l]}\\\\\n",
    "v_{b}^{[l]} = \\beta \\cdot  v_{b}^{[l]} + (1 - \\beta) \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b{[l]}} \n",
    "\\quad \\rightarrow \\quad b^{[l]} = b^{[l]} - \\alpha \\cdot v_{b}^{[l]}\\\\\n",
    "$$\n",
    "\n",
    "Note: \n",
    "- If $\\beta = 0$, then this just becomes standard gradient descent without momentum.\n",
    "- $\\beta = 0.9$ is often a reasonable default value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Bias correction in exponentially weighted averages\n",
    "\n",
    "</font>\n",
    "\n",
    "$$v = \\beta \\cdot v + (1 - \\beta)\\cdot t^{n} \\quad \\rightarrow \\quad \n",
    "{v\\,}^{corrected} = \\frac{v}{1 - \\beta^t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## RMSProp (root mean square prop)\n",
    "\n",
    "</font>\n",
    "\n",
    "For $l = 1, ..., L$: \n",
    "$$\n",
    "s_{W}^{[l]} = \\beta \\cdot s_{W}^{[l]} + (1 - \\beta) \\cdot (\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]} })^2 \n",
    "\\quad\\quad\\quad\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{W^{[l]}}{\\sqrt{s_{W}^{[l]}} + \\varepsilon}\n",
    "\\\\\n",
    "s_{b}^{[l]} = \\beta \\cdot s_{b}^{[l]} + (1 - \\beta) \\cdot (\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]} })^2 \n",
    "\\quad\\quad\\quad\n",
    "b^{[l]} = b^{[l]} - \\alpha \\frac{b^{[l]}}{\\sqrt{s_{b}^{[l]}} + \\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Adam \n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum. \n",
    "\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
    "\n",
    "The update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\n",
    "v_{W}^{[l]} = \\beta_1 \\cdot  v_{W}^{[l]} + (1 - \\beta_1)\\cdot   \\frac{\\partial \\mathcal{L} }{ \\partial W^{[l]} }; \n",
    "\\quad\\quad\\quad\n",
    "(v_{W}^{[l]})^{corrected} = \\frac{v_{W}^{[l]}}{1 - (\\beta_1)^t}\\\\\n",
    "s_{W}^{[l]} = \\beta_2 \\cdot s_{W}^{[l]} + (1 - \\beta_2) \\cdot (\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]} })^2 \n",
    "\\quad\\quad\\quad\n",
    "(s_{W}^{[l]})^{corrected} = \\frac{s_{W}^{[l]}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{(v_{W}^{[l]})^{corrected}}{\\sqrt{(s_{W}^{[l]})^{corrected}} + \\varepsilon}\n",
    "\\\\\n",
    "v_{b}^{[l]} = \\beta_1 \\cdot  v_{b}^{[l]} + (1 - \\beta_1)\\cdot   \\frac{\\partial \\mathcal{L} }{ \\partial b^{[l]} }; \n",
    "\\quad\\quad\\quad\n",
    "(v_{b}^{[l]})^{corrected} = \\frac{v_{b}^{[l]}}{1 - (\\beta_1)^t}\\\\\n",
    "s_{b}^{[l]} = \\beta_2 \\cdot s_{b}^{[l]} + (1 - \\beta_2) \\cdot (\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]} })^2 \n",
    "\\quad\\quad\\quad\n",
    "(s_{b}^{[l]})^{corrected} = \\frac{s_{b}^{[l]}}{1 - (\\beta_2)^t} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha \\frac{(v_{b}^{[l]})^{corrected}}{\\sqrt{(s_{b}^{[l]})^{corrected}} + \\varepsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- t counts the number of steps taken of Adam \n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Learning rate decay \n",
    "\n",
    "</font>\n",
    "\n",
    "Using minibatch gradient descent you may need to decrease $\\alpha$. \n",
    "\n",
    "There are veriaty of implementations\n",
    "e.g.\n",
    "\n",
    "$\\alpha_{0} = 0.2;  \\quad\\ \\text{decay_rate} = 1; \\quad\\quad\\quad \\alpha = \\frac{1}{1+ \\text{epoch_number} \\cdot \\text{decay_rate}} \\cdot \\alpha_{0}$ \n",
    "\n",
    "or \n",
    "\n",
    "$\\alpha =  0.95 ^{\\text{epoch_number}} \\cdot \\alpha_{0}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "    \n",
    "# Multi class classification\n",
    "\n",
    "## Softmax Regression\n",
    "</font>\n",
    " \n",
    " \n",
    "For binary classification there is single unit in the last layer $L$ and sigmoid function $\\sigma$.\n",
    "<br>For multi class classification the last layer has $n^{[L]} = n_{c}$ units, where $n_{c}$ is number of classes.\n",
    "\n",
    "The output $Z^{[L]}$ is converted to probability distribution by following formula:\n",
    "$$A^{[L]} = \\frac{{e\\,}^{Z^{[L]}}} {\\| Z^{[L]} \\|} \\quad \\text{i.e.}\\quad a^{[L]_{i}} = \\frac{t_{i}} {\\sum \\limits_{k} t_{k}} \n",
    "\\text{ where } t = {e\\,}^{z^{[L]}}$$\n",
    "e.g.\n",
    "\n",
    "$Z^{[L]} = \\begin{bmatrix} 5 \\\\ 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\quad\\rightarrow\\quad  t =\\begin{bmatrix} e^{5}\\\\ e^{2}\\\\ e^{-1}\\\\ e^{3} \\end{bmatrix}  \n",
    "\\quad\\rightarrow\\quad A^{[L]}= \\begin{bmatrix}0.84203357\\\\  0.04192238\\\\ 0.00208719\\\\  0.11395685 \\end{bmatrix}\n",
    "\\quad\\rightarrow\\quad  \\hat{y} = argmax \\, A^{[L]} \\\\ \\text{} $\n",
    "\n",
    "\n",
    "<font color = green >\n",
    "    \n",
    "### Training softmax classification\n",
    "</font>\n",
    " \n",
    "\n",
    "\n",
    "$$y=\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\quad \\quad \\hat { y } =\\begin{bmatrix} 0.5 \\\\ 0.1 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix}\\\\\\text{ }\\\\ \\mathcal{ L }(y,\\hat { y } )=-\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ y_{ i }\\, log\\hat { y_{ i } }  }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "</font>\n",
    "\n",
    "**High bias**  (underfitting): train classification error is high and dev classification error is high.\n",
    "<br>**High variance** (overfitting): train classification error is low but dev classification error is high.\n",
    "<br>**Optimal** : train classification error is low and dev classification error is low.\n",
    "<br><br>Note: need to consider **human classification error** e.g. \n",
    "<br>train classification error = 20% , dev classification error = 22% is **high bias** when human classification error is close to zero,<br> but it is close to **optimal classification** when human classification error is close to 20%.\n",
    "\n",
    "\n",
    "Main machine learning appoach: Resolve **high bias first**, then resolve **high variance**.\n",
    "\n",
    "The following can resolve **high bias**: \n",
    "- build larger network (more layers/more units) \n",
    "- train longer \n",
    "- apply advanced optimization algorithm\n",
    "- reduce learning rate \n",
    "- reduce regularization \n",
    "- normalize input \n",
    "- use advanced params initialization\n",
    "- use other network architecture \n",
    "- use more/other features\n",
    "\n",
    "The following can resolve **high variance**: \n",
    "- get more data (consider augmentated data)\n",
    "- increase regularization \n",
    "- use other network architecture \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Regularization: Weight decay (frobenius norm/ L2 norm regularization )\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))\\quad\\rightarrow \\quad\n",
    "\\mathcal{L}_{F} = \\mathcal{L} +  \\frac{\\lambda}{2m} \\sum\\limits_{l = 1}^{L}  \\sum\\limits_{i = 1}^{n^{[l]}}  \\sum\\limits_{j = 1}^{n^{[l-1]}} (w^{[l]}_{ij}) ^2\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad\\rightarrow \\quad \n",
    "\\frac{\\partial \\mathcal{L}_{F} }{\\partial W^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}}+ \\frac{\\lambda}{m} W^{[l]}\\\\\n",
    "\\quad \\\\\n",
    "W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad=\\quad W^{[l]} - \\alpha \\cdot (\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}}+ \\frac{\\lambda}{m} W^{[l]}) \\quad = \\quad W^{[l]} \\cdot (1 -\\frac{\\alpha \\lambda}{m}) - \\alpha \\cdot \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}}\n",
    "\\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Implementation\n",
    "\n",
    "</font>\n",
    "\n",
    "- Develop cost with regularization\n",
    "- Correct the backward propagation \n",
    "- Correct the training model to consider lambda and new cost with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Regularization: dropout\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_dropout2.png\" align = 'left' height = '500' width = '500'>\n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "Note: Dropout is only on training model stage but not at predicting \n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Implementation\n",
    "\n",
    "</font>\n",
    "\n",
    "Define `keep_prob` e.g. `0.8` that means to shutdown approximately `20%` of units. In forward propagation correct computing post-activation: \n",
    "\n",
    "- Generate random matrix of shape $A$:<br> \n",
    "    $\\quad $ `D = np.random.rand(A.shape[0], A.shape[1])` \n",
    "    \n",
    "- convert entries of D to 0 or 1 (using keep_prob as the threshold):<br>\n",
    "    $\\quad $ `D = D <keep_prob` \n",
    "    \n",
    "- shut down some neurons of A:<br>\n",
    "    $\\quad $`A = A * D`\n",
    "    \n",
    "- scale the value of neurons that haven't been shut down (**Inverted dropout**):<br>\n",
    "    $\\quad$ `A = A / keep_prob` \n",
    "\n",
    "Note: \n",
    "- You may apply different thresshold (`keep_prob`) values to defferent layers  \n",
    "- Don't apply dropout to the last layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Data Augmentation to reduce the variance \n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_cat.png\" align = 'left' height = '300' width = '300'>\n",
    "\n",
    "<img src = \"data/19_cat_2.png\" align = 'left' height = '300' width = '300'>\n",
    "<img src = \"data/19_cat_3.png\" align = 'left' height = '300' width = '300'>\n",
    "<img src = \"data/19_cat_4.png\" align = 'left' height = '900' width = '900'>\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Early stopping\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_early_stop.png\" align = 'left' height = '500' width = '500'>\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Vanishing / Exploding gradients\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_van.png\" align = 'left' height = '500' width = '500'>\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "Simplified case: all $b^{[l]}=0$ and all $g^{[l]}(z) = z$\n",
    "<br>Then $\\hat{y} = W^{[1]} \\, @ \\, W^{[1]} \\, @ \\, W^{[2]} \\, @ \\, ... \\, @ \\, W^{[L]} \\, @ \\, X$\n",
    "\n",
    "<br>Let's assume the init $W^{[l]} = \\begin{bmatrix} 0.9 & 0 \\\\ 0 & 0.9 \\end{bmatrix}$  Then $\\hat{y} =  \\begin{bmatrix} 0.9 & 0 \\\\ 0 & 0.9 \\end{bmatrix}^{L-1} @ W^{[L]}  @  X $\n",
    "<br>If $L$ is large then the gradients of first layers will be very small and the gradient steps is very little productive, thus the convergence is very slow.\n",
    "\n",
    "<br>\n",
    "In practice there is also case of exploding gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal case of learning weight**\n",
    "\n",
    "<img src = \"data/vanish_1.png\" align = 'left' height = '550' width = '550'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanishing case of learning weight**\n",
    "\n",
    "<img src = \"data/vanish_2.png\" align = 'left' height = '550' width = '550'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to chain rule of calculating the derivatives, in the calculation there is product of lots of multipliers**\n",
    "\n",
    "<img src = \"data/vanish_3.png\" align = 'left' height = '600' width = '600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Solutions for vanishing/explosing gradient\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "- Changing initialization \n",
    "- Using non-saturation functions \n",
    "- Batch normalization \n",
    "- Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Changing initialization \n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "Normalizing inputs allows to avoid exploding gradients and speeds up the training process\n",
    "\n",
    "<img src = \"data/19_norm_1.png\" align = 'left' height = '400' width = '400'>\n",
    "<img src = \"data/19_norm_3.png\" align = 'left' height = '400' width = '400'>\n",
    "<img src = \"data/19_norm_2.png\" align = 'left' height = '300' width = '300'>\n",
    "\n",
    "<img src = \"data/19_norm_4.png\" align = 'left' height = '350' width = '350'>\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the size of layer $l$ is large then the $Z^{[l]} = W^{[l]} @ A^{[l-1]}+ b^{[l]}$ is very large. e.g. for single unit:  $z= w_{1} x_{1} + w_{2} x_{2} + ...+  w_{n} x_{n}$, to avoid output of layer very large, need to initialize the parameters being based on $n$: the larger $n$ the smaller $W$  \n",
    "<br> One of approach is to use **Xavier (Glorot) initialization**: $$W^{[l]} = random \\cdot (\\frac{1}{\\sqrt{(n^{[l-1]}}}) $$\n",
    "\n",
    "or even modified version:  <br> $$W^{[l]} = random \\cdot (\\frac{1}{\\sigma}),\\quad \\text{where} ,\\quad\n",
    "\\sigma^2 = \\frac{ n^{[l-1]}+ n^{[l]}}{2} $$\n",
    "\n",
    "<img src = \"data/xavier.png\" align = 'left' height = '700' width = '700'>\n",
    "<img src = \"data/init_1.png\" align = 'left' height = '700' width = '700'>\n",
    "<img src = \"data/init_2.png\" align = 'left' height = '700' width = '700'>\n",
    "<img src = \"data/init_3.png\" align = 'left' height = '500' width = '500'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Using non-saturation functions\n",
    "\n",
    "<img src = \"data/satur_0.png\" align = 'left' height = '700' width = '700'>\n",
    "<img src = \"data/satur_1.png\" align = 'left' height = '337' width = '337'>\n",
    "<img src = \"data/satur_2.png\" align = 'left' height = '400' width = '400'>\n",
    "  \n",
    "</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Batch normalization \n",
    "   \n",
    "</font>\n",
    "\n",
    "<img src = \"data/batch_norm_2.png\" align = 'left' height = '700' width = '700'>    \n",
    "<img src = \"data/batch_norm.png\" align = 'left' height = '700' width = '700'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "- Batch n ormalization works on batch level  not on all data. But it uses rolling average i.e. calculates the mean for first batch lets say it equals to `10` and then for second batch `12` and then it uses `11` to estimate the mean of whole data set (on certain layer)\n",
    "\n",
    "- $\\gamma$ and $\\beta$  are trainable as model parameters \n",
    "- $\\mu$ and $\\sigma$ are  learned as average during the training \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Gradient clipping\n",
    "   \n",
    "</font>\n",
    "\n",
    "\n",
    "#### Clip by threshold \n",
    "Clip every value in gradient vector to be in certain thresold e.g. `-1` and `1` \n",
    "\n",
    "<img src = \"data/clipping.png\" align = 'left' height = '400' width = '400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip by norm \n",
    "\n",
    "Clip values by norm to keep the same direction of gradient\n",
    "\n",
    "\n",
    "<img src = \"data/clipping_2.png\" align = 'left' height = '400' width = '400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Clip by gradsient may bring small numbers and small learning.  The solution is to experiment with thresholds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[What is Vanishing/Exploding Gradients Problem in NNs](https://youtu.be/2f_45VzKEfE?si=ROktoR-2OyUH-sgN)\n",
    "\n",
    "[How to Choose the Correct Initializer for your Neural Network](https://youtu.be/ix3FcOaU6UI?si=AxSBpbu_RpshlOAG)\n",
    "\n",
    "[How to Choose an Activation Function for Neural Networks](https://youtu.be/_UjA9sk_fK4?si=xXFPi1LfTmwB3ThU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
