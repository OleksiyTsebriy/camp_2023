{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Deep learning\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<img src = \"data/19_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Notation\n",
    "\n",
    "</font>\n",
    "\n",
    "$n_{x}$ - number of features <br>\n",
    "$m$ - number of samples<br>\n",
    "$X$ - input features of shape = $(n_{x}, m)$ <br> \n",
    "$Y$ - labels of shape = $(1, m)$ <br> \n",
    "$L$ - number of layers (excluding input layer)<br> \n",
    "Index $[l]$ corresponds to layer number $l \\in (1...L)$ <br> \n",
    "Index $(i)$ corresponds to sample number $i \\in (1,m) $<br> \n",
    "Bottom index corresponds to unit number e.g. $a^{[2](3)}_{4}$ is the 4th activation unit in 2nd layer of 3rd sample<br> \n",
    "$n^{[l]}$  - number of units in layer $l$<br> \n",
    "$g^{[l]}$ - activation function of layer $l$ <br>\n",
    "$A^{[l]} = g^{[l]}(Z^{[l]})$  - pos-activation values of layer $l \\in (1...L)$  ($A^{[0]} = X$ ) <br>\n",
    "$W^{[l]}$ and $b^{[l]} $  - weights and bias of layer $l$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Shapes\n",
    "\n",
    "</font>\n",
    "\n",
    "#### Sample for 4 layer neural network \n",
    "\n",
    "\n",
    "<img src = \"data/19_2.png\" align = 'left'>\n",
    "<br>$\n",
    "\\quad n^{[0]} = n_{x} =  2\\\\\n",
    "\\quad n^{[1]} = 3\\\\\n",
    "\\quad n^{[2]} = 5\\\\\n",
    "\\quad n^{[3]} = 4\\\\\n",
    "\\quad n^{[4]} = 2\\\\\n",
    "\\quad n^{[5\n",
    "]} = 1$\n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Shape of $\\quad W^{[1]} = (n^{[1]}, n^{[0]}) = (3,2)\\quad\\quad\\quad$     Shape of $\\quad b^{[1]} = (n^{[1]}, 1) = (3,1)$ <br>\n",
    "Shape of $\\quad W^{[2]} = (n^{[2]}, n^{[1]})= (5,3)\\quad\\quad\\quad$     Shape of $\\quad b^{[2]} = (n^{[2]}, 1) = (5,1)$ <br>\n",
    "Shape of $\\quad W^{[3]} = (n^{[3]}, n^{[2]})= (4,5)\\quad\\quad\\quad$     Shape of $\\quad b^{[3]} = (n^{[3]}, 1) = (4,1)$ <br>\n",
    "Shape of $\\quad W^{[4]} = (n^{[4]}, n^{[3]})= (2,4)\\quad\\quad\\quad$     Shape of $\\quad b^{[4]} = (n^{[4]}, 1) = (2,1)$ <br>\n",
    "Shape of $\\quad W^{[5]} = (n^{[5]}, n^{[4]})= (1,2)\\quad\\quad\\quad$     Shape of $\\quad b^{[5]} = (n^{[5]}, 1) = (1,1)$ <br>\n",
    "<br>In general:\n",
    "<br>Shape of $\\quad W^{[l]} = (n^{[l]}, n^{[l-1]})\\,\\quad$     Shape of $\\quad b^{[l]} = (n^{[l]}, 1)$ \n",
    "\n",
    "<br>Shape of $$\\quad A^{[l]}, Z^{[l]}, \\frac{d\\mathcal{L}}{\\partial a^{[l]}}, \\frac{\\partial\\mathcal{L}}{\\partial z^{[l]}} = (n^{[l]}, m) \\quad,\\quad  \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} = (n^{[l]}, 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation \n",
    "\n",
    "</font>\n",
    "\n",
    "Whole process: \n",
    "\\[LINEAR -> RELU\\] $\\times$ (L-1) -> \\[LINEAR -> SIGMOID\\]\n",
    "\n",
    "\n",
    "$A^{[0]} = X$ - input layer \n",
    "\n",
    "Iterate for all $l \\in (1.. L)$:\n",
    "\n",
    "$\\quad\\quad\\quad A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$, where $g(Z)$ is one of activation functions: \n",
    "\n",
    "$$\\sigma(Z) = \\frac{1}{ 1 + e^{-Z}}, \\quad\\quad\\quad RELU(Z) = max(0, Z)$$\n",
    "\n",
    "$\\hat{Y} = A^{[L]}$ - output layer (predicted value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Cost Function\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation\n",
    "\n",
    "</font>\n",
    "\n",
    "Compute the derivative for last layer: \n",
    "$$\\frac{\\partial \\mathcal{L}}{dA^{[L]}} =  -\\frac{Y}{A^{[L]}} + \\frac{1 - Y}{1 - A^{[L]}}$$\n",
    "\n",
    "Iterate through all layres back:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}} \\cdot \n",
    "\\frac{\\partial g^{[L]}}{\\partial z}(Z^{[L]}); \\quad \n",
    "\\sigma\\,(z)=\\frac {1}{1+{e}^{-z}} \\Rightarrow \n",
    "\\frac { d\\sigma }{ dz } = \\sigma(z)(1-\\sigma(z));\n",
    "\\quad  \n",
    "RELU(z) = max(0, z)\\Rightarrow \n",
    "\\frac { d}{dz}(RELU) = \\begin{cases} 0, z \\le 0 \\\\ 1,\\quad z > 0\\quad \\end{cases}\n",
    "\\\\ \\quad \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\quad (axis=1)\\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} \\,@\\, \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward and backward propagation diagram\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "<img src = \"data/19_forward_backward.png\" align = 'left'>\n",
    "\n",
    "\n",
    "$$\\quad$$\n",
    "\n",
    "Note: It may be worth to cashe the $b^{[l]}$ parameters with other $ W^{[l]} ,A^{[l-1]}, Z^{[l]}$ to make sure the shape of $\\frac {\\partial\\mathcal{L}}{\\partial b^{[l]}}$ is the same as shape of $b^{[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Garadient descent \n",
    "\n",
    "</font>\n",
    "\n",
    "For  $l \\in (1.. L)$:\n",
    "\n",
    "$$W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad\\quad\n",
    "b^{[l]}  = b^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Implementation steps \n",
    "\n",
    "</font>\n",
    "\n",
    " - Prepare data \n",
    " - Initialize parameters \n",
    " - Implement forward propagation step \n",
    " - Сompute сost\n",
    " - Init backward propagation\n",
    " - Implement backward propagation step\n",
    " - Update parameters in gradient descent \n",
    " - (Build two-layer model)\n",
    " - Train model \n",
    " - Evaluate model\n",
    " - Implement forward propagation whole process \n",
    " - Implement backward propagation whole process \n",
    " - Build deep neural network model \n",
    " - Predict \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "</font>\n",
    "\n",
    "Cat vs Non-cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "### Load data\n",
    "\n",
    "</font>\n",
    "\n",
    "Cat vs Non-cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    file_name=  os.path.join(path , 'train_catvnoncat.h5')\n",
    "    train_dataset = h5py.File(file_name, \"r\")\n",
    "    X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    file_name=  os.path.join(path , 'test_catvnoncat.h5')\n",
    "    test_dataset = h5py.File(file_name, \"r\")\n",
    "    X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = ['non-cat','cat']\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Review samples\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train, X_test, Y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 11\n",
    "# Implement the code to review the picture with index = 11 and print the labeled class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/cat_11.png\" >\n",
    "\n",
    "y = 1, it's a 'cat' picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Analyze dimensions   \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the code to print the outpus provided in expected result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "    X_train.shape=  (209, 64, 64, 3)\n",
    "    X_test.shape=  (50, 64, 64, 3)\n",
    "    Y_train.shape=  (209,)\n",
    "    Y_test.shape=  (50,)\n",
    "    Number of training examples: m_train = 209\n",
    "    Number of testing examples: m_test = 50\n",
    "    Height/Width of each image: num_px = 64\n",
    "    Each image is of size: (64, 64, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Reshape data\n",
    "\n",
    "</font>\n",
    "\n",
    "$X$ - input features of shape = $(n_{x}, m)$ <br> \n",
    "$Y$ - labels of shape = $(1, m)$ <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data \n",
    "X_train_flatten = 0 \n",
    "X_test_flatten = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "    X_train_flatten shape: (12288, 209)\n",
    "    Y_train shape: (1, 209)\n",
    "    X_test_flatten shape: (12288, 50)\n",
    "    Y_test shape: (1, 50)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "###  Scale data\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_flatten/255.\n",
    "X_test_scaled = X_test_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Initialize parameters\n",
    "\n",
    "</font>\n",
    "<br>Shape of $\\quad W^{[l]} = (n^{[l]}, n^{[l-1]})\\,\\quad$     Shape of $\\quad b^{[l]} = (n^{[l]}, 1)$ \n",
    "\n",
    "Note: Initialization of $W = random  \\,/   \\sqrt{ n^{[l-1]} }  $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    layer_dims - list containing the dimensions of each layer in our network including input layer e.g. [12288,7,1]\n",
    "    Returns: dictionary with keys \"W\" and \"b\" and their values are dicts with keys corresponding to layers numbers.\n",
    "        for 'W' - value for every layer is weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        for 'b' - bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"    \n",
    "    np.random.seed(1)\n",
    "    parameters = {'W':{}, 'b':{}}\n",
    "\n",
    "    # Implement initialization using np.random.randn to match expected result\n",
    "    \n",
    "    parameters = None    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/_vz5kwcn2bxbjntlwb3gq_ph0000gp/T/ipykernel_3330/760173318.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'W[{0}] =\\n{1}\\nb[{0}] =\\n{2}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# check the initialize_parameters()\n",
    "layer_dims= [2,3,5,1] \n",
    "params = initialize_parameters(layer_dims)\n",
    "for l in range(1,len(layer_dims)):\n",
    "    print ('W[{0}] =\\n{1}\\nb[{0}] =\\n{2}\\n'.format(l, params['W'][l], params['b'][l] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "    W[1] =\n",
    "    [[ 1.14858562 -0.43257711]\n",
    "     [-0.37347383 -0.75870339]\n",
    "     [ 0.6119356  -1.62743362]]\n",
    "    b[1] =\n",
    "    [[0.]\n",
    "     [0.]\n",
    "     [0.]]\n",
    "    W[2] =\n",
    "    [[ 1.00736754 -0.43948301  0.18419731]\n",
    "     [-0.14397405  0.84414841 -1.18942279]\n",
    "     [-0.18614766 -0.22173389  0.65458209]\n",
    "     [-0.63502252 -0.09955147 -0.50683179]\n",
    "     [ 0.02437212  0.33648852 -0.63544278]]\n",
    "    b[2] =\n",
    "    [[0.]\n",
    "     [0.]\n",
    "     [0.]\n",
    "     [0.]\n",
    "     [0.]]\n",
    "    W[3] =\n",
    "    [[ 0.51193601  0.40320363  0.2247223   0.40287503 -0.30577239]]\n",
    "    b[3] =\n",
    "    [[0.]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation step\n",
    "\n",
    "</font>\n",
    "\n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$, where $g(Z)$ is one of activation functions: \n",
    "\n",
    "$\\sigma(Z) = \\frac{1}{ 1 + e^{-Z}}, \\quad\\quad\\quad RELU(Z) = max(0, Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_step(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    A_prev - activations from previous layer: (size of previous layer, number of examples)\n",
    "    W - weights matrix: array of shape (size of current layer, size of previous layer)\n",
    "    b - bias vector, array of shape (size of the current layer, 1)\n",
    "    activation - text string \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -  post-activation value \n",
    "    cache - tuple containing W, b, A_prev, Z stored for computing the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = None\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A= None\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        A = None\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "\n",
    "    cache = (W, b, A_prev, Z) # used at backward propagation. Note: b looks as need just to check the shape of dJ_db\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Compute cost\n",
    "\n",
    "</font>\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    A_last - vector of predicted probabilties - activations of last layer L, shape (1, number of examples)\n",
    "    Y - true label e.g. cat vs non-cat, shape (1, number of examples)\n",
    "    Returns:\n",
    "    cost - cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    assert (A_last.shape == Y.shape)\n",
    "    cost = None\n",
    "\n",
    "    assert(cost.shape == ()) \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Init backward propagation \n",
    "\n",
    "</font>\n",
    "\n",
    "Compute the derivative for last layer: \n",
    "$$\\frac{\\partial \\mathcal{L}}{dA^{[L]}} =  -\\frac{Y}{A^{[L]}} + \\frac{1 - Y}{1 - A^{[L]}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_backward_propagation(Y, A_last):\n",
    "    dL_dA_last =  None\n",
    "    return dL_dA_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation step\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}} \\cdot \n",
    "\\frac{\\partial g^{[L]}}{\\partial z}(Z^{[L]}); \\quad \n",
    "\\frac { d\\sigma }{ dz } = \\sigma(z)(1-\\sigma(z));\n",
    "\\quad  \n",
    "\\frac { d}{dz}(RELU) = \\begin{cases} 0, z \\le 0 \\\\ 1,\\quad z > 0\\quad \\end{cases}\n",
    "\\\\ \\quad \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\quad (axis=1)\\quad \\quad  \n",
    "\\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} \\,@\\, \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_step(dL_dA, cache, activation):\n",
    "    \"\"\"\n",
    "    dL_dA - activation gradient for current layer l\n",
    "    cache - (W, b, A_prev, Z) stored for current layer  l\n",
    "    activation - string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dL_dA_prev - Gradient activation of the previous layer l-1, same shape as A_prev\n",
    "    dL_dW - Gradient of W current layer l, same shape as W\n",
    "    dL_db - Gradient of b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    W, b, A_prev, Z = cache \n",
    "\n",
    "    # backward activation part:\n",
    "    if activation == \"relu\":\n",
    "        dg_dz = None\n",
    "    elif activation == \"sigmoid\":\n",
    "        dg_dz = None\n",
    "        \n",
    "    assert (dL_dA.shape == dg_dz.shape)\n",
    "    dL_dZ = None\n",
    "\n",
    "    # backward linear part:\n",
    "   \n",
    "    dL_dW = None\n",
    "    dL_db = None\n",
    "    dL_dA_prev = None\n",
    "    \n",
    "\n",
    "    assert (dL_dA_prev.shape == A_prev.shape)\n",
    "    assert (dL_dW.shape == W.shape)\n",
    "    assert (dL_db.shape == b.shape)\n",
    "\n",
    "    return dL_dA_prev, dL_dW, dL_db\n",
    "        \n",
    "\n",
    "def relu_backward(Z):\n",
    "    dg_dz = None\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_backward(Z):\n",
    "\n",
    "    dg_dz = None\n",
    "    assert (dg_dz.shape == Z.shape)    \n",
    "    return dg_dz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Update parameters \n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "For  $l \\in (1.. L)$:\n",
    "\n",
    "$W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad\\quad\n",
    "b^{[l]}  = b^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters due to gradient descent rule \n",
    "    parameters - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \n",
    "    grads - dictionary with keys 'W' and 'b' each is dict with keys of layer numbers \n",
    "   \n",
    "    Returns: updated parameters the same shape as input parameters \n",
    "    \"\"\"\n",
    "   \n",
    "    parameters = None\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Build two-layer model\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    '''\n",
    "    X - input layer of shape (input size, number of examples)\n",
    "    Y - output layer of shape (1,m)\n",
    "    layers_dims - list of layers dims including input layer \n",
    "    '''\n",
    "  \n",
    "    np.random.seed(1)\n",
    "    grads = {'W':{}, 'b':{}}\n",
    "    costs = []   # track the cost\n",
    "    m = X.shape[1] # number of examples\n",
    "\n",
    "    # Initialize parameters \n",
    "    parameters = None\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(None):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        A1, cache1 = None\n",
    "        A2, cache2 = None\n",
    "\n",
    "        # Compute cost        \n",
    "        cost = None\n",
    "                \n",
    "        # Initialize backward propagation        \n",
    "        dL_dA2 = None\n",
    "\n",
    "        # Backward propagation.\n",
    "        dL_dA1, grads['W'][2], grads['b'][2] = None\n",
    "        _, grads['W'][1], grads['b'][1] = None\n",
    "        \n",
    "       \n",
    "        # Update parameters    \n",
    "        parameters =  None\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "    None\n",
    "\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Train the model learning\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_train_scaled.shape[0]\n",
    "n_h= 7 \n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x, n_h, n_y]\n",
    "\n",
    "parameters = two_layer_model(\n",
    "    X_train_scaled, Y_train, layers_dims, learning_rate = 0.003, num_iterations = 3000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "    Cost after iteration 0: 0.6950464961800915\n",
    "    Cost after iteration 100: 0.6195808854384666\n",
    "    Cost after iteration 200: 0.5865026104533535\n",
    "    Cost after iteration 300: 0.5467810398248231\n",
    "    Cost after iteration 400: 0.49825722524914073\n",
    "    Cost after iteration 500: 0.4565738532427634\n",
    "    Cost after iteration 600: 0.4094471539583378\n",
    "    Cost after iteration 700: 0.3631730375845946\n",
    "    Cost after iteration 800: 0.32861131098831003\n",
    "    Cost after iteration 900: 0.29718068617894683\n",
    "    Cost after iteration 1000: 0.269008932771751\n",
    "    Cost after iteration 1100: 0.24464399315834634\n",
    "    Cost after iteration 1200: 0.22384078814076205\n",
    "    Cost after iteration 1300: 0.20652180532903278\n",
    "    Cost after iteration 1400: 0.1893838186161027\n",
    "    Cost after iteration 1500: 0.17501499357058234\n",
    "    Cost after iteration 1600: 0.16159871226192452\n",
    "    Cost after iteration 1700: 0.14745343426258842\n",
    "    Cost after iteration 1800: 0.13478215612014718\n",
    "    Cost after iteration 1900: 0.1231337962128783\n",
    "    Cost after iteration 2000: 0.11228120642500024\n",
    "    Cost after iteration 2100: 0.10296514643048342\n",
    "    Cost after iteration 2200: 0.09462444340407362\n",
    "    Cost after iteration 2300: 0.08577949788493319\n",
    "    Cost after iteration 2400: 0.07838067328265923\n",
    "    Cost after iteration 2500: 0.07228557169893012\n",
    "    Cost after iteration 2600: 0.06685754016383366\n",
    "    Cost after iteration 2700: 0.06210220111202463\n",
    "    Cost after iteration 2800: 0.05787619296178021\n",
    "    Cost after iteration 2900: 0.053884527381680314\n",
    "\n",
    "<img src = \"data/19_check_2_layer.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Evaluate 2-layers model\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_two_layers(X, Y, parameters):\n",
    "    \"\"\"        \n",
    "    X - array to predict \n",
    "    parameters - parameters of the trained model\n",
    "    \"\"\"\n",
    "  \n",
    "    # Forward propagation\n",
    "    A1,_ = None\n",
    "    Y_pred, _ = None\n",
    "\n",
    "    accuracy =  None\n",
    "    print(\"Accuracy: {:.3f}\".format( accuracy))\n",
    "       \n",
    "\n",
    "evaluate_two_layers(X_train_scaled, Y_train, parameters)\n",
    "test_accuracy = evaluate_two_layers(X_test_scaled, Y_test, parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "    Accuracy: 1.000\n",
    "    Accuracy: 0.760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Forward propagation whole process\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation_whole_process(X, parameters):\n",
    "    \"\"\"\n",
    "    [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    X - data, array of shape (input size, number of examples)\n",
    "    parameters - initialized parameters foreach of 'W' and 'b' keas values have keys 1,2,...L \n",
    "    \n",
    "    Returns:\n",
    "    A_last - last activation value (y_pred)\n",
    "    caches - dict of caches containing every cache of forward propagation indexed from 0 to L-1\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters['W']) # number of layers in the neural network\n",
    "\n",
    "    # [LINEAR -> RELU]*(L-1)\n",
    "    for l in range(1, L):\n",
    "        A_prev = None\n",
    "        A, cache = None\n",
    "        caches[l] = None\n",
    "\n",
    "    \n",
    "    #LINEAR -> SIGMOID\n",
    "    A_last, cache = None\n",
    "    caches[L] = None\n",
    "\n",
    "    assert(A_last.shape == (1, X.shape[1])) # (1,m) \n",
    "            \n",
    "    return A_last, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Backward propagation whole process\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_whole_process(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    A_last - probability vector, output(y_pred) of the forward propagation \n",
    "    Y - true labels (0 if non-cat, 1 if cat)\n",
    "    caches - dict of caches for each layer that contains (W, b, A, Z)\n",
    "    Returns: grads - of keys 'W' and 'b' each containing the  dictionaries of keys 1..L  \n",
    "    \"\"\"\n",
    "    dL_dA= {}\n",
    "    dL_dW = {}\n",
    "    dL_db= {}\n",
    "    \n",
    "    L = len(caches) # the number of layers\n",
    "    m = None # number of samples\n",
    "    Y = Y.reshape(A_last.shape) # make sure Y is the same shape as A_last(y_pred)\n",
    "    \n",
    "    # Initialize the backpropagation    \n",
    "    dL_dA[L] = None\n",
    "\n",
    "    # layer (SIGMOID -> LINEAR) gradients\n",
    "    current_cache = None\n",
    "    dL_dA[L-1], dL_dW[L], dL_db[L] = None\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(1,L)): #  starts with L-1 ends with 1 \n",
    "        # l-th layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = None\n",
    "        dL_dA[l-1], dL_dW[l], dL_db[l] = None\n",
    "        \n",
    "    grads= None\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Build deep neural network model \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, verbose = False):\n",
    "    \"\"\"\n",
    "    X - data, array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y - true label vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims - list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate - learning rate of the gradient descent update rule\n",
    "    num_iterations - number of iterations of the optimization loop\n",
    "    verbose - if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters - parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    print ('Training {}-layers neural network with layers dimensions: {}'.format (len(layers_dims)-1, layers_dims))\n",
    "    np.random.seed(1)\n",
    "    costs = [] # to track of cost\n",
    "            \n",
    "    parameters = None\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        A_last, caches = None\n",
    "    \n",
    "        # Compute cost\n",
    "        cost = None\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = None\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = None\n",
    "       \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if verbose and i % 100 == 0:\n",
    "            print (\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    None\n",
    "    \n",
    "   \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Run for 2 layers \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_train_scaled.shape[0]\n",
    "n_h= 7 \n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x, n_h, n_y]\n",
    "\n",
    "parameters = model(\n",
    "    X_train_scaled, Y_train, layers_dims, num_iterations = 5000, verbose = True, \n",
    "    learning_rate = 0.003) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "    Training 2-layers neural network with layers dimensions: [12288, 7, 1]\n",
    "    Cost after iteration 0: 0.6950464961800915\n",
    "    Cost after iteration 100: 0.6195808854384666\n",
    "    Cost after iteration 200: 0.5865026104533535\n",
    "    Cost after iteration 300: 0.5467810398248231\n",
    "    Cost after iteration 400: 0.49825722524914073\n",
    "    ...\n",
    "    Cost after iteration 4500: 0.023333388131945094\n",
    "    Cost after iteration 4600: 0.022437269637948253\n",
    "    Cost after iteration 4700: 0.021604560218738137\n",
    "    Cost after iteration 4800: 0.02083238901344186\n",
    "    Cost after iteration 4900: 0.020094155779688965\n",
    "\n",
    "<img src = \"data/19_check_2_layer_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Run for 4 layers \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_x = X_train_flatten.shape[0]\n",
    "n_y = Y_train.shape[0]\n",
    "layers_dims = [n_x,20,7,5,n_y]\n",
    "\n",
    "parameters = model(\n",
    "    X_train_scaled, Y_train, layers_dims, num_iterations = 3000, verbose = True, \n",
    "    learning_rate = 0.0075) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Training 4-layers neural network with layers dimensions: [12288, 20, 7, 5, 1]\n",
    "Cost after iteration 0: 0.7717493284237688\n",
    "Cost after iteration 100: 0.6720534400822913\n",
    "Cost after iteration 200: 0.6482632048575212\n",
    "Cost after iteration 300: 0.6115068816101354\n",
    "Cost after iteration 400: 0.567047326836611\n",
    "...\n",
    "Cost after iteration 2500: 0.08841251177615041\n",
    "Cost after iteration 2600: 0.08595130416146428\n",
    "Cost after iteration 2700: 0.08168126914926334\n",
    "Cost after iteration 2800: 0.07824661275815534\n",
    "Cost after iteration 2900: 0.07544408693855482`\n",
    "\n",
    "<img src = \"data/19_check_4_layer.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Predict \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"        \n",
    "    X - array set to predict \n",
    "    parameters - parameters of the trained model\n",
    "    Returns:\n",
    "    Y_pred - predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    # Forward propagation\n",
    "    A_last, _ = None\n",
    "    Y_pred= None\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Evaluate\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_pred_train = predict (X_train_scaled, parameters)\n",
    "Y_pred_test = predict (X_test_scaled, parameters)\n",
    "\n",
    "print ('Train accuracy = {:.3%}'.format(np.mean (Y_pred_train == Y_train)))\n",
    "print ('Test accuracy = {:.3%}'.format(np.mean (Y_pred_test == Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "###  Expected result\n",
    "\n",
    "</font>\n",
    "\n",
    "`Train accuracy = 99.043%\n",
    "Test accuracy = 82.000%`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
